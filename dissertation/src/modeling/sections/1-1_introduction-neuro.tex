%!TEX root = ../../_preamble.tex

\section{Neural Mechanisms}

Until now our very simple model has been entirely theoretical, describing the general requirements of the computation of phonetic category identity, but the form of any biological computation is necessarily constrained by the substrate of its implementation (roughly, Marr's levels, for a recent discussion see \citep{rooijTheoryTestHow2020}). Though the model could be retained in its current form by recasting $\mathbf{P}$ as the neural representation of perceptual dimensions from which category $c \in \mathbf{C}$ is inferred, this would require strong assumptions about the form of the neural representation of perceptual dimensions, and in a practical modeling context assumes we have enough information to infer it. To preserve generality at the cost of complexity, we add an additional "layer" to the model, 

\begin{equation}
\mathbf{n} = \{n_0, n_i, ... n_{dn} : n \in \mathbf \mathbf{N}^{dm} \subseteq \mathbb{R}^{dn}\}
\end{equation}

where a neural state $\mathbf{n}$, a $dn$-dimensional instantaneous firing rate of neurons $n_i$ in some neural manifold $\mathbf{N}^{dm}$ of dimension $dm$ embedded within $\mathbb{R}^{dn}$. The manifold embedding $N$ reflects the intrinsic constrants network structure poses on the possible states $\mathbf{n} \in \mathbf{N} \subseteq \mathbb{R}$, but the embedding is arbitrary.

The neural layer is incorporated by modifying equation \ref{eqn:w} such that

\begin{align}
M_n &= f(\mathbf{s}, \mathbf{p}) : \mathbf{S} \rightarrow \mathbf{N}\\
M_p &= f(\mathbf{n}) : \mathbf{N} \rightarrow \mathbf{P}
\end{align}

where some sensory input $\mathbf{s}$ is mapped to some neural state $\mathbf{n}$, which supports some percept $\mathbf{p}$ from which phonetic category is computed. The dependence of $M_n$ on $\mathbf{p}$ reflects the possibility of top-down influence on the neural representation of a given stimulus. 

What we're doing here is largely accounting for incomplete observation of the implementation of perceptual representation. For example there might be some real perceptual dimension that is not independently represented in the neural space, but is computed "downstream" by some structure that we're not observing. In the case of making a claim on the structure of neural representation (eg. that short-run firing rates are meaningful) with full observation, $\mathbf{N} = \mathbf{P}$,  where $\mathbf{P}$ is then the perceptual space represented by the brain from which category identity is computed. This is the typical assumption of "decoding analyses." Instead our expansion of the model allows us to consider a "basis set" of neural representations that are non-isomorphic with their perceptual representations.

Arguably a computational strategy common to all sensory systems is to exploit regularities in the statistical structure of the natural world to form an efficient sensory representation\citep{kuhlBrainMechanismsEarly2010,kingRecentAdvancesUnderstanding2018a,smithEfficientAuditoryCoding2006a,stilpRapidEfficientCoding2010,schiavoCapacitiesNeuralMechanisms2019,barlowSingleUnitsSensation1972}. Though the task of phonetic perception is a truly monstrous one, work since the heyday of motor theories has demonstrated the remarkable ability of the auditory system to perform the fundamental computations of phonetic categorization has given the problem an air of tractability. And though we still are methodologically limited in our ability to study spech perception in humans at the spatiotemporal scales of its computation, work in animal models as well as recent advances in human brain electrophysiology have given some of the first glimpses. 

Several features of our model are happily known to be true of neurons in mammalian auditory cortex. 

Neurons in primary auditory cortex jointly encode multiple dimensions of sound \citep{kingRecentAdvancesUnderstanding2018a}. In ferrets presented with an array of stimuli that varied by pitch, timbre, and azimuth \citep{bizleyInterdependentEncodingPitch2009b}, more A1 neurons were observed to be sensitive to two or three dimensions (36\% and 29\%, respectively) than a single dimension (23\%). In a subset of neurons, these responses were temporally complex such that the dimensions could be partially recovered by separating sustained from onset responses \citep{walkerMultiplexedRobustRepresentations2011}. Similar results have been observed in marmosets (combined sensitivity to amplitude modulation, frequency modulation, etc. \citep{Wang2005a}) and in studies that estimated the dimensionality of receptive fields from complex stimuli like dynamic ripples in cats \citep{atencioMultidimensionalReceptiveField2017}. This is perhaps unsurprising, as cortical neurons being sensitive to multiple dimensions of a stimulus is a trivial reformulation of the well-known model of hierarchical processing throughout the auditory system (for a review, see \citep{sharpeeHierarchicalRepresentationsAuditory2011b}): cortical neurons representing "higher order" properties of a stimulus necessarily implies sensitivity to multiple features of the stimulus (provided a generously-enough low-level description of the stimulus feature space).

Maciello and colleagues recently argued that joint, rather than independent encoding of multiple stimulus dimensions is computationally advantageous \citep{macellaioWhySensoryNeurons2020}. Though sensitivity to multiple features makes response patterns ambiguous with respect to the value of any individual dimension, joint encoding provides more information about all represented dimensions to a downstream decoder. If it is the case that joint encoding is constitutive of auditory representations, and individual stimulus or perceptual dimensions are never (or rarely) represented independently, behavior that reflects sensitivity to family resemblance structure rather than optimal rule-based categorization is parsimonious. If all features are estimated simultaneously, influence of "nontarget" dimensions becomes unsurprising. 

A rich body of research has described the many conditions that auditory representations are modulated by context (for a review, see \citep{angeloniContextualModulationSound2018}) at timescales as short as hundreds of milliseconds\citep{deanRapidNeuralAdaptation2008b,rabinowitzContrastGainControl2011c}. Processes like forward masking, stimulus-specific adaptatation (SSA), and suppression of background noise all reflect the general principle that auditory representations adapt to predictable acoustic statistics in order to form robust, invariant representations of auditory objects\citep{rabinowitzConstructingNoiseinvariantRepresentations2013} by emphasizing the maximally informative dimensions\citep{atencioMultidimensionalReceptiveField2017}. 

Adaptation to noise or stimulus statistics can be characterized as a short-term `reweighting' of features through processes like synaptic depression\citep{mesgaraniMechanismsNoiseRobust2014,davidRapidSynapticDepression2009} or microcircuit interactions\citep{natanComplementaryControlSensory2015b,natanCorticalInterneuronsDifferentially2017}. In tasks based on simple parametric sounds, representations of task-relevant stimuli are enhanced on the order of minutes\citep{fritzRapidTaskrelatedPlasticity2003a}. Animals trained on multiple tasks had neurons that adapted their receptive fields to facilitate the different task demands\citep{fritzActiveListeningTaskdependent2005b} and reward structures\citep{davidTaskRewardStructure2012}. David and Shamma \citep{davidIntegrationMultipleTimescales2013} argue that short-term integration of auditory context could also be a substrate for representing and comparing auditory features that occur through time. 

The auditory system is also plastic on longer timescales to represent the dimensions of sound that are maximally informative to the demands placed on it. Rats trained using a single set of stimuli had differential enhancement of sensitivity to frequency or intensity depending on which they were trained to attend to\citep{Polley2006}. Bieszczad and Weinberger observed that such enhancement correlated with the strength of a learned memory trace\citep{bieszczadRepresentationalGainCortical2010}. 

\textbf{Speech in the Brain}

The Superior Temporal Gyrus (STG) in humans, or secondary parabelt regions in some other species, of auditory cortex is the primary candidate for representation of higher-order auditory features used in speech perception. Damage to the left posterior Superior Temporal Gyrus, containing BA 22 "Wernicke's area," has long been associated with receptive aphasia, but a variety of human and animal studies have given further insight on the character of speech processing within the STG.

A series of studies from Edward Chang and colleagues recording electrophysiological activity in human temporal lobe using high-density multi-electrode arrays have contributed greatly to our understanding of the encoding of speech sounds, particularly in the superior temporal gyrus (STG) \citep{yiEncodingSpeechSounds2019}. 

Recordings of high-gamma (70-150Hz) power show individual electrode sites in middle to posterior STG are selective to acoustically similar groups of phonemes (eg. obstruent vs. sonorant selectivity, plosive vs. fricative selectivity, etc.) in humans passively listening to natural speech samples \citep{mesgaraniPhoneticFeatureEncoding2014}. These phonetic sensitivites were reflective of sensitivity to multiple complex acoustic features that are correlated within phonetic categories and that "maximiz[e] vowel discriminability in the neural domain." \citep{mesgaraniPhoneticFeatureEncoding2014}. Lower frequency (<50Hz) macrocortigraphy recordings also show that subpopulations of pSTG neurons carry information that allows discrimination of consonant-vowel token category analogously to behavioral categorization \citep{changCategoricalSpeechRepresentation2010b}. 

In the anterior STG (aSTG), individual sorted units recorded from one person demonstrated complex, speech-specific reponses when one subject was presented with a wide array of sounds \citep{chanSpeechSpecificTuningNeurons2014}. Many (66 of 141) units demonstrated selectivity to one or a few words that was invariant across speaker. Speech selectivity was only partially explained by a linear combination of acoustic features (linear spectrograms and MFCCs), and did not (over-)generalize to noise-vocoded speech, time-reversed speech. Unit responses to individual phonemes also differed by the recent phonetic past, all together suggesting that some units in aSTG are selective to the fine spectrotemporal structure of speech sounds at single-to-few phoneme timescales \citep{chanSpeechSpecificTuningNeurons2014}. 

Though acoustic response profiles are spatially heterogeneous across the STG and between individuals \citep{mesgaraniPhoneticFeatureEncoding2014,hamiltonSpatialMapOnset2018a}, there does appear to be some functional distinction between anterior and posterior STG with respect to speech sound processing. In macroelectrode recordings in humans listening to natural sentences, pSTG electrodes selectively track phrase-level onsets, while aSTG electrodes have more sustained responses through a phrase. The dissociation between onset and sustained responses was not reflective of the discontinuous vs. continuous nature of consonants and vowels, as selectivity to groups of phonemes (vowels, plosives, nasals, etc.) was mixed in both anterior and posterior STG \citep{hamiltonSpatialMapOnset2018a}. Information useful for discrimination of phonetic identity in the pSTG develops and reaches a peak 100-150ms or so after speech sound onset \citep{mesgaraniPhoneticFeatureEncoding2014,changCategoricalSpeechRepresentation2010b}, and neural state space projections onto axes representing the activity of neurons sensitive to sound onset or sustained sound show a reliable sweep between posterior and anterior STG on the order of seconds. In short, the picture that emerges is multiple "codes" or "codecs" that overlap in multiple regions at multiple timescales.

Animal research of neural mechanisms of speech sound processing is quite sparse, and so our understanding is relatively coarse and by analogy from more general auditory research. Speech training in rats evokes a complex set of changes to acoustic response properties in several auditory cortical fields loosely analogous to secondary cortical areas in humans\citep{engineerSpeechTrainingAlters2015a}. Neurons in the anterior auditory field (AAF) and A1 were more responsive to the initial consonant in consonant-vowel (/CV/) pairs in trained vs. control rats (27\% and 57\% more spiking activity, respectively). Additionally, the proportion of neurons that were responsive to 2kHz tones (the spectral peak in the speech tokens used) increased by 65\% in AAF and 38\% in A1 after speech training compared to control rats. In contrast, in response to vowels VAF and PAF were less responsive following speech training (42\% and 30\% fewer spikes, respectively, vs. controls). In neurons that had similar frequency tuning, responses to consonants were more correlated in AAF and VAF, and responses to vowels were less correlated in AAF, A1, and VAF after speech training (vs. controls)\citep{engineerSpeechTrainingAlters2015a}. 

These results \citep{engineerSpeechTrainingAlters2015a} may not establish definitive roles for secondary auditory fields in rodent auditory cortex, but in sum do suggest that speech training induces long-lasting plasticity in auditory cortex, and suggests that processing may be distinct for different acoustic features in anterior vs. posterior fields as in humans. Mice trained to discriminate speech sounds were returned to chance following lesions of auditory cortex \citep{saundersMiceCanLearn2019}, indicating its necessity. Task-specific plasticity \citep{takahashiLearningstagedependentFieldspecificMap2011} and contribution to processing task-relevant auditory stimulus categories \citep{shiAnteriorAuditoryField2019} has been previously demonstrated in AAF, which is thought to operate as a parallel processing system, with response latencies comparable to or lower than A1 in cats \citep{carrascoNeuronalActivationTimes2011} and mice \citep{lindenSpectrotemporalStructureReceptive2003b}. PAF is a secondary auditory cortical area and thought to be downstream from both A1 and AAF \citep{pandyaSpectralTemporalProcessing2008,carrascoEvidenceHierarchicalProcessing2009}. Though their functional specialization of computational role might not be equivalent in humans, it is parsimonious to assume that primary and secondary auditory cortical areas in nonhuman mammalian auditory systems process acoustic information in such a way that supports the recognition of phonetic identity. 

\section{Towards a Research Program}

In lightly constraining the constitution of $\mathbf{N}$, loosely the neural "representation" of phonetic information, the human and animal results hint at the dissociation between $\mathbf{N}$ and $\mathbf{P}$ in our model --- en passant to the statement of the research problem. 

Suppose that one dimension $b_{vot} \in \mathbf{P}$ is the voice onset time, which dissociates voiced from unvoiced consonants (eg. /b/ vs. /p/) as the time between the onset of phonation and the occlusion of the stop. Further suppose a neural system analogous to the temporal landmark model suggested by \citep{hamiltonSpatialMapOnset2018a} where the high-energy plosive of the occlusion is "encoded" by the activity of some region analogous to the phrase-onset sensitivity of pSTG, and the sonorant, spectral quality of the voicing is encoded by another region. In this scheme, some downstream region infers VOT by comparing the relative timing of gross spiking activity between these two regions. In this hopelessly na\"ive instantiation of our model, the dimension $b_{vot}$ is some real-valued (though not necessarily linear) value from negative to positive voice onset times. Such a dimension is not present in $\mathbf{N}$ as characterized by the n-dimensional space of, say, instantaneous firing rate of n neurons, requiring $M_p$, the mapping between them. 

The dissociation of the descriptions of $\mathbf{N}$ and $\mathbf{P}$ thus, in our model, defines the research problem: 

1) We characterize the problem the brain faces in auditory phonetic perception is to learn some perceptual space $\mathbf{P}$ of maximally informative perceptual dimensions that supports the identification of received phonemes by flexibly adapting to the information present in the phoneme. 

2) Understanding the neural mechanisms of auditory phonetic perception is describing the way $\mathbf{P}$ is implemented by some neural state manifold $\mathbf{N}$ in such a way that the information loss by the model projecting $\mathbf{N}$ to $\mathbf{P}$ is minimized. 

It is not necessarily the case that we should expect to find neurons, or even collections of neurons, whose time-averaged firing rate is the literal measurement of the perceptual dimensions used to compute phonetic identity. We also don't expect to be able to estimate the full manifold of all neurons that are involved with the process, so there will ultimately always be some gap between $\mathbf{P}$ and $\mathbf{N}$. Roughly, kept independent, $\mathbf{P}$ is the level of "representation" --- the basis from which the brain derives its use of phonetic information, though we dont' characterize $\mathbf{P}$ as the unique source of information, as information is represented at multiple scales (syntactic, semantic) and is bidirectional (predictive as well as receptive) --- while $\mathbf{N}$ is the level of "implementation".

This distinction may read as trivial, but it precludes a majority of the common methodological kinks of contemporary cognitive neuroscience. The implicit assumption of "decoding"-based analysis strategies is that neural representation is encoded in the language of time-averaged firing rate, and that the accuracy of some (usually uninterrogated) classification algorithm on the timeseries of firing rates (or BOLD level, or EEG bandpass amplitude, etc.) is reflective of the presence or absence of category information in the data. The same assumption is made in the case of so-called "Representational Similarity Analysis," and any number of other analytical ruts that uncritically characterize the geometry of the brain and the perceptual reality it supports as euclidean spaces with the axes of whatever recording methodology is handy for the dataset. 

In both, the geometry of the perceptual space is also typically uninterrogated, where the parameters that were used to synthesize the stimuli, or the category labels imposed by the researcher are analyzed as if they were faithfully represented by the brain. This, despite the creation of non-isomorphic representations of physical phenomena being the entire goal of efficient perception (see \citep{stilpEfficientCodingStatistically2012,wangNeuralCodingStrategies2007}) --- if representation operated like an isomorphism then perceptual learning would be entirely unnecessary.

Rather than assuming the perceptual structure of phonemes by prespecifying cues and synthesizing sounds, or assuming the representational language of the brain to be time-averaged firing rate, we take the role of empirical geometers and attempt to preserve as much of the natural complexity of the problem and derive both from the data. The combination of apparent category structure without clear parameterization of naturally produced phonemes is, I argue, a promising way to interrogate the similarly loose category structure of the neural computations that support their perception.

(Here is where I would have continued on to describe the experiments that would follow, if I did not abruptly change disciplines.)