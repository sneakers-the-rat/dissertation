\documentclass[11pt]{article}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{siunitx}                  % Typesetting SI units
\usepackage{booktabs}                 % Tables
\usepackage[autostyle, english=american]{csquotes} % Does smart quotations
\usepackage{tabularx}
%\usepackage{capt-of} % figure captions spanning pages
\usepackage{setspace}
%\usepackage{todonotes}
\usepackage{graphicx}
%\usepackage{alltt}
\usepackage{microtype}
\usepackage[english]{babel}           % Various English stylistic things
\usepackage{amsmath,amsfonts,amsthm}  % Math
\usepackage{sectsty}                  % Style sections
\usepackage{geometry}                 % Document reshaping/sizing
%\usepackage{fontspec}
%\usepackage{textcomp}
%\usepackage{cite}                     % Citations
\usepackage{wrapfig}                  % Wrap text around figures
%\usepackage{ulem}                     % Strikeouts & Underlines
%\usepackage{tcolorbox}                % colorful boxes
\usepackage{stfloats}                 % Image/figure positioning
\usepackage{titlesec}                 % Allows customization of titles
\usepackage[colorlinks = true,
            urlcolor = OrangeRed,
            citecolor = OrangeRed,
            backref= section]{hyperref}
\usepackage{caption}                  % Enable caption styling
\usepackage{abstract} % to get saythanks

\usepackage{titling}

%\usepackage[amsfonts, amssymb]{concmath} % concrete math type
\usepackage[default]{lato}
\usepackage[T1]{fontenc}

\usepackage{float}
%\usepackage{unicode-math}
%\usepackage{ccfonts}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\setmathfont{CMU Concrete}
%\renewcommand{\rmdefault}{ccr}

% Formatting
\MakeOuterQuote{"} % Symbol for smart-quotes

% Set page geometry
\tolerance=5000 %prevents overfull hboxes
\geometry{
  top=43pt,
  left=40pt,
  right=40pt,
  bottom=50pt,
  headsep=10pt,
  columnsep=13.5pt,
  twocolumn=true,
}

% Set indentation & line spacing
\setlength{\parindent}{0.5cm}
\renewcommand{\baselinestretch}{1.3}

% Set space around figures
\setlength{\floatsep}{0pt}
\setlength{\dbltextfloatsep}{0pt}
\setlength{\intextsep}{0pt}
\setlength{\belowcaptionskip}{0pt}

\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
% \renewcommand{\topfraction}{0.85}
% \renewcommand{\bottomfraction}{0.99}
% \renewcommand{\textfraction}{0.10}
% \renewcommand{\floatpagefraction}{0.7}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Restyling

\pretitle{\fontfamily{Chivo-TLF}\selectfont\Huge\bfseries}
\posttitle{\normalfont\hfill\break\vspace{14pt}}
\preauthor{\large\normalfont}
\postauthor{\hfill\break\vspace{16pt}}
\predate{}
\postdate{\hfill\vspace*{-\baselineskip}\break%
{\fontsize{1pt}{1pt}\noindent\makebox[\linewidth]{\rule{\paperwidth}{1pt}}}}

% Section titles
\titleformat{\section}
  {\raggedright\fontfamily{Chivo-TLF}\selectfont\Large\bfseries}
  {\fontfamily{Chivo-TLF}\Large\thesection}{10pt}{}

% Subsection titles
\titleformat{\subsection}
  {\raggedright\fontfamily{Chivo-TLF}\selectfont\bfseries}
  {\fontfamily{Chivo-TLF}\selectfont\bfseries\thesubsection}{5pt}{}

% Set spacing around section titles
\titlespacing{\section}
  {0em}{2em}{0em}
\titlespacing{\subsection}
  {0em}{1em}{0em}

% Set figure tags as bold
\captionsetup[figure]{labelfont=bf,font=small}
\captionsetup[table]{labelfont=bf,font=small}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}
\title{Mice can learn phonetic categories}
\author{Jonny Saunders, Michael Wehr 
\thanks{wehr@uoregon.edu} 
\thanks{University of Oregon, Institute of Neuroscience and Department of Psychology, Eugene, OR 97403, United States.} 
\thanks{Additional thanks to Aldis Weible, Lucas Ott, and Connor O'Sullivan. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. 1309047. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.}}
\date{\small J. Acoust. Soc. Am. \textbf{145}(3) - \href{https://doi.org/10.1121/1.5091776}{doi:10.1121/1.5091776} - \href{https://github.com/wehr-lab/SaundersWehr-JASA2019}{Author typeset version}, compiled \today }



<<install,echo=FALSE,include=FALSE>>==
# To install dependencies, uncomment and run once
# install.packages(c("ggplot2", "binom",   "plyr",     "reshape",   "xtable", "rio", "dplyr",
#                    "lme4",    "effects", "stats",    "multcomp",  "grid", 
#                    "rsvg",    "gtable",  "ggdendro", "gridExtra", "knitr" ))
@

<<setup,echo=FALSE,include=FALSE>>=
## Load libraries


require(ggplot2)
require(binom)
require(plyr)
require(reshape)
require(xtable)
require(lme4)
require(effects)
require(stats)
require(multcomp) # to compute the tukey shit below
#summary(glht(novtypes.lmer4, linfct=mcp(gentype2="Tukey")))
@

<<load,echo=FALSE,include=FALSE,cache=TRUE>>=
# Change this to update all the cached analyses
change_bit <- 1

# call dirname twice to get base directory
# if running interactively, you can get wd this way:
#base_dir <- dirname(dirname(rstudioapi::callFun("getActiveDocumentContext")$path))
#setwd(base_dir)
base_dir <- dirname(getwd())
message(base_dir)
fig_dir <- paste(base_dir, "/figures/", sep="")
code_dir <- paste(base_dir, "/code/", sep="")
data_dir <- paste(base_dir, "/data/", sep="")

# generalization data
gendat <- readRDS(paste(data_dir,'gendata.RData', sep=""))

# generalization data with speaker numbers remapped:
# gendat has speaker such that numbers represent the structure of presentation
# ie. for both training cohorts, training tokens are from speaker "1 and 2"
# gendat.rm instead has the same speaker numbers between cohorts
# (ie. all mice have cohort 1's speaker numbering)
gendat.rm <- readRDS(paste(data_dir,'gendata_remap.RData', sep=""))


# generalization data including the last training step
gendat.13 <- readRDS(paste(data_dir,"gendat_ts_w13.RData",sep=""))


# Formant data
f2 <- readRDS(paste(data_dir,"formants.RData",sep=""))

# Mouse demographics - birth dates, train dates, train boxes, etc.
demog <- readRDS(paste(data_dir,'Mouse_Demographics.RData',sep=""))
@

<<imports,echo=FALSE,include=FALSE>>=
source(paste(code_dir,'plotting_fns.R',sep=""))
@


<<demographics,echo=FALSE,include=FALSE,cache=TRUE,dependson='load'>>=
# snag some values for the text
n_total <- nrow(demog)
n_excluded <- 46-n_total
n_learners <- length(unique(gendat$mouse))
pct_learn <- round((n_learners/n_total)*100,1)

# Subset
demog.gen <- demog[demog$Status=="Generalizer",]

learn_time <- round(mean(demog.gen$DurTillGen),1)
learn_time_sd <- round(sd(demog.gen$DurTillGen),1)
age_onset <- round(mean(demog.gen$AgeStart),1)
age_onset_sd <- round(sd(demog.gen$AgeStart),1)
age_offset <- round(mean(demog.gen$AgeEnd),1)
age_offset_sd <- round(sd(demog.gen$AgeEnd),1)



# Methods summaries & tests
demog.count_sex <- ddply(demog,.(Status, Sex),summarize, count = length(Sex))
demog.count_box <- ddply(demog,.(Status, Box),summarize, count = length(Box))
demog.count_litter <- ddply(demog,.(Status, Litter), summarize, count = length(Litter))

demog.count_sex <- as.matrix(cast(demog.count_sex, Status ~ Sex, value="count", fill=0))
demog.count_box <- as.matrix(cast(demog.count_box, Status ~ Box, value="count", fill=0))
demog.count_litter <- as.matrix(cast(demog.count_litter, Status ~ Litter, value="count", fill=0))

chi_sex <- round(fisher.test(demog.count_sex)$p.value,3)
chi_box <- round(fisher.test(demog.count_box)$p.value,3)

demog.lm_age <- summary(glm(Status ~ AgeStart, data=demog, family="binomial"))
demog_age_z <- round(demog.lm_age$coefficients['AgeStart','z value'],3)
demog_age_p <- round(demog.lm_age$coefficients['AgeStart','Pr(>|z|)'],3)

@

<<summaries,echo=FALSE,include=FALSE,cache=TRUE,dependson='load'>>=
gendat.mouse <- ddply(gendat,.(mouse),plyr::summarize,
                      meancx   = mean(correct),
                      meanresp = mean(response),
                      meantarg = mean(target),
                      nobs     = length(correct),
                      cohort=cohort[1])


# Mean corrects & conf. intervals by mouse and generalization type
gendat.mouse_type <- ddply(gendat,.(mouse,gentype2),plyr::summarize,
                      meancx   = mean(correct),
                      meanresp = mean(response),
                      meantarg = mean(target),
                      cilo     = binom.confint(sum(correct),length(correct),
                                               conf.level=0.95,method="exact")[[5]],
                      cihi     = binom.confint(sum(correct),length(correct),
                                               conf.level=0.95,method="exact")[[6]],
                      nobs     = length(correct),
                      cohort=cohort[1])

# minimum and maximum accuracies by generalization type
gendat.minmax <- ddply(gendat.mouse_type,.(gentype2),plyr::summarize,
                       minimum = min(meancx),
                       maximum = max(meancx),
                       meancx=mean(meancx))

gendat.mouse_minmax <- ddply(gendat.mouse_type,.(mouse),plyr::summarize,
                       minimum = min(meancx),
                       maximum = max(meancx),
                       meancx=mean(meancx),
                       cohort = cohort[1])

gendat.novelnot <- ddply(gendat, .(mouse, novelnot), plyr::summarize,
                         meancx   = mean(correct),
                         meanresp = mean(response),
                         meantarg = mean(target),
                         bias = meanresp-meantarg,
                         cohort = cohort[1])
gendat.novel <- gendat.novelnot[gendat.novelnot$novelnot==1,]

# reorder by max performance
gendat.mouse_type$mouse <- factor(gendat.mouse_type$mouse,
                                  levels=gendat.mouse_minmax[
                                    order(gendat.mouse_minmax$maximum,decreasing=TRUE),
                                    "mouse"],
                                  ordered=TRUE)

###################
# Remapped gendat summaries for heatmap
gendat.vowmus <- ddply(gendat.rm,.(mouse,consonant,speaker,vowel),plyr::summarize,
                       meancx = mean(correct),
                       cilo = binom.confint(sum(correct),length(correct),
                                            conf.level=0.95,method="exact")[[5]],
                       cihi = binom.confint(sum(correct),length(correct),
                                            conf.level=0.95,method="exact")[[6]],
                       bias = mean(response)-mean(target),
                       nobs = length(correct))

gendat.tokmus      <- ddply(gendat.rm,.(mouse,consonant,speaker,vowel,token),
                            plyr::summarize,
                            meancx = mean(correct),
                            meanrs = mean(response),
                            n_tr = length(correct),
                            cohort= cohort[1])
gendat.tokmus$cohort <- as.factor(gendat.tokmus$cohort)

gendat.tokmus$ID   <- as.factor(paste(gendat.tokmus$consonant,
                                      gendat.tokmus$speaker,
                                      gendat.tokmus$vowel,
                                      gendat.tokmus$token))

gendat.token <- ddply(gendat.rm,.(consonant,speaker,vowel,token),plyr::summarize,
                      meancx = mean(correct),
                      meanrs = mean(response),
                      meantg = mean(target),
                      cilo = binom.confint(sum(correct),length(correct),
                                           conf.level=0.95,method="exact")[[5]],
                      cihi = binom.confint(sum(correct),length(correct),
                                           conf.level=0.95,method="exact")[[6]],
                      nobs = length(correct))

# join formant data with token data
gendat.tokf2 <- dplyr::left_join(gendat.token, f2, by=c("consonant","speaker","vowel","token"))
gendat.tokmusf2 <- dplyr::left_join(gendat.tokmus, f2,
                                    by=c("consonant","speaker","vowel","token"))

# token summary for plotting
gendat.tokmus_cast <- cast(gendat.tokmus,mouse~ID,value="meancx")

gendat.mouse_minmax <- dplyr::left_join(gendat.mouse_minmax,gendat.novel[,c('mouse','bias')], by="mouse")
gendat.mouse_minmax$cohort <- as.factor(gendat.mouse_minmax$cohort)


@

<<secondaryglm,echo=FALSE,include=FALSE,cache=TRUE,dependson="load">>==
summarize_glm <- function(aglm, rowname, round_digs=3){

  glm_sum <- summary(aglm)
  glm_ci <- confint(aglm)

  #anova(gendat.tsglm1,gendat.tsglm2,gendat.tsglm3, gendat.tsglm4, gendat.tsglm5, test="Chisq")

  p <- round(glm_sum$coefficients[rowname,"Pr(>|z|)"],round_digs)
  z <- round(glm_sum$coefficients[rowname,"z value"],round_digs)
  cilo <- round(glm_ci[rowname,1],round_digs)
  cihi <- round(glm_ci[rowname,2],round_digs)

  return(list("p"=p,"z"=z,"cilo"=cilo,"cihi"=cihi))

}


# Dataframe for regressions
gendat.mouse.rs <- cast(gendat.mouse_type,mouse~gentype2,value="meancx")
names(gendat.mouse.rs) <- c("mouse","gt1","gt2","gt3","gt4","gt5")

# Regression of GT1 vs. the rest
base.gen <- lm(gt1 ~ gt2+gt3+gt4+gt5, data=gendat.mouse.rs)
base.gensum <- summary(base.gen)
base.gen.r <- round(base.gensum$adj.r.squared,digits=2)
base.gen.df <- round(base.gensum$fstatistic[2:3],digits=2)
base.gen.f <- round(base.gensum$fstatistic[1],digits=2)

# Logistic regression of appearance
gendat.ts <- gendat.13[gendat.13$nappear>0,]

gendat.tsglm1 <- glm(correct ~ nappear, data=gendat.ts, family="binomial")
gendat.tsglm2 <- glm(correct ~ log(nappear), data=gendat.ts, family="binomial")
gendat.tsglm3 <- glm(correct ~ novelnot + log(nappear), data=gendat.ts, family="binomial")
gendat.tsglm4 <- glm(correct ~ novelnot + log(nappear) + mouse, data=gendat.ts, family="binomial")
gendat.tsglm5 <- glm(correct ~ novelnot * log(nappear) + mouse, data=gendat.ts, family="binomial")

#anova(gendat.tsglm1,gendat.tsglm2,gendat.tsglm3, gendat.tsglm4, gendat.tsglm5, test="Chisq")

# split by novelty type
gendat.tsglm6 <- glm(correct ~ gentype2 + log(nappear)+mouse, data=gendat.ts, family="binomial")
gendat.tsglm7<- glm(correct ~ gentype2 * log(nappear)+mouse, data=gendat.ts, family="binomial")

# less than 40 trials
gendat.ts_10 <- gendat.ts[gendat.ts$nappear <= 10,]
gendat.ts_20 <- gendat.ts[gendat.ts$nappear <= 20,]

gendat.ts_10glm4 <- glm(correct ~ novelnot + log(nappear) + mouse, data=gendat.ts_10, family="binomial")
gendat.ts_10glm5 <- glm(correct ~ novelnot * log(nappear) + mouse, data=gendat.ts_10, family="binomial")
gendat.ts_20glm4 <- glm(correct ~ novelnot + log(nappear) + mouse, data=gendat.ts_20, family="binomial")
gendat.ts_20glm5 <- glm(correct ~ novelnot * log(nappear) + mouse, data=gendat.ts_20, family="binomial")
#anova(gendat.ts_10glm4, gendat.ts_10glm5)
#anova(gendat.ts_20glm4, gendat.ts_20glm5)

# pull summaries for the text
ts_irax <- summarize_glm(gendat.tsglm5, "novelnot1:log(nappear)")
ts_type <- summarize_glm(gendat.tsglm7, "gentype24:log(nappear)")
ts_10   <- summarize_glm(gendat.ts_10glm5, "novelnot1:log(nappear)")
ts_20   <- summarize_glm(gendat.ts_20glm5, "novelnot1:log(nappear)")



@

<<heat,echo=FALSE,include=FALSE,cache=TRUE,dependson="load">>==



@

<<etc,echo=FALSE,include=FALSE,cache=TRUE,dependson="load">>==
#gendat_learned <- gendat.mouse_type[gendat.mouse_type$gentype2==1,]
#gendat_learned$Mouse <- gendat_learned$mouse

#demog.gen <- dplyr::left_join(demog.gen,gendat_learned,by="Mouse")

#gentime.lm <- lm(meancx~DurTillGen,data=demog.gen)

#ggplot(demog.gen, aes(x=DurTillGen, y=meancx))+geom_point()+geom_smooth(method="lm")

# first presentations
gendat.first <- gendat.13[gendat.13$nappear == 1 & gendat.13$novelnot == 1,]

first_novelnot <- binom.test(sum(gendat.first$correct), nrow(gendat.first), p=0.5, alternative="greater")
first_n <- first_novelnot$parameter[[1]]
first_cx <- first_novelnot$statistic[[1]]
first_pct <- round(first_novelnot$estimate[[1]],3)
first_ci <- round(first_novelnot$conf.int[[1]],3)

gendat.hard <- gendat.first[gendat.first$gentype2 == 5,]
hard_novelnot <- binom.test(sum(gendat.hard$correct), nrow(gendat.hard), p=0.5, alternative="greater")
hard_n <- hard_novelnot$parameter[[1]]
hard_cx <- hard_novelnot$statistic[[1]]
hard_pct <- round(hard_novelnot$estimate[[1]],3)
hard_ci <- round(hard_novelnot$conf.int[[1]],3)

# cohort differences
n_cohort1 <- length(unique(gendat[gendat$cohort==1,]$mouse))
n_cohort2 <- length(unique(gendat[gendat$cohort==2,]$mouse))

gendat.rm$ID <- as.factor(paste(gendat.rm$consonant, gendat.rm$speaker, gendat.rm$vowel, gendat.rm$token))
cohort.lm1 <- lm(meancx ~ ID, data=gendat.tokmus)
cohort.lm2 <- lm(meancx ~ ID + ID:cohort, data=gendat.tokmus)
cohort.lm3 <- lm(meancx ~ ID + ID:cohort + as.factor(consonant), data=gendat.tokmus)
cohort_aov<- anova(cohort.lm1,cohort.lm2,cohort.lm3,test="LRT")
pct_var <- cohort_aov$`Sum of Sq`[2]/cohort_aov$RSS[1]
cohort <- list("df" = cohort_aov$Df[2],
               "p"= round(cohort_aov$`Pr(>Chi)`[2],3),
               "pct_var" = round(pct_var,3)*100)

# bias/accuracy
bias.lm1 <- summary(lm(meancx ~ bias, data=gendat.mouse_minmax))
bias_acc <- list("b"= round(bias.lm1$coefficients['bias','Estimate'],3),
                 "t"= round(bias.lm1$coefficients['bias','t value'],3),
                 "p"= round(bias.lm1$coefficients['bias','Pr(>|t|)'],3),
                 "df"= bias.lm1$df[2])

# R version
r_version <-R.version$version.string
rstud_version <- "1.1.456"

# Kluender calcs
# numbers from Table 1 in Kluender 1987
pecks <- data.frame(corrects=c(55.3,67.9, 74.6, 74.4, 90.9),
                    incorrects=c(8.4, 30.0, 42.5,
                                 8.9, 18.6))
pecks$accuracy <- pecks$corrects/(pecks$corrects+pecks$incorrects)
# rows 1/4, 2/5 are the same birds.
pecks[1,] <- colMeans(pecks[c(1,4),])
pecks[2,] <- colMeans(pecks[c(2,5),])
pecks <- pecks[c(1,2,3),]
kluender_acc <- round(mean(pecks$accuracy)*100,1)


@

<<acoustic,echo=FALSE,include=FALSE,cache=TRUE,dependson='load'>>=
# get first principal component split by consonant to get locus equation lines (the right way)
f2_pc_g <- prcomp(f2[f2$human_cons=="g", c("f2_onset","f2_vowel")], scale=FALSE)
f2_pc_b <- prcomp(f2[f2$human_cons=="b", c("f2_onset","f2_vowel")], scale=FALSE)

gpc_var <- round(summary(f2_pc_g)$importance[2,1],3)*100
bpc_var <- round(summary(f2_pc_b)$importance[2,1],3)*100

#regression on raw
gendat.tokmusf2$diffkhz <- gendat.tokmusf2$dist_diff/1000

f2_lm0 <- lm(meanrs ~ consonant, data=gendat.tokmusf2)
f2_lm1 <- lm(meanrs ~ diffkhz, data=gendat.tokmusf2)
f2_lm2 <- lm(meanrs ~ diffkhz + consonant, data=gendat.tokmusf2)
f2_lm3 <- lm(meanrs ~ diffkhz * consonant, data=gendat.tokmusf2)
f2_lm4 <- lm(meanrs ~ diffkhz * consonant * cohort, data=gendat.tokmusf2)

summarize_f2 <- function(amodel, coef1, coef2){
  amodel_sum <- summary(amodel)
  amodel_conf <- round(confint(amodel),3)

  acoust <- list(g_b = round(amodel$coefficients[[coef1]],3),
               g_b_lo = amodel_conf[coef1,1],
               g_b_hi = amodel_conf[coef1,2],
               g_sd = round(sd(f2[f2$consonant==1,"dist_diff"]/1000),3),
               b_b = round(amodel$coefficients[[coef2]],3),
               b_b_lo = amodel_conf[coef2,1],
               b_b_hi = amodel_conf[coef2,2],
               b_sd = round(sd(f2[f2$consonant==2,"dist_diff"]/1000),3))

  return(acoust)

}

acoust <- summarize_f2(f2_lm3, "diffkhz", "diffkhz:consonant2")


# cohort regressions
gendat.c1 <- gendat.tokmusf2[gendat.tokmusf2$cohort==0,]
gendat.c1$c1diffkhz <- gendat.c1$dist_diff_c1/1000
f2_c1_lm0 <- lm(meanrs ~ diffkhz * consonant, data=gendat.c1)
f2_c1_lm1 <- lm(meanrs ~ c1diffkhz * consonant, data=gendat.c1)
#anova(f2_c1_lm0, f2_c1_lm1)

gendat.c2 <- gendat.tokmusf2[gendat.tokmusf2$cohort==1,]
gendat.c2$c2diffkhz <- gendat.c2$dist_diff_c2/1000
f2_c2_lm0 <- lm(meanrs ~ diffkhz * consonant, data=gendat.c2)
f2_c2_lm1 <- lm(meanrs ~ c2diffkhz * consonant, data=gendat.c2)
#anova(f2_c2_lm0, f2_c2_lm1)

acoust_c1 <- summarize_f2(f2_c1_lm1, "c1diffkhz", "c1diffkhz:consonant2")
acoust_c2 <- summarize_f2(f2_c2_lm1, "c2diffkhz", "c2diffkhz:consonant2")
@

<<lmerrun,eval=FALSE,echo=FALSE,include=FALSE,cache=TRUE,dependson='load'>>=

# refs:
# http://www.statmethods.net/advstats/bootstrapping.html
# https://datascienceplus.com/introduction-to-bootstrap-with-applications-to-mixed-effect-models/
# http://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html
# https://hlplab.wordpress.com/2009/01/19/plotting-effects-for-glmer-familybimomial-models/
# https://arxiv.org/pdf/1406.5823.pdf


# Make control object
ctrl <- glmerControl(compDev = 1,
                     tolPwrss=1e-5, # tolerance for declaring convergence in iterative SS
                     # options for optimizer
                     optCtrl=list(maxfun=10000, # max number of function evals
                                  FtolAbs=1e-4, # tolerance on change in fn values
                                  XtolRel=1e-6, # tolerance on change in param values
                                  rhoend=2e-5))

# Extra null model
novtypes.lmer0 <- glm(correct ~ gentype2,data=gendat,family=binomial)

# Null model - see if mouse has an effect on correct (it does)
novtypes.lmer1 <- glmer(correct ~ (1|mouse),
                        data=gendat,family=binomial, verbose=2, control=ctrl)

# Basic model - effect of gentype2 same across all mice, only intercept changes
novtypes.lmer2 <- glmer(correct ~ gentype2 + (1|mouse),
                        data=gendat,family=binomial, verbose=2, control=ctrl)

# Interaction model - effect of gentype2 allowed to vary by mouse, no correlation between intercept devs and gentype devs
# Predict the degree to which gentype diff deviates from global effect of gentype
novtypes.lmer3 <- glmer(correct ~ gentype2 + (1|mouse) + (0+gentype2|mouse),
                        data=gendat,family=binomial,verbose=2,control=ctrl)

# Interaction model - effect of gentype2 allowed to vary by mouse, allow correlation between intercept devs (estimate correlation between intercept devs and gentype devs across mice)
novtypes.lmer4 <- glmer(correct ~ gentype2 + (1 + gentype2|mouse),
                        data=gendat,family=binomial,verbose=2,control=ctrl)


# Profile the model to plot the deviance of fixed effects
#nt4.pr <- profile(novtypes.lmer4,
#                  which=3, # fixed effects
#                  maxpts=5,
#                  delta.cutoff=1/5,
#                  parallel="multicore",ncpus=7,verbose=2)


# save model so we don't have to take forever compiling every time
#saveRDS(novtypes.lmer1, file=paste(data_dir,"lmer1.RData",sep=""))
#saveRDS(novtypes.lmer2, file=paste(data_dir,"lmer2.RData",sep=""))
#saveRDS(novtypes.lmer3, file=paste(data_dir,"lmer3.RData",sep=""))
#saveRDS(novtypes.lmer4, file=paste(data_dir,"lmer4.RData",sep=""))
@

<<lmerload,echo=FALSE,include=FALSE>>==
novtypes.lmer1 <- readRDS(file=paste(data_dir,"lmer1.RData",sep=""))
novtypes.lmer2 <- readRDS(file=paste(data_dir,"lmer2.RData",sep=""))
novtypes.lmer3 <- readRDS(file=paste(data_dir,"lmer3.RData",sep=""))
novtypes.lmer4 <- readRDS(file=paste(data_dir,"lmer4.RData",sep=""))
@
<<lmercalc,echo=FALSE,include=FALSE,cache=TRUE>>==


# simple anova
novtypes.aov <- data.frame(anova(novtypes.lmer1,novtypes.lmer2,novtypes.lmer4,refit=FALSE))

rownames(novtypes.aov) <- c("Mouse", "Mouse + Type", "Type $|$ Mouse")
novtypes.aov <- novtypes.aov[,c("Df", "Chisq", "Chi.Df", "Pr..Chisq.")]
ps = c()
for (i in seq(length(novtypes.aov$Pr..Chisq.))){
  p <- novtypes.aov$Pr..Chisq.[i]
  if (is.na(p)){
    ps[i] <- NA

  } else {
  if (p<.001){
    ps[i] <- "$\\ll$ 0.001"
  } else {
    ps[i] <- p
  }
  }
}
novtypes.aov$`Pr..Chisq.` <- ps

names(novtypes.aov) <- c("DF", "$\\chi^2$", "$DF_{\\chi^2}$", "Pr($>\\chi^2$)")

# Make DF for xtable
logits2ps <- function(x){
  p <- exp(x)/(1+exp(x))
  return(p)
}

coefx <- effect("gentype2", novtypes.lmer4)
p_correct = paste(round(logits2ps(coefx[[5]]),digits=3),"*",sep="")
ci_low <- round(logits2ps(coefx$lower),digits=3)
ci_high <- round(logits2ps(coefx$upper),digits=3)
cis <- paste("[",ci_low,", ",ci_high,"]",sep="")
rand_corr <- VarCorr(novtypes.lmer4)
rand_corr <- rand_corr$mouse
rand_corr <- attr(rand_corr, "correlation")
rand_corr_1 <- c("",abs(round(rand_corr[(-1),1],2)))
rand_corr_2 <- c("","",round(rand_corr[-(1:2),2],2))
rand_corr_3 <- c("","","",round(rand_corr[-(1:3),3],2))
rand_corr_4 <- c("","","","",round(rand_corr[-(1:4),4],2))

lm_df <- data.frame("pc"=p_correct, "ci"=cis,"rc1"=rand_corr_1,"rc2"=rand_corr_2,"rc3"=rand_corr_3,"rc4"=rand_corr_4)
colnames(lm_df) <- c("Accuracy", "95% Wald CI","Corr","","","")
rownames(lm_df) <- c("Learned","Token","Vowel","Speaker", "Vow+Spk")


lme.aov <- anova(novtypes.lmer4,novtypes.lmer2)
lme.ch <- round(lme.aov$Chisq[2], digits=2)
lme.df <- round(lme.aov$`Chi Df`[2], digits=2)

#summary(glht(novtypes.lmer4, linfct=mcp(gentype2="Tukey")))
@




\twocolumn[
%\begin{@twocolumnfalse}
\maketitle
%\begin{onecolabstract}
%\end{onecolabstract}
%\end{@twocolumnfalse}
]
\saythanks

\bfseries\noindent We perceive speech as a series of relatively invariant phonemes despite extreme variability in the acoustic signal. To be perceived as nearly-identical phonemes, speech sounds that vary continuously over a range of acoustic parameters must be perceptually discretized by the auditory system. Such many-to-one mappings of undifferentiated sensory information to a finite number of discrete categories are ubiquitous in perception. Although many mechanistic models of phonetic perception have been proposed, they remain largely unconstrained by neurobiological data. Current human neurophysiological methods lack the necessary spatiotemporal resolution to provide it: speech is too fast and the neural circuitry involved is too small. Here we demonstrate that mice are capable of learning generalizable phonetic categories, and can thus serve as a model for phonetic perception. Mice learned to discriminate consonants, and generalized consonant identity across novel vowel contexts and speakers, consistent with true category learning. A mouse model, given the powerful genetic and electrophysiological tools for probing neural circuits available for them, has the potential to powerfully augment our mechanistic understanding of phonetic perception.\normalfont

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Introduction
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\subsection{Lack of acoustic invariance in phonemes}

We perceive speech as a series of relatively invariant phonemes despite extreme variability in the acoustic signal. This lack of order within phonemic categories remains one of the fundamental problems of speech perception \cite{Sussman1998}. Plosive stop consonants (such as /b/ or /g/) are the paradigmatic example of phonemes with near-categorical perception \cite{Holt2010,Kronrod2016a,LIBERMAN1957} without invariant acoustic structure \cite{Elman1988,Kluender1987}. The problem is not just that phonemes are acoustically variable, but rather that there is a fundamental lack of invariance in the relation between phonemes and the acoustic signal \cite{Kluender1987}. Despite our inability to find a source of invariance in the speech signal, the auditory system learns some acoustic-perceptual mapping such that a plosive stop like /b/ is perceived as nearly identical across phonetic contexts. A key source of variability is coarticulation, which causes the sound of a spoken consonant to be strongly affected by neighboring segments, such as vowels. Coarticulation occurs during stop production because the articulators (such as the tongue or lips) have not completely left the positions from the preceding phoneme, and are already moving to anticipate the following phoneme \cite{Liberman1967,Farnetani1990}.  Along with many other sources of acoustic variation like speaker identity, sex, accent, or environmental noise; coarticulation guarantees that a given stop consonant does not have a uniquely invariant acoustic structure across phonetic contexts. In other words, there is no canonical acoustic /b/ \cite{Liberman1967,Holt2010}. Phonetic perception therefore cannot be a simple, linear mapping of some continuous feature space to a discrete phoneme space. Instead it requires a mapping that flexibly uses evidence from multiple, imperfect cues depending on context \cite{Holt2010, Diehl2004}. This invariant perception of phonemes, despite extreme variability in the physical speech signal, is referred to as the non-invariance problem \cite{Perkell1986}.

\subsection{Generality of phonetic perception}

The lack of a simple mapping between acoustic attributes and phoneme identity has had a deep influence on phonetics, in part motivating the hypothesis that speech is mechanistically unique to humans \cite{Lieberman1984}, and the development of non-acoustic theories of speech perception (most notably motor theories \cite{Diehl2004,Liberman1967,Liberman1985}). However, it has been clear for more than 30 years that at least some auditory components of speech perception are not unique to humans, suggesting that human speech perception exploits evolutionarily-preserved functions of the auditory system \cite{Kluender1987,Carbonell2014,Ghazanfar1999,Bornkessel-Schlesewsky2015}. For example, nonhuman animals like quail \cite{Kluender1987, Kluender1994}, chinchillas \cite{Kuhl1978}, rats \cite{Engineer2015}, macaques \cite{Kuhl1983}, and songbirds \cite{Dooling1995} are capable of learning phonetic categories that share some perceptual qualities with humans \cite{Lotto1997,Kluender2000}. This is consistent with the idea that categorizing phonemes is just one instance of a more general problem faced by all auditory systems, which typically extract useable information from complex acoustic environments by reducing them to a small number of 'auditory objects' (for review, see \cite{Bizley2013}).%
%
\subsection{Neurolinguistic theories of phonetic perception}

Many neurolinguistic theories of phonetic perception have been proposed \cite{Rauschecker2009a,Strauss2007,Kluender2013a,Liberman1985,Gaskell1997}, but neurophysiological evidence to support them is limited. One broad class of models follows the paradigm of hierarchical processing first described by Hubel and Weisel in the visual system \cite{Rauschecker2009a,Hubel1962,Strauss2007}. In these models, successive processing stages in the auditory system extract acoustic features with progressively increasing complexity by combining the simpler representations present in preceding stages. Such hierarchical processing is relatively well-supported by experimental data. For example, the responses of neurons in primary auditory cortex (A1) to speech sounds are more diverse than those in inferior colliculus \cite{Ranasinghe2013} (but see \cite{Bartlett2013}). While phoneme identity can be classified post-hoc from population-level activity in A1 \cite{Centanni2013,Engineer2008,Steinschneider2003}, neurons in secondary auditory cortical regions explicitly encode higher-order properties of speech sounds \cite{Mesgarani2014,Belin2000a,Chang2010,Pasley2012,Bidelman2013}.

Another class of models proposes that phonemes have no positive acoustic "prototype", and that we instead learn only the acoustic features useful for telling them apart \cite{Kluender2013a}. Theoretically, these discriminative models provide better generalization and robustness to high variance \cite{Ng2002a}. Theories based on discrimination rather than prototype-matching have a long history in linguistics \cite{Saussure1916}, but have rarely been implemented as neurolinguistic models. A possible neural implementation of discriminative perception is that informative contrast cues could evoke inhibition to suppress competing phonetic percepts, similar to predictive coding \cite{Rutishauser2015,Kluender2013a,Dresher2008}. Neurophysiological evidence supports the existence of discriminative predictive coding, but its specific implementation is unclear \cite{Blank2016,Gagnepain2012}.

These two very different classes of models illustrate a major barrier faced by phonetic research: both classes can successfully predict human categorization performance, making it difficult to empirically validate or refute either of them using psychophysical experiments alone. Mechanistic differences have deep theoretical consequences --- for example, the characterizations made by the above two classes of models regarding what phonemes \textit{are} precisely oppose one another: are they positive acoustic prototypes, or sets of negative acoustic contrasts? Perceptually, do listeners identify phonemes, or discriminate between them? Neurobiological evidence regarding how the brain actually solves these categorization problems could help overcome this barrier.

\subsection{The utility of a mouse model for speech research}

Neurolinguistic research in humans faces several limitations that could be overcome using animal models.

First, most current human neurophysiological methods lack the spatiotemporal resolution to probe the fine spatial scale of neuronal circuitry and the millisecond timescale of speech sounds. A causal, mechanistic understanding of computation in neural circuits is also greatly aided by the ability to manipulate individual neurons or circuit components, which is difficult in humans. Optogenetic methods available in mice provide the ability to activate, inactivate, or record activity from specific types of neurons at the millisecond timescales of speech sounds.

Second, it is difficult to isolate the purely auditory component of speech perception in humans. Humans can use contextual information from syntax, semantics or task structure to infer phoneme identity \cite{Fox2016,Schouten2003}. It is also difficult to rule out the contribution of multimodal information \cite{Rosenblum2008}, or of motor simulation predicted by motor theories. Certainly, these and other non-auditory strategies are used during normal human speech perception. Nevertheless, speech perception is possible without these cues, so any neurocomputational theory of phonetic perception must be able to explain the purely auditory case. Animal models allow straightforward isolation of purely auditory phonetic categorization without interference from motor, semantic, syntactic, or other non-auditory cues.

Third, it is difficult to control for prior language experience in humans. Experience-dependent effects on phonetic perception are present from infancy \cite{Kuhl1992}. It can therefore be challenging to separate experience-driven effects from innate neurocomputational constraints imposed by the auditory system. Completely language-naive subjects (such as animals) allow the precise control of language exposure, permitting phonetics and phonology to be disentangled in neurolinguistics.

\begin{figure*}[!hb]
\begin{centering}
\includegraphics[width=\linewidth]{../figures/Figure1.pdf}
\caption{\label{spectro}{\textbf{Stimuli and Task Design.} \textbf{a)} Spectrograms of stimuli. Left: Example of an original recording of an isolated consonant-vowel token (/gI/). Center: the same token pitch-shifted upwards by 10x (3.3 octaves) into the mouse hearing range. Right: Recording of the pitch-shifted token presented in the behavior box. Stimuli retained their overall acoustic structure below 34kHz (the upper limit of the speaker frequency response). \textbf{b)} Power spectra (dB, Welch's method) of tokens in \textbf{a}. Black: Original (left frequency axis), red: Pitch-shifted (right frequency axis), blue: Box Recording (right frequency axis). \textbf{c)} Mice initiated a trial by licking in a center port and responded by licking on one of two side ports. Correct responses were rewarded with water and incorrect responses were punished with a mildly-aversive white noise burst. \textbf{d)} The difficulty of the task was gradually expanded by adding more tokens (squares), vowels (labels), and speakers (rows) before the mice were tested on novel tokens in a generalization task. \textbf{e)} Mice (colored lines) varied widely in the duration of training required to reach the generalization phase. Mice were returned to previous levels if they remained at chance performance after reaching a new stage.
}}\end{centering}
\end{figure*}


Animal models of phonetic perception are a useful way to avoid these confounds, and provide an important alternative to human studies for empirically grounding the development of neurolinguistic theories. The mouse is particularly well-suited to serve as such a model. A growing toolbox of powerful electrophysiological and optogenetic methods in mice has allowed unprecedented precision in characterizing neural circuits and the computations they perform.

\subsection{The utility of phonetics for auditory neuroscience}

Conversely, auditory neuroscience stands to benefit from the framework provided by phonetics for studying how sound is transformed to meaning. Understanding how complex sounds are encoded and processed by the auditory system, ultimately leading to perception and behavior, remains a challenge for auditory neuroscience. For example, it has been difficult to extrapolate from simple frequency/amplitude receptive fields to understand the hierarchical organization of complex feature selectivity across brain areas. A great strength of neuroethological model systems such as the songbird is that both the stimulus (e.g., the bird's own song) and the behavior (song perception and production) are well understood. This has led to significant advances in understanding the hierarchical organization and function of the song system \cite{Brenowitz1997,Theunissen2014}. The long history of of speech research in humans has produced a deep understanding of the relationships between acoustic features and phonetic perception \cite{Peterson1952}. These insights have enabled specific predictions about what kinds of neuronal selectivity for features (and combinations of features) might underlie phonetic perception \cite{Sussman1998}. Although recognizing human speech sounds is not a natural ethological behavior for mice, phonetics nevertheless provides a valuable framework for studying how the brain encodes and transforms complex sounds into perception and behavior.

Here we trained mice to discriminate between pitch-shifted recordings of naturally produced consonant-vowel (CV) pairs beginning with either /g/ or /b/. Mice demonstrated the ability to generalize consonant identity across novel vowel contexts and speakers, consistent with true category learning. To our knowledge this is the first demonstration that any animal can generalize consonant identity across both novel vowel contexts and novel speakers. These results indicate that mice can solve the non-invariance problem, and suggest that mice are a suitable model for studying phonetic perception.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Results
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\Large Results}

\subsection{Generalization performance}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% General Performance Figure
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

We began training \Sexpr{n_total} mice to discriminate between consonant-vowel (CV) pairs beginning with either /b/ or /g/ in a two-alternative forced choice task. CV tokens were pitched-shifted up into the mouse hearing range (Fig. \ref{spectro}a-b). Each mouse began training with a pair of tokens (individual recordings) in a single vowel context (ie. /bI/ and /gI/) from a single speaker, and then advanced through stages that progressively introduced new tokens, vowels, and speakers (Fig. \ref{spectro}c-d, see Methods). Training was discontinued in \Sexpr{n_total-n_learners} $(\Sexpr{round(100.0-pct_learn,1)}\%)$ of these mice because their performance on the first stage was not significantly better than chance after two months. The remaining \Sexpr{n_learners} $ (\Sexpr{pct_learn}\%) $ mice progressed through all the training stages to reach a final generalization task, on average in \Sexpr{learn_time} $(\sigma\pm \Sexpr{learn_time_sd})$ weeks (Fig. \ref{spectro}e). This success rate and training duration suggest that the task is difficult but achievable. 


<<fig2,echo=FALSE,include=FALSE,eval=FALSE>>=
fig_2 <- gen_perform_fig(gendat.mouse_type, fig_dir)
ggsave(file=paste(fig_dir,'Figure2_render.pdf',sep=""),fig_2,
       device="pdf",width=3.6,height=6,units="in",
       dpi=500,bg="transparent", useDingbats=FALSE)
#grid.newpage()
#grid.draw(fig_2)
@
\begin{figure}[!ht]
\includegraphics[width=\linewidth]{../figures/Figure2.pdf}
\caption{\label{gen}{\textbf{Generalization accuracy by novelty class.} Mice generalized stop consonant discrimination to novel CV recordings. \textbf{a)} Four types of novelty are possible with our stimuli: novel tokens from the speakers and vowels used in the training set (red), novel vowels (blue), novel speakers (purple), and novel speakers with novel vowels (orange). Tokens in the training set are indicated in black. Colors same throughout. \textbf{b)} Mice that performed better on the training set were better at generalization. Each point shows the performance for a single mouse on a given novelty class, plotted against that mouse's performance on training tokens presented on during the generalization phase (both averaged across the entire generalization phase). Lines show linear regression for each novelty class. \textbf{c)} Mean accuracy for each novelty class (gray lines indicate individual mice). \textbf{d)} Mean accuracy for individual mice (colored bars indicate each novelty class). Error bars in \textbf{d} are 95\% binomial confidence intervals. Mice were assigned one of two sets of training tokens, black and white boxes in \textbf{d}.}}
\end{figure}

We note that this training time is similar to that reported previously for rats $ (14\pm 0.3 $weeks \cite{Engineer2015}$) $. Previous studies have not generally reported success rates. Human infants also vary in the rate and accuracy of their acquisition of phonetic categories \cite{Werker1988}, so we did not expect perfect accuracy from every mouse. The cause of such differences in ability is itself an opportunity for future study.

Generalization is an essential feature of categorical perception. By testing whether mice can generalize their phonetic categorization to novel stimuli, we can distinguish whether mice actually learn phonetic categories or instead just memorize the reward contingency for each training token. Four types of novelty are possible with our stimuli: new tokens from the speakers and vowel contexts used in the training set, new vowels, new speakers, and new vowels from new speakers (colored groups in Fig. \ref{gen}a). In the final generalization stage, we randomly interleaved tokens from each of these novelty classes on 20\% of trials, with the remaining 80\% consisting of tokens from the training set. We interleaved novel tokens with training tokens for two reasons: (1) to avoid a sudden increase in task difficulty, which can degrade performance, and (2) to minimize the possibility that mice could learn each new token by widely separating them in time (on average, generalization tokens were repeated only once every five days).

We looked for 4 hallmarks of generalization: (1) Mice should be able to accurately categorize novel tokens, (2) performance should reflect the quality of the acoustic-phonetic criteria learned in training, (3) performance on novel tokens should be correspondingly worse for tokens that differ more from those in the training set, and (4) accurate categorization of novel tokens should not require additional reinforcement.

All \Sexpr{n_learners} mice were able to categorize tokens of all generalization types with an accuracy significantly greater than chance. We estimated the impact of each generalization class on performance as a fixed factor nested within each mouse as a random factor in a mixed-effects logistic regression (see Methods). The predicted accuracy for each generalization class is shown in Table \ref{lmmtab}, each providing an estimate of the difficulty of that class after accounting for the random effects of individual mice.

Performance on all generalization types was strongly and positively correlated with performance on the training set (Fig. \ref{gen}b, adj. $R^2=\Sexpr{base.gen.r}, F(\Sexpr{base.gen.df[1]}, \Sexpr{base.gen.df[2]}) = \Sexpr{base.gen.f}, p < 0.05)$. If mice were "overfitting," that is, memorizing the training tokens rather than learning categories, then we would expect the opposite (i.e., above some threshold, mice that performed better on the training set would perform correspondingly worse on the generalization set). It appears instead that better prototypes or decision boundaries learned in the training stages allowed better generalization to novel tokens.

<<fig3,echo=FALSE,include=FALSE,eval=FALSE>>=
fig_3 <- learningcurve_fig(gendat.13)
ggsave(file=paste(fig_dir,'Figure3_render.pdf',sep=""),fig_3,
       device="pdf",width=3.6,height=3,units="in",
       dpi=500,bg="transparent", useDingbats=FALSE)
#grid.newpage()
#grid.draw(fig3)
@
\begin{figure}[t]
\includegraphics[width=\linewidth]{../figures/Figure3.pdf}
\caption{\textbf{Learning curve for novel tokens.} Performance for both novel and training set tokens dropped transiently and recovered similarly after the transition to the generalization stage. Presentation 0 corresponds to the transition to the generalization stage. The final ten trials before the transition are shown in the gray dashed box. Mean accuracy and 95\% binomial confidence intervals are collapsed across mice for novel (red, all novelty classes combined) or learned (black) tokens, by number of presentations in the generalization task. Logistic regression of binomial correct/incorrect responses fit to log-transformed presentation number (lines, shading is smoothed standard error).}
\label{lc}
\end{figure}

Mice were better at some types of generalization than others (Fig. \ref{gen}c). The estimates of their relative difficulty (Fig. \ref{gen}c) provide a ranking of the perceptual novelty of the stimulus classes based on their similarity to the training tokens. From easiest to hardest, these were: novel token, novel vowel, novel speaker (which was not significantly more difficult than novel vowel), novel speaker \& vowel. The effects of generalizing to novel vowels and novel speakers were not significantly different from each other, but pairwise comparisons between each of the other types of generalization were (Tukey's method, all $p < 0.001$, also see confidence intervals in Table \ref{lmmtab}).

Although the effect of each generalization type on performance was significantly different between mice $($Likelihood Ratio Test, $\chi^2(\Sexpr{lme.df}) = \Sexpr{lme.ch}, p \ll 0.001)$, they were highly correlated (see Table \ref{lmmtab}). The relative consistency of novelty type difficulty across mice (ie. the correlation of fixed effects, Fig. \ref{gen}c) is striking, but our results cannot distinguish whether it is due to the mice or the stimuli: it is unclear whether the acoustic/phonetic criteria learned by all mice are similarly general, or whether the "cost" of each type of generalization is similar across an array of possible acoustic/phonetic criteria.

<<fig4,echo=FALSE,include=FALSE,eval=FALSE>>=
source(paste(code_dir,"results_plotting_fnxns.R",sep=""))
fig_4 <- heatmap_fig(gendat.vowmus, gendat.tokmus_cast, gendat.novel)
ggsave(file=paste(fig_dir,'Figure4_render.pdf',sep=""),fig_4,
       device="pdf", width=3.6, height=6.75,units="in",
       dpi=500,bg="transparent", useDingbats=FALSE)
#grid.newpage()
#grid.draw(fig_4)
@
\begin{figure}[!t]
\includegraphics[width=\linewidth]{../figures/Figure4.pdf}
\caption{\label{heat}{\textbf{Patterns of individual and group variation.} \textbf{a)}Mean accuracy (color, scale at top) for each mouse (columns) on tokens grouped by consonant, speaker, and vowel (rows). The different training sets (cells outlined with black boxes) led to different patterns of accuracy on the generalization set. \textbf{b)} Ward clustering dendrogram, colored by cluster. \textbf{c)} Training set cohorts differed in bias but not mean accuracy.
\vspace{48pt}
}}
\end{figure}

True generalization requires that one set of discrimination criteria can be successfully applied to novel cases without reinforcement. It is possible that the mice were instead able to rapidly learn the reward contingency of novel tokens during the generalization stage. If mice were learning rapidly rather than generalizing, this would predict that novel token performance (1) would be indistinguishable from chance on the first presentation, and (2) would increase relative to performance on already-learned tokens with repeated presentations.

Performance on the first presentation of novel tokens was significantly greater than chance $($Fig. 3, all mice, all tokens from all novelty classes: one-sided binomial test, $n=\Sexpr{first_n}, P_{correct} = \Sexpr{first_pct},$ lower 95\% CI $= \Sexpr{first_ci}, p \ll 0.001;$ all mice, worst novelty class:  $n=\Sexpr{hard_n}, P_{correct} = \Sexpr{hard_pct},$ lower 95\% CI $= \Sexpr{hard_ci}, p < 0.001)$. This demonstrates that mice were able to generalize immediately without additional reinforcement. Although performance on novel tokens did increase with repetition, so did performance on training tokens (Fig. \ref{lc}). We noted that performance on all tokens (both novel and previously learned tokens) transiently dropped after each transition between task stages, suggesting a non-specific effect of an increase in task difficulty. To distinguish an increase in performance due to learning from an increase due to acclimating to a change in the task, we compared performance on generalization and training tokens over the first 40 presentations of each token. If the mice were learning the generalization tokens, the increase in performance with repeated presentations should be significantly greater than that of the already trained tokens.

Performance was well fit by a logistic regression of correct/incorrect responses from each mouse against the novelty of a token (trained vs. novel tokens), and the number of times it had been presented (Fig. \ref{lc}). The effect of the number of presentations on accuracy was not significantly different for novel tokens compared to trained tokens (interaction between novelty and the number of presentations: Wald test, $z = \Sexpr{ts_irax$z}, 95\% CI = [\Sexpr{ts_irax$cilo}, \Sexpr{ts_irax$cihi}], p=\Sexpr{ts_irax$p})$. This was also true when the model was fit with with the generalization types themselves rather than trained vs. novel tokens (most significant interaction, generalization to novel speakers x number of presentations: Wald test, $z = \Sexpr{ts_type$z}, 95\% CI = [\Sexpr{ts_type$cilo}, \Sexpr{ts_type$cihi}], p=\Sexpr{ts_type$p})$ and with different numbers of repetitions $($10: $z = \Sexpr{ts_10$z}, 95\% CI = [\Sexpr{ts_10$cilo}, \Sexpr{ts_10$cihi}], p=\Sexpr{ts_10$p};$ 20: $z = \Sexpr{ts_20$z}, 95\% CI = [\Sexpr{ts_20$cilo}, \Sexpr{ts_20$cihi}], p=\Sexpr{ts_20$p})$. This indicates that the asymptotic increase in performance on novel tokens was a general effect of adapting to a change in the task rather than a learning period for the novel stimuli.

In summary, the behavior of the mice is consistent with an ability to generalize some learned acoustic criteria to novel stimuli. It is unlikely that the mice rapidly learned the novel tokens because (1) performance on the first presentation of novel tokens was significantly above chance, (2) performance on subsequent presentations of novel tokens did not improve compared to trained tokens, and (3) learning each token would have to take place over unrealistically long timescales: there were an average of 2355 trials (5 days) between the first and second presentation of each novel token.

\subsection{Training Set Differences}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Heatmap Figure
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
One strength of studying phonetic perception in animal models is the ability to precisely control exposure to speech sounds. To test whether and how the training history impacted the pattern of generalization, we divided mice into two cohorts trained with different sets of speech tokens. In the first cohort $ (n = \Sexpr{n_cohort1}$ mice$) $, mice were trained with tokens from speakers 1 and 2 (speaker number in Fig. \ref{heat}a), whereas the second cohort $ (n = \Sexpr{n_cohort2}$ mice$) $ were trained on speakers 4 and 5.

The two training cohorts had significantly different patterns of which tokens were accurately categorized (Fig. \ref{heat}a, Likelihood-Ratio test, regression of mean accuracy on tokens with and without token x cohort interaction: $\chi^2_{\Sexpr{cohort$df}}, p \ll 0.001)$. Put another way, accuracy patterns were markedly similar within training cohorts: cohort differences accounted for fully \Sexpr{cohort$pct_var}\% of all accuracy variance (sum of squared-error) between tokens.

Mice from the second training cohort were far more likely to report novel tokens as a /g/ than the first cohort (Fig. \ref{heat}b), an effect that was not significantly related to their overall accuracy $(b=\Sexpr{bias_acc$b}, t(\Sexpr{bias_acc$df})=\Sexpr{bias_acc$t}, p=\Sexpr{bias_acc$p} )$. Since the only difference between these mice were the tokens they were exposed to during training (they were trained contemporaneously in the same boxes),  we interpret this response bias as the influence of the training tokens on whatever acoustic cues the mice had learned in order to perform the generalization task. This suggests that the acoustic properties of training set 2 caused the /g/ "prototype" to be overbroad.

We searched for additional sub-cohort structure with hierarchical clustering (Ward's Method, dendrogram in fig \ref{heat}b). Within each training cohort there appeared to be two additional clusters of accuracy patterns. Though our sample size was too small to meaningfully interpret these clusters, they raise the possibility that even when trained using the same set of sounds mice might learn multiple sets of rules to distinguish between consonant classes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Acoustic-Behavioral Correlates
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Acoustic-behavioral correlates}

<<fig5,echo=FALSE,include=FALSE,eval=FALSE>>=
fig_5 <- acoustic_fig(gendat.tokf2)
ggsave(paste(fig_dir, "Figure5_render.pdf",sep=""), plot=fig_5, device="pdf",
         width=2.5, height=5, units="in", useDingbats=FALSE)

@
\begin{figure}[!t]
\includegraphics[width=\linewidth]{../figures/Figure5.pdf}
\caption{\textbf{Acoustic-Behavior Correlates} F2 Onset-Vowel transitions do not explain observed response patterns. \textbf{a)} Locus equations relating F2 at burst onset and vowel steady state (sustained) for each token (points), split by consonant (colors, same as \textbf{b)}). \textbf{b)} As the difference of a token's distance from the ideal /g/ and /b/ locus equation lines increased (x axis, greater distance from /g/, smaller distance from /b/), /b/ tokens obeyed the predicted categorization while /g/ tokens did not (slopes of colored lines). \vspace{24pt}}
\label{acoustic}
\end{figure}

Humans can flexibly use several acoustic features such as burst spectra and formant transitions to discriminate plosive consonants, and we wondered to what extent mice were sensitive to these same features.

One dominant acoustic cue for place of articulation in stop consonants is the transition of the second formant following the plosive burst \cite{Wright2004,Sussman1998,Lindblom2012}. Formant transitions are complex and dependent on vowel context, but tokens for a given place of articulation cluster around a line -- or "locus equation" -- relating F2 frequency at release to its mid-vowel steady-state \cite{Sussman1998,Lindblom2012} (Fig. \ref{acoustic}a).  If mice were sensitive to this cue, the distance from both locus equation lines should influence response. For example, a /g/ token between the locus equation lines should have a greater rate of misclassification than a token at an equal distance above the red /g/ line. Therefore we tested how classification depended on the difference of distances from each line (/g/ distance - /b/ distance, which we refer to as "locus difference").

Mean responses to tokens (ranging from 100\% /g/ - 100\% /b/) were correlated with locus differences (black line, Fig. \ref{acoustic}b). However, it is important to note that this correlation does not necessarily demonstrate that mice relied on this acoustic cue. Because multiple acoustic features are correlated with consonant identity, performance that is correlated with one such cue would also be correlated with all the others. The mice learned some acoustic property of the consonant classes, and since the acoustic features are all highly correlated with one another, they are all likely to correlate with mean responses.

To distinguish whether mice specifically relied on F2 locus distance, we therefore measured the marginal effect of this acoustic cue within a consonant class. This is shown by the slopes of the red and blue lines in Fig. \ref{acoustic}b. For example, is a /g/ token that is further away from the blue /b/ line more likely to be identified as a /g/ than one very near the /b/ line? Mean responses to /g/ tokens were negatively correlated with locus distance (Mean response /g/ to /b/ between 0 and 1, $b=\Sexpr{acoust$g_b}kHz, 95\% CI = [\Sexpr{acoust$g_b_lo}, \Sexpr{acoust$g_b_hi}], p \ll 0.001)$. In other words, tokens that should have been more frequently confused with /b/ were actually more likely to be classified as /g/. Note the red points at locus distance of zero in Fig. \ref{acoustic}b: these tokens have an equal distance from both the /b/ and /g/ locus equation prototypes but are some of the most accurately categorized /g/ tokens. /b/ tokens obeyed the predicted direction of locus distance $ (b=\Sexpr{acoust$b_b}, 95\% CI = [\Sexpr{acoust$b_b_lo}, \Sexpr{acoust$b_b_hi}], p \ll 0.001) $, but the effect was very small: moving one standard deviation $ (\sigma_{/b/}=\Sexpr{acoust$b_sd}kHz) $ towards the /g/ line only changed responses by \Sexpr{round(acoust$b_b*acoust$b_sd, 3)*100}\%. These results suggest that mice did not rely on F2 transitions to categorize these consonants.

We repeated this analysis separately for each training cohort to test whether the two cohorts could have developed different acoustic templates that better explained their response patterns. We derived cohort-specific locus-equation lines and distances using only the tokens from each of their respective training sets. These models were qualitatively similar to the model that included all tokens and mice and did not improve the model fit $($Cohort 1: /g/: $b=\Sexpr{acoust_c1$g_b}, 95\% CI = [\Sexpr{acoust_c1$g_b_lo}, \Sexpr{acoust_c1$g_b_hi}]$, /b/: $b=\Sexpr{acoust_c1$b_b}, 95\% CI = [\Sexpr{acoust_c1$b_b_lo}, \Sexpr{acoust_c1$b_b_hi}]$; Cohort 2: /g/: $b=\Sexpr{acoust_c2$g_b}, 95\% CI = [\Sexpr{acoust_c2$g_b_lo}, \Sexpr{acoust_c2$g_b_hi}]$, /b/: $b=\Sexpr{acoust_c2$b_b}, 95\% CI = [\Sexpr{acoust_c2$b_b_lo}, \Sexpr{acoust_c2$b_b_hi}]$).

We conclude that while our stimulus set had the expected F2 formant transition structure, this was unable to explain the behavioral responses we observed both globally and within training cohorts. There are, of course, many more possible acoustic parameterizations to test, but the failure of F2 transitions to explain our behavioral data is notable because of its perceptual dominance in humans and its common use in parametrically synthesized speech sounds. This demonstrates one advantage of using natural speech sounds: mice trained on synthesized speech that varied parametrically only on F2 transitions would likely show sensitivity to this cue, but this does not mean that mice show the same feature sensitivity when trained with natural speech. Preserving the complexity of natural speech stimuli is important for developing a general understanding of auditory category learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Discussion
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\Large Discussion}

These results demonstrate that mice are capable of learning and generalizing phonetic categories. Indeed, this is the first time to our knowledge that mice have been trained to discriminate between any classes of natural, non-species-specific sounds. Thus mice join a number of model organisms that have demonstrated categorical learning with speech sounds \cite{Kluender1987,Lotto1997,Kluender2000,Kuhl1978,Engineer2015,Kuhl1983,Dooling1995}, making a new suite of genetic and electrophysiological tools available for phonetic research.

Two subgroups of our mice that were trained using different sets of speech tokens demonstrated distinct patterns of consonant identification, presumably reflecting differences in underlying acoustic prototypes. The ability to precisely control exposure to speech sounds provides an opportunity to probe the neurocomputational constraints that govern the possible solutions to consonant identification.

Here we opted to use naturally recorded speech tokens in order to demonstrate that mice could perform a "hard version" of phonetic categorization that preserves the full complexity of the speech sounds and avoids \textit{a priori} assumptions about the parameterization of phonetic contrasts. Although our speech stimuli had the expected F2 formant transition structure, that did not explain the response patterns of our mice. This suggests that the acoustic rules that mice learned are different from those that would be learned from synthesized speech varying only along specifically chosen parameters.

Future experiments using parametrically synthesized speech sounds are a critical next step, and will support a qualitatively different set of inferences. Being able to carefully manipulate reduced speech sounds is useful to probe the acoustic cue structure of learned phonetic categories, but the reduction in complexity that makes them useful also makes it correspondingly more difficult to probe the learning and recognition mechanisms for a perceptual category that is defined by multiple imperfect, redundant cues. It is possible that the complexity of natural speech may have caused our attrition rate to be higher, and task performance lower, than other sensory-driven tasks. Neither of those concerns, however, detracts from the possibility for the mouse to shed mechanistic insight on phonetic perception. Indeed, error trials may provide useful neurophysiological data about how and why the auditory system fails to learn or perceive phonetic categories.

We hope in future experiments to directly test predictions made by neurolinguistic models regarding phonetic acquisition and discrimination. For example, one notable model proposes that consonant perception relies on combination-sensitive neurons that selectively respond to specific combinations of acoustic features \cite{Sussman1998}. This model predicts that mice trained to discriminate stop consonants would have neurons selective for the feature combinations that drive phoneme discrimination, perhaps in primary or higher auditory cortical areas. Combination-selective neurons have been observed in A1 \cite{Sadagopan2009,Wang2005a}, and speech training can alter the response properties of A1 neurons in rats \cite{Engineer2015}, but it is unclear whether speech training induces combination-selectivity that would facilitate phonetic discrimination.

The ability to record from hundreds of neurons in awake behaving animals using tetrode electrophysiology or 2-photon calcium imaging presents exciting opportunities to test predictions like these. Should some candidate population of cells be found with phonetic selectivity, the ability to optogenetically activate or inactivate specific classes of neurons (such as excitatory or inhibitory cell types, or specific projections from one region to another) could shed light on the circuit computations and transformations that confer that selectivity.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Methods
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\Large Methods}

\subsection{Animals}

All procedures were performed in accordance with National Institutes of Health guidelines, as approved by the University of Oregon Institutional Animal Care and Use Comittee.

We began training 23 C57BL/6J mice to discriminate and generalize stop consonants in CV (consonant-vowel) pairs. \Sexpr{n_total-n_learners} mice failed to learn the task (see Training, below). \Sexpr{n_learners} mice $ (\Sexpr{pct_learn}\%) $ progressed through all training stages and reached the generalization task in an average \Sexpr{learn_time} $(\sigma = \Sexpr{learn_time_sd})$ weeks. Mean age at training onset was \Sexpr{age_onset} $(\sigma = \Sexpr{age_onset_sd})$ weeks, and at discontinuation of training was \Sexpr{age_offset} $(\sigma = \Sexpr{age_offset_sd})$ weeks. Sex did not significantly affect the probability of passing or failing training (Fisher's Exact Test: $p = \Sexpr{chi_sex})$, nor did the particular behavioral chamber used for training $ (p = \Sexpr{chi_box}) $, nor age at the start of training (Logistic regression: $z = \Sexpr{demog_age_z}, p=\Sexpr{demog_age_p})$. Although this task was difficult, our training time $(14\pm 0.3$ weeks as in \cite{Engineer2015}), and accuracy (generalization: \Sexpr{kluender_acc}\%\cite{Kluender1987}, training tokens only: $84.1\%$ \cite{Engineer2015}) are similar to comparable experiments in other animals.

\subsection{Speech stimuli}

Speech stimuli were recorded in a sound-attenuating booth with a head-mounted microphone attached to a Tascam DR-100mkII handheld recorder sampling at 96kHz/24bit. Each speaker produced a set of 3 recordings (tokens) of each of 12 CV pairs beginning with either /b/ or /g/, and ending with /I/, /o/, /a/, /\ae/, /$\varepsilon$/, /u/. To reduce a slight hiss that was present in the recordings, they were denoised using a Daubechies wavelet with two vanishing moments in MATLAB. The typical human hearing range is 20 Hz - 20 kHz, whereas the mouse hearing range is ~1 kHz - 80 kHz \cite{Radziwon2009}. The $F_0$ of our recorded speech sounds ranged from 100 - 200 Hz, which is well below the lower frequency limit of the mouse hearing range. We therefore pitch shifted all stimuli upwards by 10x (3.3 octaves) in MATLAB \cite{Mathworks}. This shifted all spectral information equally upwards into an analogous part of mouse hearing range while preserving temporal information unaltered.

Tokens from five speakers (one male - speaker 1 throughout, four female - speakers 2-5 throughout) were used. Three vowel contexts (/\ae/, /$\varepsilon$/, and /u/) were not recorded from one speaker. It is unlikely that this had any effect on our results, as our primary claims are based on the ability to generalize at all, rather than generalization to tokens from a particular speaker. Tokens were normalized to a common mean amplitude, but were otherwise unaltered to preserve natural variation between speakers --- indeed, preserving such variation was the reason for using naturally recorded rather than synthesized speech.

Formant frequency values were measured manually using Praat \cite{Boersma2001}. F2 at onset was measured at its center as soon as it was discernible, typically within 20 ms of burst onset, and at vowel steady-state, typically 150-200ms after burst onset.

\subsection{Training}

We trained mice to discriminate between CV pairs beginning with /b/ or /g/ in a two-alternative forced choice task. Training sessions lasted approximately 1 hour, 5 days a week. Each custom-built sound-attenuating training chamber contained two free-field JBL Duet speakers for stimulus presentation with a high-frequency rolloff of 34 kHz, and a smaller 15 x 30 cm plastic box with three "lick ports." Each lick port consisted of a water delivery tube and an IR beam-break sensor mounted above the tube. Beam breaks triggered water delivery by actuating a solenoid valve. Water-restricted mice were trained to initiate each trial with an unrewarded lick at the center port, which started playback of a randomly selected stimulus, and then to indicate their stimulus classification by licking at one of the ports on either side. Tokens beginning with /g/ were always on the left, with /b/ on the right. Two cohorts were trained on two separate sets of tokens. Training set 1 started with speaker 1 (Fig. 4a) and had speaker 2 introduced on the fourth stage, where Training set 2 started training with speaker 5 and had speaker 4 introduced on the fourth stage. Correct classifications received \SI{\sim10}{\micro\liter} water rewards, and incorrect classifications received a 5s time-out that included a mildly aversive 60 dB SPL white noise burst.

Training advanced in stages that progressively increased the number of tokens, vowel contexts, and speakers. Mice first learned a simple pure-tone frequency discrimination task to familiarize them with the task and shape their behavior; the tones were gradually replaced with the two CV tokens of the first training stage. CV discrimination training proceeded in 5 stages outlined in Table 2. Mice automatically graduated from each stage when 75\% of the preceding 300 trials were answered correctly. In a few cases, a mouse was returned to the previous stage if its performance fell to chance for more than a week after graduating. Training was discontinued after two to three months if performance in the first stage never rose above chance. Mice that reached the final training stage were allowed to reach asymptotic performance, and then advanced to a generalization task.%

In the generalization task, stimuli from the set of all possible speakers, vowel contexts, and tokens (140 total, not including the stage 5 stimulus set) were randomly presented on 20\% of trials and the stage 5 stimulus set was used on the remaining 80\%. Training tokens were drawn from a uniform random distribution so that each was equally likely to occur during both the stage 5 training and generalization phases. Novel tokens were drawn uniformly at random by their generalization class, but since there were unequal numbers of tokens in each class (Novel token only: 16 tokens, Novel Vowel: 36, Novel Speaker: 54, Novel Speaker + Vowel: 54), tokens in each class had an unequal number of presentations. We note that the logistic regression analysis with restricted maximum likelihood that we used is robust to unequal sample sizes \cite{Patterson1971}.

\subsection{Data analysis}
Data were excluded from days on which a mouse had a $>10\%$ drop in accuracy from their mean performance on the previous day $(44/636 = 7\%$ of sessions$)$. Anecdotally, mice are sensitive to environmental conditions (e.g., thunderstorms), so even though all efforts were made to minimize variation between days, even the best performing mice had "bad days" where they temporarily fell to near-chance performance and exhibited strong response bias. We thus assume these "bad days" were the result of temporary environmental or other performance issues, and were unrelated to the difficulty of the task itself. 

All analyses were performed in R (\Sexpr{r_version})\cite{Team2016} using RStudio (\Sexpr{rstud_version})\cite{Team2015}. Generalization performance was modeled using a logistic generalized linear mixed model (GLMM) using the R package "lme4"\cite{Bates2015}. Binary correct/incorrect responses were fit hierarchically to models of increasing complexity (see Table \ref{hiertab}), with a final model consisting of the generalization class (as in Fig. 2a: training tokens, novel tokens from the speakers and vowels in the training set, novel speaker, novel vowel, and novel speaker and vowel) as a fixed effect with random slopes and intercepts nested within each mouse as a random effect. There was no evidence of overdispersion (i.e., deviance $\approx$ degrees of freedom, or less than  $\sim$ 2 times degrees of freedom), and the profile of the model showed that the deviances by each fixed effect were approximately normal. Accordingly, we report Wald confidence intervals. We also computed bootstrapped confidence intervals, which had only minor disagreement with the Wald confidence intervals and agreed with our interpretation in the text.

Clustering was performed with the "cluster"\cite{Maechler2017} package. Ward clustering split the mice into two notable clusters, which are plotted in Fig. \ref{heat}.

We estimated locus equations relating F2 onset and F2 vowel using total least squares linear regression. The locus equations of the /b/ and /g/ tokens accounted for \Sexpr{bpc_var}\% and \Sexpr{gpc_var}\% of the variance in the F2 measurements of our tokens, respectively.

Spectrograms in Figure \ref{spectro}a were computed with the "spectrogram" function in MATLAB 2017b, and power spectra in Figure \ref{spectro}b were computed with the "pwelch" function in MATLAB 2018b with the same window and overlap as \ref{spectro}a spectrograms.

The remaining analyses are described in the text and used the "binom"\cite{Sundar2014}, "reshape"\cite{Wickham2007}, and "plyr"\cite{Wickham2011} packages. Data visualization and tabulation was performed with the "ggplot2,"\cite{Wickham2009} and "xtable"\cite{Dahl2016} packages.


\onecolumn
{\small\setstretch{1.0}\bibliography{SaundersWehr_JASA2019_retypeset}{}}

\bibliographystyle{plain}{}


%\bibliographystyle{jasanum2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\section{Tables}

\begin{table}[ht]
\caption{\label{lmmtab}{\textbf{Impact of each generalization class on performance.} Accuracy values provide an estimate of the difficulty of that class after accounting for the random effects of individual mice. Accuracies are logistic GLMM coefficients transformed from logits, and model coefficients are logit differences from training set accuracy, which was used as an intercept. Correlation values are between fixed effects (novelty classes) across random effects (mice). * indicates significance ($p(>|z|)\ll.001$).}}
%\begin{ruledtabular}
\begin{tabular}{@{}rrr|llll@{}}
<<regression_tab,results="asis",echo=FALSE,cache=TRUE,dependson="regression_calcs">>=
tab <- xtable(lm_df,caption="Note: * = <0.001 Wald z-test")
align(tab) <- "rrr|llll"
digits(tab) <- c(0, 3, 3, 2, 2, 2, 2)
label(tab) <- "lmmtab"
caption(tab) <- "Coefficients and correlations from linear mixed model"
print(tab,floating=FALSE,latex.environments=NULL)
@
\end{tabular}
%\end{ruledtabular}
\end{table}
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
\label{trainsteps}
\caption{\textbf{Token structure of training stages}}
%\begin{ruledtabular}
\begin{tabular}{rccc}
Stage & Speakers & Vowels & Total Tokens\\
\midrule
1 & 1 & 1 & 2\\
2 & 1 & 1 & 4\\
3 & 1 & 2 & 6\\
4 & 2 & 2 & 12\\
5 & 2 & 3 & 20\\
Generalization & 5 & 6 & 160 (20 training, 140 novel)\\
\end{tabular}
%\end{ruledtabular}

\end{table}
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{table}
\label{hiertab}
\caption{\textbf{hierarchical GLMM:} To reach the appropriate complexity of model, we first modeled correct/incorrect answers as a function of each mouse as a fixed effect (row 1), then added the generalization type (as in Fig. \ref{gen}) as a fixed effect (row 2), and finally modeled generalization type as a fixed effect nested within each mouse as a random effect (row 3). Since the final model had the best fit, it was used in all reported analyses related to the GLMM.}
%\begin{ruledtabular}
\begin{tabular}{@{\extracolsep{5pt}} rrrrr}
<<hiertab,results="asis",echo=FALSE>>=
nosan <- function(x){return(x)}

tab <- xtable(novtypes.aov)
align(tab) <- "rrrrr"
digits(tab) <- c(0,0,2,0,2)
label(tab) <- "hiertab"
print(tab,floating=FALSE,latex.environments=NULL,sanitize.text.function=nosan)
@
\end{tabular}
%\end{ruledtabular}
\end{table}
\clearpage

\end{document}



\end{document}
