Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Werker1988,
author= {Werker, Janet F and Lalonde, Chris E},
title = {{Cross-language speech perception: Initial capabilities and developmental change.}},
journal = {Developmental psychology},
volume = {24},
number = {5},
pages = {672},
year = {1988}
}

@article{Wright2004,
author = {Wright, R.},
title = {{A review of perceptual cues and cue robustness}},
journal = {Phonetically based phonology},
pages = {34--57},
year = {2004}
}

@article{Mathworks,
title = {{Pitch Shifting and Time Dilation Using a Phase Vocoder in MATLAB}},
url = {https://www.mathworks.com/help/audio/examples/pitch-shifting-and-time-dilation-using-a-phase-vocoder-in-matlab.html}
}


@article{Wickham2007,
author = {Wickham, H.},
title = {{Reshaping data with the reshape package}},
journal = {Journal of Statistical Software} ,
volume = {21},
number = {12},
year = {2007},
url = {http://www.jstatsoft.org/v21/i12/paper}
}


@article{Weible2014,
abstract = {Auditory cortex is necessary for the perceptual detection of brief gaps in noise, but is not necessary for many other auditory tasks such as frequency discrimination, prepulse inhibition of startle responses, or fear conditioning with pure tones. It remains unclear why auditory cortex should be necessary for some auditory tasks but not others. One possibility is that auditory cortex is causally involved in gap detection and other forms of temporal processing in order to associate meaning with temporally structured sounds. This predicts that auditory cortex should be necessary for associating meaning with gaps. To test this prediction, we developed a fear conditioning paradigm for mice based on gap detection. We found that pairing a 10 or 100 ms gap with an aversive stimulus caused a robust enhancement of gap detection measured 6 h later, which we refer to as fear potentiation of gap detection. Optogenetic suppression of auditory cortex during pairing abolished this fear potentiation, indicating that auditory cortex is critically involved in associating temporally structured sounds with emotionally salient events.},
annote = {From Duplicate 1 (Auditory cortex is required for fear potentiation of gap detection. - Weible, Aldis P; Liu, Christine; Niell, Cristopher M; Wehr, Michael)

10.1523/JNEUROSCI.3408-14.2014},
author = {Weible, Aldis P and Liu, Christine and Niell, Cristopher M and Wehr, Michael},
doi = {10.1523/JNEUROSCI.3408-14.2014},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Weible et al. - 2014 - Auditory cortex is required for fear potentiation of gap detection(3).pdf:pdf;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Weible et al. - 2014 - Auditory cortex is required for fear potentiation of gap detection(4).pdf:pdf},
isbn = {1529-2401 (Electronic)$\backslash$r0270-6474 (Linking)},
issn = {1529-2401},
journal = {The Journal of neuroscience},
keywords = {auditory cortex,fear conditioning,gap detection,optogenetics},
number = {46},
pages = {15437--45},
pmid = {25392510},
title = {{Auditory cortex is required for fear potentiation of gap detection.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25392510},
volume = {34},
year = {2014}
}
@article{Giraud2012a,
author = {Giraud, Anne-Lise and Poeppel, David},
doi = {10.1038/nn.3063},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {mar},
number = {4},
pages = {511--517},
publisher = {Nature Research},
title = {{Cortical oscillations and speech processing: emerging computational principles and operations}},
url = {http://www.nature.com/doifinder/10.1038/nn.3063},
volume = {15},
year = {2012}
}
@article{Stevens1978a,
abstract = {In a series of experiments, identification responses for place of articulation were obtained for synthetic stop consonants in consonant–vowel syllables with different vowels. The acoustic attributes of the consonants were systematically manipulated, the selection of stimulus characteristics being guided in part by theoretical considerations concerning the expected properties of the sound generated in the vocal tract as place of articulation is varied. Several stimulus series were generated with and without noise bursts at the onset, and with and without formant transitions following consonantal release. Stimuli with transitions only, and with bursts plus transitions, were consistently classified according to place of articulation, whereas stimuli with bursts only and no transitions were not consistently identified. The acoustic attributes of the stimuli were examined to determine whether invariant properties characterized each place of atriculation independent of vowel context. It was determined that the ...},
author = {Stevens, K. N. and Blumstein, S. E.},
doi = {10.1121/1.382102},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {nov},
number = {5},
pages = {1358--1368},
publisher = {Acoustical Society of America},
title = {{Invariant cues for place of articulation in stop consonants}},
url = {http://asa.scitation.org/doi/10.1121/1.382102},
volume = {64},
year = {1978}
}
@inproceedings{Salakhutdinov2011a,
author = {Salakhutdinov, Ruslan and Torralba, Antonio and Tenenbaum, Josh},
booktitle = {CVPR 2011},
doi = {10.1109/CVPR.2011.5995720},
file = {::},
isbn = {978-1-4577-0394-2},
month = {jun},
pages = {1481--1488},
publisher = {IEEE},
title = {{Learning to share visual appearance for multiclass object detection}},
url = {http://ieeexplore.ieee.org/document/5995720/},
year = {2011}
}
@article{Bornkessel-Schlesewsky2015,
author = {Bornkessel-Schlesewsky, Ina and Schlesewsky, Matthias and Small, Steven L. and Rauschecker, Josef P.},
doi = {10.1016/j.tics.2014.12.008},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
month = {mar},
number = {3},
pages = {142--150},
title = {{Neurobiological roots of language in primate audition: common computational properties}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661314002757},
volume = {19},
year = {2015}
}
@unpublished{Gagnepain2012,
abstract = {Humans can recognize spoken words with unmatched speed and accuracy. Hearing the initial portion of a word such as “formu{\ldots}” is sufficient for the brain to identify “formula” from the thousands of other words that partially match [1–6]. Two alternative computational accounts propose that partially matching words (1) inhibit each other until a single word is selected (“formula” inhibits “formal” by lexical competition [7–9]) or (2) are used to predict upcoming speech sounds more accurately (segment prediction error is minimal after sequences like “formu{\ldots}” [10–12]). To distinguish these theories we taught participants novel words (e.g., “formubo”) that sound like existing words (“formula”) on two successive days [13–16]. Computational simulations show that knowing “formubo” increases lexical competition when hearing “formu{\ldots}”, but reduces segment prediction error. Conversely, when the sounds in “formula” and “formubo” diverge, the reverse is observed. The time course of magnetoencephalographic brain responses in the superior temporal gyrus (STG) is uniquely consistent with a segment prediction account. We propose a predictive coding model of spoken word recognition in which STG neurons represent the difference between predicted and heard speech sounds. This prediction error signal explains the efficiency of human word recognition and simulates neural responses in auditory regions.},
author = {Gagnepain, Pierre and Henson, Richard N. and Davis, Matthew H.},
booktitle = {Current Biology},
doi = {10.1016/j.cub.2012.02.015},
file = {::},
issn = {09609822},
number = {7},
pages = {615--621},
title = {{Temporal Predictive Codes for Spoken Words in Auditory Cortex}},
volume = {22},
year = {2012}
}
@article{Patterson1971,
author = {Patterson, H. D. and Thompson, R.},
doi = {10.2307/2334389},
issn = {00063444},
journal = {Biometrika},
month = {dec},
number = {3},
pages = {545},
title = {{Recovery of Inter-Block Information when Block Sizes are Unequal}},
url = {http://www.jstor.org/stable/2334389?origin=crossref},
volume = {58},
year = {1971}
}
@article{Ashby2005,
abstract = {Much recent evidence suggests some dramatic differences in the way people learn perceptual categories, depending on exactly how the categories were constructed. Four different kinds of category-learning tasks are currently popular—rule-based tasks, information-integration tasks, prototype distortion tasks, and the weather prediction task. The cognitive, neuropsychological, and neuroimaging results obtained using these four tasks are qualitatively different. Success in rule-based (explicit reasoning) tasks depends on frontal-striatal circuits and requires working memory and executive attention. Success in information-integration tasks requires a form of procedural learning and is sensitive to the nature and timing of feedback. Prototype distortion tasks induce perceptual (visual cortical) learning. A variety of different strategies can lead to success in the weather prediction task. Collectively, results from these four tasks provide strong evidence that human category learning is mediated by multiple, qua...},
author = {Ashby, F. Gregory and Maddox, W. Todd},
doi = {10.1146/annurev.psych.56.091103.070217},
file = {::},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {decision bound,exemplar,multiple systems,prototype,striatum},
month = {feb},
number = {1},
pages = {149--178},
publisher = { Annual Reviews },
title = {{Human Category Learning}},
url = {http://www.annualreviews.org/doi/10.1146/annurev.psych.56.091103.070217},
volume = {56},
year = {2005}
}
@article{Polley2006,
abstract = {The primary sensory cortex is positioned at a confluence of bottom-up dedicated sensory inputs and top-down inputs related to higher-order sensory features, attentional state, and behavioral reinforcement. We tested whether topographic map plasticity in the adult primary auditory cortex and a secondary auditory area, the suprarhinal auditory field, was controlled by the statistics of bottom-up sensory inputs or by top-down task-dependent influences. Rats were trained to attend to independent parameters, either frequency or intensity, within an identical set of auditory stimuli, allowing us to vary task demands while holding the bottom-up sensory inputs constant. We observed a clear double-dissociation in map plasticity in both cortical fields. Rats trained to attend to frequency cues exhibited an expanded representation of the target frequency range within the tonotopic map but no change in sound intensity encoding compared with controls. Rats trained to attend to intensity cues expressed an increased proportion of nonmonotonic intensity response profiles preferentially tuned to the target intensity range but no change in tonotopic map organization relative to controls. The degree of topographic map plasticity within the task-relevant stimulus dimension was correlated with the degree of perceptual learning for rats in both tasks. These data suggest that enduring receptive field plasticity in the adult auditory cortex may be shaped by task-specific top-down inputs that interact with bottom-up sensory inputs and reinforcement-based neuromodulator release. Top-down inputs might confer the selectivity necessary to modify a single feature representation without affecting other spatially organized feature representations embedded within the same neural circuitry.},
author = {Polley, D. B.},
doi = {10.1523/JNEUROSCI.3771-05.2006},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Polley, Steinberg, Merzenich - 2006 - Perceptual learning directs auditory cortical map reorganization through top-down influences(2).pdf:pdf},
isbn = {1529-2401 (Electronic)},
issn = {0270-6474},
journal = {Journal of Neuroscience},
keywords = {attention,conditioning,cortex,plasticity,reward,topographic map},
number = {18},
pages = {4970--4982},
pmid = {16672673},
title = {{Perceptual Learning Directs Auditory Cortical Map Reorganization through Top-Down Influences}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3771-05.2006},
volume = {26},
year = {2006}
}
@article{Mottonen2006,
abstract = {The left superior temporal cortex shows greater responsiveness to speech than to non-speech sounds according to previous neuroimaging studies, suggesting that this brain region has a special role in speech processing. However, since speech sounds differ acoustically from the non-speech sounds, it is possible that this region is not involved in speech perception per se, but rather in processing of some complex acoustic features. "Sine wave speech" (SWS) provides a tool to study neural speech specificity using identical acoustic stimuli, which can be perceived either as speech or non-speech, depending on previous experience of the stimuli. We scanned 21 subjects using 3T functional MRI in two sessions, both including SWS and control stimuli. In the pre-training session, all subjects perceived the SWS stimuli as non-speech. In the post-training session, the identical stimuli were perceived as speech by 16 subjects. In these subjects, SWS stimuli elicited significantly stronger activity within the left posterior superior temporal sulcus (STSp) in the post- vs. pre-training session. In contrast, activity in this region was not enhanced after training in 5 subjects who did not perceive SWS stimuli as speech. Moreover, the control stimuli, which were always perceived as non-speech, elicited similar activity in this region in both sessions. Altogether, the present findings suggest that activation of the neural speech representations in the left STSp might be a pre-requisite for hearing sounds as speech. ?? 2005 Elsevier Inc. All rights reserved.},
author = {M{\"{o}}tt{\"{o}}nen, Riikka and Calvert, Gemma A. and J{\"{a}}{\"{a}}skel{\"{a}}inen, Iiro P. and Matthews, Paul M. and Thesen, Thomas and Tuomainen, Jyrki and Sams, Mikko},
doi = {10.1016/j.neuroimage.2005.10.002},
file = {::},
isbn = {1053-8119 (Print)$\backslash$n1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Sine wave speech,Speech,Superior temporal sulcus,fMRI},
month = {apr},
number = {2},
pages = {563--569},
pmid = {16275021},
title = {{Perceiving identical sounds as speech or non-speech modulates activity in the left posterior superior temporal sulcus}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811905007676},
volume = {30},
year = {2006}
}
@article{Engineer2008,
abstract = {Neural activity in the cerebral cortex can explain many aspects of sensory perception. Extensive psychophysical and neurophysiological studies of visual motion and vibrotactile processing show that the firing rate of cortical neurons averaged across 50-500 ms is well correlated with discrimination ability. In this study, we tested the hypothesis that primary auditory cortex (A1) neurons use temporal precision on the order of 1-10 ms to represent speech sounds shifted into the rat hearing range. Neural discrimination was highly correlated with behavioral performance on 11 consonant-discrimination tasks when spike timing was preserved and was not correlated when spike timing was eliminated. This result suggests that spike timing contributes to the auditory cortex representation of consonant sounds.},
author = {Engineer, Crystal T and Perez, Claudia A and Chen, YeTing H and Carraway, Ryan S and Reed, Amanda C and Shetake, Jai A and Jakkamsetti, Vikram and Chang, Kevin Q and Kilgard, Michael P},
doi = {10.1038/nn.2109},
file = {::},
issn = {1097-6256},
journal = {Nature neuroscience},
month = {may},
number = {5},
pages = {603--8},
pmid = {18425123},
publisher = {NIH Public Access},
title = {{Cortical activity patterns predict speech discrimination ability.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18425123 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2951886},
volume = {11},
year = {2008}
}
@article{Bates2015,
author = {Bates, Douglas and Machler, Martin and Bolker, Ben and Walker, Steve},
doi = {10.18637/jss.v067.i01},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--48},
title = {{Fitting Linear Mixed-Effects Models Using lme4}},
volume = {67},
year = {2015}
}
@misc{Team2016,
author = {Team, R Core},
booktitle = {R Foundation for Statistical Computing, Vienna, Austria.},
title = {{R: A language and environment for statistical computing.}},
year = {2016}
}
@article{Miyawaki1975,
abstract = {To test the effect of linguistic experience on the perception of a cue that is known to be effective in distinguishing between [1') and [I) in English, 21 Japanese and 39 American adults were tested on discrimination of a set of synthetic speech-like stimuli. The 13 "speech" stimuli in this set varied in the initial stationary frequency of the third formant (F3) and its subsequent transition into the vowel over a range sufficient to produce the perception of [1' a) and [I a) for American subjects and to produce [1' a) (which is not in phonemic contrast to [I a)) for Japanese subjects. Discrimination tests of a comparable set of stimuli consisting of the isolated F3 components provided a "nonspeech" control. For Americans, the discrimination of the speech stimuli was nearly categorical, i.e., comparison pairs which were identified as different phonemes were discriminated with high accuracy, while pairs which were identified as the same phoneme were discriminated relatively poorly. In comparison, discrimination of speech stimuli by Japanese subjects was only slightly better than chance for all comparison pairs. Performance on nonspeech stimuli, however, was virtually identical for Japanese and American subjects; both groups showed highly accurate discrimination of all comparison pairs. These results suggest that the effect of linguistic experience is specific to perception in the "speech mode." One way to examine the effect of linguistic experience on the perception of speech is to compare the discrimination of phonetic segments by two groups of speakers: one group speaks a language in},
author = {Miyawaki, Kuniko and Strange, Winifred and Verbrugge, Robert and Liberman, Alvin M and Jenkins, James J and Fujimura, Osamu},
doi = {10.3758/BF03211209},
isbn = {0031-5117},
issn = {0031-5117},
journal = {Perception {\&} Psychophysics},
month = {sep},
number = {5},
pages = {331--340},
publisher = {Springer-Verlag},
title = {{An effect of linguistic experience: The discrimination of [r] and [1] by native speakers of Japanese and English}},
url = {http://www.springerlink.com/index/10.3758/BF03211209},
volume = {18},
year = {1975}
}
@article{KewleyPort1983,
abstract = {Two recent accounts of the acoustic cues which specify place of articulation in syllable‐initial stop consonants claim that they are located in the initial portions of the CV waveform and are context‐free. Stevens and Blumstein [J. Acoust. Soc. Am. 64, 1358–1368 (1978)] have described the perceptually relevant spectral properties of these cues as static, while Kewley‐Port [J. Acoust. Soc. Am. 73, 322–335 (1983)] describes these cues as dynamic. Three perceptual experiments were conducted to test predictions derived from these accounts. Experiment 1 confirmed that acoustic cues for place of articulation are located in the initial 20–40 ms of natural stop‐vowel syllables. Next, short synthetic CV's modeled after natural syllables were generated using either a digital, parallel‐resonance synthesizer in experiment 2 or linear prediction synthesis in experiment 3. One set of synthetic stimuli preserved the static spectral properties proposed by Stevens and Blumstein. Another set of synthetic stimuli preserved ...},
author = {Kewley‐Port, Diane and Pisoni, David B. and Studdert‐Kennedy, Michael},
doi = {10.1121/1.389402},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {may},
number = {5},
pages = {1779--1793},
publisher = {Acoustical Society of America},
title = {{Perception of static and dynamic acoustic cues to place of articulation in initial stop consonants}},
url = {http://asa.scitation.org/doi/10.1121/1.389402},
volume = {73},
year = {1983}
}
@misc{Dahl2016,
author = {Dahl, David B.},
title = {{xtable: Export Tables to LaTeX or HTML}},
year = {2016}
}
@misc{Hlavac2015,
address = {Cambridge, MA},
author = {Hlavac, Marek},
title = {{stargazer: Well-Formatted Regression and Summary Statistics Tables}},
year = {2015}
}
@article{Chevillet2013,
author = {Chevillet, Mark A. and Jiang, Xiong and Rauschecker, Josef P. and Riesenhuber, Maximilian},
journal = {Journal of Neuroscience},
number = {12},
title = {{Automatic Phoneme Category Selectivity in the Dorsal Auditory Stream}},
url = {http://www.jneurosci.org/content/33/12/5208.short},
volume = {33},
year = {2013}
}
@article{Ashby1992,
author = {Ashby, F. Gregory and Maddox, W. Todd},
doi = {10.1037/0096-1523.18.1.50},
file = {::},
issn = {1939-1277},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
keywords = {experience level,naive vs experienced adults},
number = {1},
pages = {50--71},
publisher = {American Psychological Association},
title = {{Complex decision rules in categorization: Contrasting novice and experienced performance.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.18.1.50},
volume = {18},
year = {1992}
}
@book{Wickham2009,
address = {New York, NY},
author = {Wickham, Hadley},
isbn = {978-0-387-98140-6},
publisher = {Springer-Verlag},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
year = {2009}
}
@article{KewleyPort1983a,
abstract = {Running spectral displays derived from linear prediction analysis were used to examine the initial 40 ms of stop‐vowel CV syllables for possible acoustic correlates to place of articulation. Known spectral and temporal properties associated with the stop consonant release gesture were used to define a set of three‐time‐varying features observable in the visual displays. Judges identified place of articulation using these proposed features from running spectra of the syllables /b,d,g/ paired with eight vowels produced by three talkers. Average correct identification of place was 88{\%}; identification was better for the male talkers (92{\%}) than the one female talker (78{\%}). Post hoc analyses suggested, however, that simple rules could be incorporated in the feature definitions to account for differences in vocal tract size. The nature of the information contained in linear prediction running spectra was analyzed further to take account of known properties of the peripheral auditory system. The three proposed ti...},
author = {Kewley‐Port, Diane},
doi = {10.1121/1.388813},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {jan},
number = {1},
pages = {322--335},
publisher = {Acoustical Society of America},
title = {{Time‐varying features as correlates of place of articulation in stop consonants}},
url = {http://asa.scitation.org/doi/10.1121/1.388813},
volume = {73},
year = {1983}
}
@article{Lachlan2015a,
abstract = {Some of the psychological abilities that underlie human speech are shared with other species. One hallmark of speech is that linguistic context affects both how speech sounds are categorized into phonemes, and how different versions of phonemes are produced. We here confirm earlier findings that swamp sparrows categorically perceive the notes that constitute their learned songs and then investigate how categorical boundaries differ according to context. We clustered notes according to their acoustic structure, and found statistical evidence for clustering into 10 population-wide note types. Examining how three related types were perceived, we found, in both discrimination and labeling tests, that an "intermediate" note type is categorized with a "short" type when it occurs at the beginning of a song syllable, but with a "long" type at the end of a syllable. In sum, three produced note-type clusters appear to be underlain by two perceived categories. Thus, in birdsong, as in human speech, categorical perception is context-dependent, and as is the case for human phonology, there is a complex relationship between underlying categorical representations and surface forms. Our results therefore suggest that complex phonology can evolve even in the absence of rich linguistic components, like syntax and semantics.},
author = {Lachlan, Robert F and Nowicki, Stephen},
doi = {10.1073/pnas.1410844112},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Acoustic Stimulation,Analysis of Variance,Animal,Animal: physiology,Animals,Cluster Analysis,Discrimination (Psychology),Discrimination (Psychology): physiology,Flight,Linear Models,New York,Pennsylvania,Principal Component Analysis,Sound Spectrography,Sparrows,Sparrows: physiology,Speech Perception,Speech Perception: physiology,Vocalization,Wetlands,Wing,Wing: physiology},
number = {6},
pages = {1892--7},
pmid = {25561538},
title = {{Context-dependent categorical perception in a songbird.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25561538},
volume = {112},
year = {2015}
}
@article{Kluender1987,
abstract = {Japanese quail (Coturnix coturnix) learned a category for syllable-initial [d] followed by a dozen different vowels. After learning to categorize syllables consisting of [d], [b], or [g] followed by four different vowels, quail correctly categorized syllables in which the same consonants preceded eight novel vowels. Acoustic analysis of the categorized syllables revealed no single feature or pattern of features that could support generalization, suggesting that the quail adopted a more complex mapping of stimuli into categories. These results challenge theories of speech sound classification that posit uniquely human capacities.},
author = {Kluender, K R and Diehl, R L and Killeen, P R},
doi = {10.1126/science.3629235},
isbn = {0036-8075 (Print)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
keywords = {Animals,Coturnix,Coturnix: physiology,Female,Humans,Learning,Phonetics,Quail,Quail: physiology,Reinforcement (Psychology),Speech Perception},
language = {en},
month = {sep},
number = {4819},
pages = {1195--1197},
pmid = {3629235},
publisher = {American Association for the Advancement of Science},
title = {{Japanese quail can learn phonetic categories.}},
url = {http://science.sciencemag.org/content/237/4819/1195.abstract},
volume = {237},
year = {1987}
}
@article{Poeppel2008,
abstract = {Speech perception consists of a set of computations that take continuously varying acoustic waveforms as input and generate discrete representations that make contact with the lexical representations stored in long-term memory as output. Because the perceptual objects that are recognized by the speech perception enter into subsequent linguistic computation, the format that is used for lexical representation and processing fundamentally constrains the speech perceptual processes. Consequently, theories of speech perception must, at some level, be tightly linked to theories of lexical representation. Minimally, speech perception must yield representations that smoothly and rapidly interface with stored lexical items. Adopting the perspective of Marr, we argue and provide neurobiological and psychophysical evidence for the following research programme. First, at the implementational level, speech perception is a multi-time resolution process, with perceptual analyses occurring concurrently on at least two time scales (approx. 20-80 ms, approx. 150-300 ms), commensurate with (sub)segmental and syllabic analyses, respectively. Second, at the algorithmic level, we suggest that perception proceeds on the basis of internal forward models, or uses an 'analysis-by-synthesis' approach. Third, at the computational level (in the sense of Marr), the theory of lexical representation that we adopt is principally informed by phonological research and assumes that words are represented in the mental lexicon in terms of sequences of discrete segments composed of distinctive features. One important goal of the research programme is to develop linking hypotheses between putative neurobiological primitives (e.g. temporal primitives) and those primitives derived from linguistic inquiry, to arrive ultimately at a biologically sensible and theoretically satisfying model of representation and computation in speech.},
author = {Poeppel, D. and Idsardi, W. J and van Wassenhove, V.},
doi = {10.1098/rstb.2007.2160},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
month = {mar},
number = {1493},
pages = {1071--1086},
pmid = {17890189},
title = {{Speech perception at the interface of neurobiology and linguistics}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17890189 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2606797 http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2007.2160},
volume = {363},
year = {2008}
}
@article{Clerkin2016a,
author = {Clerkin, Elizabeth M. and Hart, Elizabeth and Rehg, James M. and Yu, Chen and Smith, Linda B.},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
number = {1711},
title = {{Real-world visual statistics and infants' first-learned object names}},
url = {http://rstb.royalsocietypublishing.org/content/372/1711/20160055.long},
volume = {372},
year = {2016}
}
@article{King2009,
author = {King, Andrew J and Nelken, Israel},
doi = {10.1038/nn.2308},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {jun},
number = {6},
pages = {698--701},
publisher = {Nature Publishing Group},
title = {{Unraveling the principles of auditory cortical processing: can we learn from the visual system?}},
url = {http://www.nature.com/doifinder/10.1038/nn.2308},
volume = {12},
year = {2009}
}
@article{Kleinschmidt2015,
author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
doi = {10.1037/a0038695},
file = {::},
issn = {1939-1471},
journal = {Psychological Review},
keywords = {adaptation,generalization,lack of invariance,speech perception,statistical learning},
number = {2},
pages = {148--203},
publisher = {American Psychological Association},
title = {{Robust speech perception: Recognize the familiar, generalize to the similar, and adapt to the novel.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0038695},
volume = {122},
year = {2015}
}
@article{Mines1978,
abstract = {The phoneme identification process of an automatic speech recognition system may be aided through the use of statistics of phoneme occurrence in conversational English. These statistics are also applicable to the fields of linguistics and speech, to teaching English as a foreign language and to speech pathology. In this study a data base containing 103,887 phoneme occurrences taken from casual conversational American English was obtained through interviews of sixteen adult males and ten adult females. The speech was transcribed using a quasi-phonemic system, known as ARPAbet, plus selected phoneme alternates and was analysed with computer assistance to obtain the rank order of phonemes according to frequency of occurrence. Also, the radius of the confidence interval for the observed frequency of occurrence was calculated at the 95{\%} level for each phoneme. The top ten phonemes (in order, / a, n, t, i, s, r, i, l, d, $\epsilon$ /) account for 47{\%} of all the data. As expected, the results of the present study correla...},
author = {Mines, M. Ardussi and Hanson, Barbara F. and Shoup, June E.},
doi = {10.1177/002383097802100302},
file = {::},
journal = {Language and Speech},
number = {3},
pages = {221=241},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{Frequency of Occurrence of Phonemes in Conversational English}},
url = {http://journals.sagepub.com/doi/abs/10.1177/002383097802100302},
volume = {21},
year = {1978}
}
@article{Suga1978,
author = {Suga, N and O'Neill, WE and Manabe, T},
journal = {Science},
number = {4343},
title = {{Cortical neurons sensitive to combinations of information-bearing elements of biosonar signals in the mustache bat}},
url = {http://science.sciencemag.org/content/200/4343/778},
volume = {200},
year = {1978}
}
@inproceedings{Petek1993,
author = {Petek, B. and Ferligoj, A.},
booktitle = {IEEE International Conference on Acoustics Speech and Signal Processing},
doi = {10.1109/ICASSP.1993.319287},
isbn = {0-7803-0946-4},
pages = {267--270 vol.2},
publisher = {IEEE},
title = {{Exploiting prediction error in a predictive-based connectionist speech recognition system}},
url = {http://ieeexplore.ieee.org/document/319287/},
year = {1993}
}
@incollection{Lindblom1988,
author = {Lindblom, Bjorn and Maddieson, Ian},
booktitle = {Language, speech, and mind},
isbn = {0-12-524940-3 (alk. paper) 0-12-524941-3 (paperback)},
pages = {62--78},
pmid = {308},
title = {{Phonetic Universals in Consonant Systems}},
url = {https://www.researchgate.net/profile/Ian{\_}Maddieson/publication/30352344{\_}Phonetic{\_}Universals{\_}in{\_}Consonant{\_}Systems/links/0912f5089cc36c5b0c000000.pdf},
year = {1988}
}
@article{Shannon1948,
abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, Claude E},
doi = {10.1145/584091.584093},
eprint = {9411012},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Shannon - 1948 - A mathematical theory of communication(3).pdf:pdf;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Shannon - 1948 - A mathematical theory of communication(4).pdf:pdf},
isbn = {0252725484},
issn = {15591662},
journal = {The Bell System Technical Journal},
number = {July 1928},
pages = {379--423},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{A mathematical theory of communication}},
url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html},
volume = {27},
year = {1948}
}
@article{Radziwon2009,
abstract = {Tone detection and temporal gap detection thresholds were determined in CBA/CaJ mice using a Go/No-go procedure and the psychophysical method of constant stimuli. In the first experiment, audiograms were constructed for five CBA/CaJ mice. Thresholds were obtained for eight pure tones ranging in frequency from 1 to 42 kHz. Audiograms showed peak sensitivity between 8 and 24 kHz, with higher thresholds at lower and higher frequencies. In the second experiment, thresholds for gap detection in broadband and narrowband noise bursts were measured at several sensation levels. For broadband noise, gap thresholds were between 1 and 2 ms, except at very low sensation levels, where thresholds increased significantly. Gap thresholds also increased significantly for low pass-filtered noise bursts with a cutoff frequency below 18 kHz. Our experiments revised absolute auditory thresholds in the CBA/CaJ mouse strain and demonstrated excellent gap detection ability in the mouse. These results add to the baseline behavioral data from normal-hearing mice which have become increasingly important for assessing auditory abilities in genetically altered mice.},
author = {Radziwon, Kelly E and June, Kristie M and Stolzberg, Daniel J and Xu-Friedman, Matthew A and Salvi, Richard J and Dent, Micheal L},
doi = {10.1007/s00359-009-0472-1},
file = {::},
issn = {1432-1351},
journal = {Journal of comparative physiology. A, Neuroethology, sensory, neural, and behavioral physiology},
month = {oct},
number = {10},
pages = {961--9},
pmid = {19756650},
publisher = {NIH Public Access},
title = {{Behaviorally measured audiograms and gap detection thresholds in CBA/CaJ mice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19756650 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2813807},
volume = {195},
year = {2009}
}
@incollection{Lindblom1986,
author = {Lindblom, Bjorn},
booktitle = {Experimental Phonology},
file = {::},
isbn = {0-12-524940-3 (alk. paper) 0-12-524941-3 (paperback)},
pages = {13--44},
pmid = {308},
title = {{Phonetic Universals in Vowel Systems}},
url = {https://pdfs.semanticscholar.org/68c4/3489716b60bd2fc3ea842ec653547caa76a6.pdf},
year = {1986}
}
@article{Margoliash1992,
abstract = {Song learning shapes the response properties of auditory neurons in the song system to become highly selective for the individual bird's own ("autogenous") song. The auditory representation of autogenous song is achieved in part by neurons that exhibit facilitated responses to combinations of components of song. To understand the circuits that underlie these complex properties, the combination sensitivity of single units in the hyperstriatum ventrale, pars caudale (HVc) of urethane-anesthetized zebra finches was studied. Some neurons exhibited nonlinear temporal summation, spectral summation, or both. The majority of these neurons exhibited low spontaneous rates and phasic responses. Most combination-sensitive neurons required highly accurate copies of sounds derived from the autogenous song and responded weakly to tone bursts, combinations of simple stimuli, or conspecific songs. Temporal combination-sensitive (TCS) neurons required either two or more segments of a single syllable, or two or more syllables of the autogenous song, to elicit a facilitated, excitatory response. TCS neurons integrated auditory input over periods ranging from 80 to 350 msec, although this represents a lower limit. Harmonic combination-sensitive (HCS) neurons required combinations of two harmonics with particular frequency and temporal characteristics that were similar to autogenous song syllables. Both TCS and HCS neurons responded much more weakly when the dynamical spectral features of the autogenous song or syllables were modified than when the dynamical amplitude (waveform) features of the songs were modified. These results suggest that understanding the temporal dynamics of auditory responses in HVc may provide insight into neuronal circuits modified by song learning.},
author = {Margoliash, D and Fortune, E S},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
month = {nov},
number = {11},
pages = {4309--26},
pmid = {1432096},
title = {{Temporal and harmonic combination-sensitive neurons in the zebra finch's HVc.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1432096},
volume = {12},
year = {1992}
}
@article{Hubel1962,
abstract = {Images$\backslash$nnull},
author = {Hubel, D. H. and Wiesel, T. N.},
doi = {10.1523/JNEUROSCI.1991-09.2009},
file = {::},
isbn = {0270-6474},
issn = {0022-3751},
journal = {The Journal of Physiology},
keywords = {CEREBRAL CORTEX/physiology},
month = {jan},
number = {1},
pages = {106--154.2},
pmid = {19776262},
publisher = {Wiley-Blackwell},
title = {{Receptive fields, binocular interaction and functional architecture in the cat's visual cortex}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14449617 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1359523},
volume = {160},
year = {1962}
}
@article{Vapnik1999,
author = {Vapnik, V.N.},
doi = {10.1109/72.788640},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
number = {5},
pages = {988--999},
title = {{An overview of statistical learning theory}},
url = {http://ieeexplore.ieee.org/document/788640/},
volume = {10},
year = {1999}
}
@article{Schultz2016,
abstract = {Reward prediction errors consist of the differences between received and predicted rewards. They are crucial for basic forms of learning about rewards and make us strive for more rewards-an evolutionary beneficial trait. Most dopamine neurons in the midbrain of humans, monkeys, and rodents signal a reward prediction error; they are activated by more reward than predicted (positive prediction error), remain at baseline activity for fully predicted rewards, and show depressed activity with less reward than predicted (negative prediction error). The dopamine signal increases nonlinearly with reward value and codes formal economic utility. Drugs of addiction generate, hijack, and amplify the dopamine reward signal and induce exaggerated, uncontrolled dopamine effects on neuronal plasticity. The striatum, amygdala, and frontal cortex also show reward prediction error coding, but only in subpopulations of neurons. Thus, the important concept of reward prediction errors is implemented in neuronal hardware.$\backslash$n$\backslash$nAbstract available from the publisher.$\backslash$n$\backslash$nAbstract available from the publisher.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Schultz, Wolfram},
doi = {10.1038/nrn.2015.26},
eprint = {9809069v1},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Schultz - 2016 - Dopamine reward prediction error coding(2).pdf:pdf},
isbn = {3-540-27590-8},
issn = {12948322},
journal = {Dialogues in Clinical Neuroscience},
keywords = {Dopamine,Neuron,Neurophysiology,Prediction,Reward,Striatum,Substantia nigra,Ventral tegmental area},
number = {1},
pages = {23--32},
pmid = {27069377},
primaryClass = {arXiv:gr-qc},
publisher = {Nature Publishing Group},
title = {{Dopamine reward prediction error coding}},
url = {http://dx.doi.org/10.1038/nrn.2015.26},
volume = {18},
year = {2016}
}
@article{Rauschecker1998b,
author = {Rauschecker, Josef P},
doi = {10.1016/S0959-4388(98)80040-8},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
month = {aug},
number = {4},
pages = {516--521},
title = {{Cortical processing of complex sounds}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0959438898800408},
volume = {8},
year = {1998}
}
@misc{Team2015,
author = {Team, RStudio},
publisher = {RStudio, Inc., Boston, MA},
title = {{RStudio: Integrated Development for R.}},
url = {http://www.rstudio.com/},
year = {2015}
}
@article{Leaver2010,
author = {Leaver, Amber M. and Rauschecker, Josef P.},
journal = {Journal of Neuroscience},
number = {22},
title = {{Cortical Representation of Natural Complex Sounds: Effects of Acoustic Features and Auditory Object Category}},
url = {http://www.jneurosci.org/content/30/22/7604.short},
volume = {30},
year = {2010}
}
@article{Kuhl1978,
abstract = {In an attempt to clearly differentiate perceptual effects that are attributable to ''auditory'' and ''phonetic'' levels of processing in speech perception we have undertaken a series of experiments with animal listeners. Four chinchillas (Chinchilla laniger) were trained to respond differently to the ''endpoints'' of a synthetic alveolar speech continuum (0 ms VOT and +80 ms VOT) and were then tested in a generalization paradigm with the VOT stimuli between these endpoints. The resulting identification functions were nearly identical to those obtained with adult English‐speaking listeners. To test the generality of this agreement, the animals were then tested with synthetic stimuli that had labial and velar places of articulation. As a whole, the functions produced by the two species were very similar; the same relative locations of the phonetic boundaries, with lowest VOT boundaries for labial stimuli and highest for velar stimuli, were obtained for each animal and human subject. No significant differences between species on the absolute values of the phonetic boundaries were obtained, but chinchillas produced identification functions that were slightly, but significantly, less steep. These results are discussed with regard to theories of speech perception, the evolution of a speech‐sound repertoire, and current interpretations of the human infant's perceptual proclivities with regard to speech‐sound perception.},
author = {Kuhl, Patricia K. and Miller, James D.},
doi = {10.1121/1.381770},
isbn = {0001-4966 (Print)$\backslash$r0001-4966 (Linking)},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {905--917},
pmid = {670558},
title = {{Speech perception by the chinchilla: Identification functions for synthetic VOT stimuli}},
url = {http://link.aip.org/link/?JAS/63/905/1{\%}5Cnhttp://asadl.org/jasa/resource/1/jasman/v63/i3/p905{\_}s1?isAuthorized=no},
volume = {63},
year = {1978}
}
@article{Ng2002a,
abstract = {Comparison of generative and discriminative classifiers is an ever-lasting topic. As an important contribution to this topic, based on their theoretical and empirical comparisons between the naive Bayes classifier and linear logistic regression, Ng and Jordan (NIPS 841-848, 2001) claimed that there exist two distinct regimes of performance between the generative and discriminative classifiers with regard to the training-set size. In this paper, our empirical and simulation studies, as a complement of their work, however, suggest that the existence of the two distinct regimes may not be so reliable. In addition, for real world datasets, so far there is no theoretically correct, general criterion for choosing between the discriminative and the generative approaches to classification of an observation x into a class y; the choice depends on the relative confidence we have in the correctness of the specification of either p(y vertical bar x) or p(x, y) for the data. This can be to some extent a demonstration of why Efron (J Am Stat Assoc 70(352):892-898, 1975) and O'Neill (J Am Stat Assoc 75(369):154-160, 1980) prefer normal-based linear discriminant analysis (LDA) when no model mis-specification occurs but other empirical studies may prefer linear logistic regression instead. Furthermore, we suggest that pairing of either LDA assuming a common diagonal covariance matrix (LDA-A) or the naive Bayes classifier and linear logistic regression may not be perfect, and hence it may not be reliable for any claim that was derived from the comparison between LDA-A or the naive Bayes classifier and linear logistic regression to be generalised to all generative and discriminative classifiers.},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1007/s11063-008-9088-7},
author = {Ng, Andrew and Jordan, Michael I.},
doi = {10.1007/s11063-008-9088-7},
eprint = {/dx.doi.org/10.1007/s11063-008-9088-7},
isbn = {1106300890},
issn = {13704621},
journal = {Proceedings of Advances in Neural Information Processing},
keywords = {asymptotic relative efficiency,discriminative classifiers,generative classifiers,logistic regression,naive bayes classifier,normal based discriminant analysis},
number = {3},
pages = {169--187},
pmid = {25246403},
primaryClass = {http:},
title = {{On generative vs. discriminative classifiers: A comparison of logistic regression and naive bayes}},
volume = {28},
year = {2002}
}
@article{Wickham2011,
author = {Wickham, Hadley},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--29},
title = {{The Split-Apply-Combine Strategy for Data Analysis}},
url = {http://www.jstatsoft.org/v40/i01/},
volume = {40},
year = {2011}
}
@book{Lieberman1984,
abstract = {Introduction : The biological framework -- Neurophysiology, neural models, and language -- Distributed neural computers and feature detectors -- Automatization and syntax -- Syntax, words, and meaning -- Respiration, speech, and meaning -- Elephant ears, frogs, and human speech -- Speech is special -- Linguistic distinctions and auditory processes -- Man on the flying trapeze : the acquisition of speech -- Apes and children -- Evolution of human speech : comparative studies -- Evolution of human speech : the fossil record -- Conclusion : On the nature and evolution of the biological bases of language.},
author = {Lieberman, Philip.},
isbn = {0674074130},
pages = {379},
publisher = {Harvard University Press},
title = {{The biology and evolution of language}},
year = {1984}
}
@article{Liberman1985a,
abstract = {A motor theory of speech perception, initially proposed to account for results of early experiments with synthetic speech, is now extensively revised to accommodate recent findings, and to relate the assumptions of the theory to those that might be made about other perceptual modes. According to the revised theory, phonetic information is perceived in a biologically distinct system, a ‘module' specialized to detect the intended gestures of the speaker that are the basis for phonetic categories. Built into the structure of this module is the unique but lawful relationship between the gestures and the acoustic patterns in which they are variously overlapped. In consequence, the module causes perception of phonetic structure without translation from preliminary auditory impressions. Thus, it is comparable to such other modules as the one that enables an animal to localize sound. Peculiar to the phonetic module are the relation between perception and production it incorporates and the fact that it must compete with other modules for the same stimulus variations. Une th{\'{e}}orie motrice de la perception propos{\'{e}}e initialement pour rendre compte des r{\'{e}}sultats des premi{\`{e}}res exp{\'{e}}riences avec de la parole synth{\'{e}}tique a {\'{e}}t{\'{e}} largement r{\'{e}}vis{\'{e}}e afin d'interpr{\'{e}}ter les donn{\'{e}}es r{\'{e}}centes et de relier les propositions de cette th{\'{e}}orie {\`{a}} celles que l'on peut faire pour d'autres modalit{\'{e}}s de perception. La r{\'{e}}vision de cette th{\'{e}}orie stipule que l'information phon{\'{e}}tique est fournie par un syst{\`{e}}me biologique distinct, un ‘module' sp{\'{e}}cialis{\'{e}} pour d{\'{e}}tecter les gestes que le locuteur a eu l'intention de faire: ces gestes fondent les cat{\'{e}}gories phon{\'{e}}tiques. La relation entre les gestes et les patterns acoustiques dans lesquels ceux-ci sont imbriqu{\'{e}}s de facon vari{\'{e}}e est unique mais r{\'{e}}gul{\'{e}}e. Cette relation est construite dans la structure du module. En cons{\'{e}}quence le module provoque la perception de la structure phon{\'{e}}tique sans traduction {\`{a}} partir d'impressions auditives pr{\'{e}}liminaires. Ce module est ainsi comparable {\`{a}} d'autres modules tels que celui qui permet {\`{a}} l'animal de localiser les sons. La particularit{\'{e}} de ce module tient {\`{a}} la relation entre perception et production qu'il incorpore et an fait qu'il doit rivaliser avec d'autres modules pour de m{\^{e}}mes variations de stimulus.},
author = {Liberman, Alvin M. and Mattingly, Ignatius G.},
doi = {10.1016/0010-0277(85)90021-6},
file = {::},
issn = {00100277},
journal = {Cognition},
number = {1},
pages = {1--36},
title = {{The motor theory of speech perception revised}},
volume = {21},
year = {1985}
}
@article{Tremblay2016,
abstract = {With the advancement of cognitive neuroscience and neuropsychological research, the field of language neurobiology is at a cross-roads with respect to its framing theories. The central thesis of this article is that the major historical framing model, the Classic “Wernicke-Lichtheim-Geschwind” model, and associated terminology, is no longer adequate for contemporary investigations into the neurobiology of language. We argue that the Classic model (1) is based on an outdated brain anatomy; (2) does not adequately represent the distributed connectivity relevant for language, (3) offers a modular and “language centric” perspective, and (4) focuses on cortical structures, for the most part leaving out subcortical regions and relevant connections. To make our case, we discuss the issue of anatomical specificity with a focus on the contemporary usage of the terms “Broca's and Wernicke's area”, including results of a survey that was conducted within the language neurobiology community. We demonstrate that there is no consistent anatomical definition of “Broca's and Wernicke's Areas”, and propose to replace these terms with more precise anatomical definitions. We illustrate the distributed nature of the language connectome, which extends far beyond the single-pathway notion of arcuate fasciculus connectivity established in Geschwind's version of the Classic Model. By illustrating the definitional confusion surrounding “Broca's and Wernicke's areas”, and by illustrating the difficulty integrating the emerging literature on perisylvian white matter connectivity into this model, we hope to expose the limits of the model, argue for its obsolescence, and suggest a path forward in defining a replacement.},
author = {Tremblay, Pascale and Dick, Anthony Steven},
doi = {10.1016/j.bandl.2016.08.004},
issn = {0093934X},
journal = {Brain and Language},
pages = {60--71},
title = {{Broca and Wernicke are dead, or moving past the classic model of language neurobiology}},
url = {http://www.sciencedirect.com/science/article/pii/S0093934X16300475},
volume = {162},
year = {2016}
}
@article{Elman1988,
abstract = {In the work described here, the backpropagation neural network learning procedure is applied to the analysis and recognition of speech. This procedure takes a set of input/output pattern pairs and attempts to learn their functional relationship; it develops the necessary representational features during the course of learning. A series of computer simulation studies was carried out to assess the ability of these networks to accurately label sounds, to learn to recognize sounds without labels, and to learn feature representations of continuous speech. These studies demonstrated that the networks can learn to label presegmented test tokens with accuracies of up to 95{\%}. Networks trained on segmented sounds using a strategy that requires no external labels were able to recognize and delineate sounds in continuous speech. These networks developed rich internal representations that included units which corresponded to such traditional distinctions as vowels and consonants, as well as units that were sensitive to novel and nonstandard features. Networks trained on a large corpus of unsegmented, continuous speech without labels also developed interesting feature representations, which may be useful in both segmentation and label learning. The results of these studies, while preliminary, demonstrate that backpropagation learning can be used with complex, natural data to identify a feature structure that can serve as the basis for both analysis and nontrivial pattern recognition.},
author = {Elman, J L and Zipser, D},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {apr},
number = {4},
pages = {1615--26},
pmid = {3372872},
title = {{Learning the hidden structure of speech.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3372872},
volume = {83},
year = {1988}
}
@article{Fox2016,
abstract = {Although much evidence suggests that the identification of phonetically ambiguous target words can be biased by preceding sentential context, interactive and autonomous models of speech perception disagree as to the mechanism by which higher level information affects subjects' responses. Some have suggested that the time course of context effects is incompatible with interactive models (e.g., TRACE). Two experiments examine this issue. In Experiment 1, subjects heard noun- and verb-biasing sentence contexts (e.g., Valerie hated the . . . vs. Brett hated to . . .), followed by stimuli from 2 voice-onset time continua: bay-pay (noun-verb) versus buy-pie (verb-noun). Consistent with prior research, identification of phonetically ambiguous targets was biased by the preceding context, and the size of this bias diminished in slower compared with faster responses. In Experiment 2, tokens from the same continua were embedded among filler target words beginning with /b/ or /p/ to elicit phonemically driven identification decisions and discourage word-level strategies. Results again revealed contextually biased responding, but this bias was as strong in slow as in fast responses. Together, these results suggest that phoneme identification decisions reflect robust, lasting top-down effects of lexical feedback on prelexical representations, as predicted by interactive models of speech perception.},
author = {Fox, Neal P. and Blumstein, Sheila E.},
doi = {10.1037/a0039965},
issn = {1939-1277},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
month = {may},
number = {5},
pages = {730--741},
pmid = {26689310},
title = {{Top-down effects of syntactic sentential context on phonetic processing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26689310 http://doi.apa.org/getdoi.cfm?doi=10.1037/a0039965},
volume = {42},
year = {2016}
}
@article{Kluender1994,
abstract = {When F1-onset frequency is lower, longer F1 cut-back (VOT) is required for human listeners to perceive synthesized stop consonants as voiceless. K. R. Kluender [J. Acoust. Soc. Am. 90, 83-96 (1991)] found comparable effects of F1-onset frequency on the "labeling" of stop consonants by Japanese quail (coturnix coturnix japonica) trained to distinguish stop consonants varying in F1 cut-back. In that study, CVs were synthesized with natural-like rising F1 transitions, and endpoint training stimuli differed in the onset frequency of F1 because a longer cut-back resulted in a higher F1 onset. In order to assess whether earlier results were due to auditory predispositions or due to animals having learned the natural covariance between F1 cut-back and F1-onset frequency, the present experiment was conducted with synthetic continua having either a relatively low (375 Hz) or high (750 Hz) constant-frequency F1. Six birds were trained to respond differentially to endpoint stimuli from three series of synthesized /CV/s varying in duration of F1 cut-back. Second and third formant transitions were appropriate for labial, alveolar, or velar stops. Despite the fact that there was no opportunity for animal subjects to use experienced covariation of F1-onset frequency and F1 cut-back, quail typically exhibited shorter labeling boundaries (more voiceless stops) for intermediate stimuli of the continua when F1 frequency was higher. Responses by human subjects listening to the same stimuli were also collected. Results lend support to the earlier conclusion that part or all of the effect of F1 onset frequency on perception of voicing may be adequately explained by general auditory processes.(ABSTRACT TRUNCATED AT 250 WORDS)},
author = {Kluender, K R and Lotto, a J},
doi = {10.1121/1.408466},
isbn = {0001-4966 (Print)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {Animals,Coturnix,Female,Humans,Male,Phonetics,Speech Acoustics,Speech Perception},
number = {2},
pages = {1044--52},
pmid = {8132898},
title = {{Effects of first formant onset frequency on [-voice] judgments result from auditory processes not specific to humans.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8132898},
volume = {95},
year = {1994}
}
@article{Pasley2012,
abstract = {How the human auditory system extracts perceptually relevant acoustic features of speech is unknown. To address this question, we used intracranial recordings from nonprimary auditory cortex in the human superior temporal gyrus to determine what acoustic information in speech sounds can be reconstructed from population neural activity. We found that slow and intermediate temporal fluctuations, such as those corresponding to syllable rate, were accurately reconstructed using a linear model based on the auditory spectrogram. However, reconstruction of fast temporal fluctuations, such as syllable onsets and offsets, required a nonlinear sound representation based on temporal modulation energy. Reconstruction accuracy was highest within the range of spectro-temporal fluctuations that have been found to be critical for speech intelligibility. The decoded speech representations allowed readout and identification of individual words directly from brain activity during single trial sound presentations. These findings reveal neural encoding mechanisms of speech acoustic parameters in higher order human auditory cortex.},
author = {Pasley, Brian N. and David, Stephen V. and Mesgarani, Nima and Flinker, Adeen and Shamma, Shihab A. and Crone, Nathan E. and Knight, Robert T. and Chang, Edward F.},
doi = {10.1371/journal.pbio.1001251},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Pasley et al. - 2012 - Reconstructing speech from human auditory cortex(2).pdf:pdf},
isbn = {1545-7885},
issn = {15449173},
journal = {PLoS Biology},
number = {1},
pmid = {22303281},
title = {{Reconstructing speech from human auditory cortex}},
volume = {10},
year = {2012}
}
@article{Schindelin2012,
abstract = {Fiji is a distribution of the popular open-source software ImageJ focused on biological-image analysis. Fiji uses modern software engineering practices to combine powerful software libraries with a broad range of scripting languages to enable rapid prototyping of image-processing algorithms. Fiji facilitates the transformation of new algorithms into ImageJ plugins that can be shared with end users through an integrated update system. We propose Fiji as a platform for productive collaboration between computer science and biology research communities.},
archivePrefix = {arXiv},
arxivId = {1081-8693},
author = {Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert},
doi = {10.1038/nmeth.2019},
eprint = {1081-8693},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
keywords = {*Software,Algorithms,Animals,Brain/ultrastructure,Computational Biology/*methods,Drosophila melanogaster/ultrastructure,Image Enhancement/methods,Image Processing, Computer-Assisted/*methods,Imaging, Three-Dimensional/methods,Information Dissemination,Software Design},
number = {7},
pages = {676--682},
pmid = {22743772},
title = {{Fiji: an open-source platform for biological-image analysis}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2019},
volume = {9},
year = {2012}
}
@misc{Schindelin2015,
abstract = {Technology in microscopy advances rapidly, enabling increasingly affordable, faster, and more precise quantitative biomedical imaging, which necessitates correspondingly more-advanced image processing and analysis techniques. A wide range of software is available—from commercial to academic, special-purpose to Swiss army knife, small to large—but a key characteristic of software that is suitable for scientific inquiry is its accessibility. Open-source software is ideal for scientific endeavors because it can be freely inspected, modified, and redistributed; in particular, the open-software platform ImageJ has had a huge impact on the life sciences, and continues to do so. From its inception, ImageJ has grown significantly due largely to being freely available and its vibrant and helpful user community. Scientists as diverse as interested hobbyists, technical assistants, students, scientific staff, and advanced biology researchers use ImageJ on a daily basis, and exchange knowledge via its dedicated mailing list. Uses of ImageJ range from data visualization and teaching to advanced image processing and statistical analysis. The software's extensibility continues to attract biologists at all career stages as well as computer scientists who wish to effectively implement specific image-processing algorithms. In this review, we use the ImageJ project as a case study of how open-source software fosters its suites of software tools, making multitudes of image-analysis technology easily accessible to the scientific community. We specifically explore what makes ImageJ so popular, how it impacts the life sciences, how it inspires other projects, and how it is self-influenced by coevolving projects within the ImageJ ecosystem. Mol. Reprod. Dev. 82: 518–529, 2015. {\textcopyright} 2015 Wiley Periodicals, Inc.},
author = {Schindelin, Johannes and Rueden, Curtis T. and Hiner, Mark C. and Eliceiri, Kevin W.},
booktitle = {Molecular Reproduction and Development},
doi = {10.1002/mrd.22489},
isbn = {1098-2795},
issn = {10982795},
number = {7-8},
pages = {518--529},
pmid = {26153368},
title = {{The ImageJ ecosystem: An open platform for biomedical image analysis}},
volume = {82},
year = {2015}
}
@article{Lowe2004,
author = {Lowe, D.},
journal = {International Journal of Computer Vision},
number = {2},
pages = {91--110},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
volume = {60},
year = {2004}
}
@article{Stevens1978,
abstract = {In a series of experiments, identification responses for place of articulation were obtained for synthetic stop consonants in consonant-vowel syllables with different vowels. The acoustic attributes of the consonants were systematically manipulated, the selection of stimulus characteristics being guided in part by theoretical considerations concerning the expected properties of the sound generated in the vocal tract as place of articulation is varied. Several stimulus series were generated with and without noise bursts at the onset, and with and without formant transitions following consonantal release. Stimuli with transitions only, and with bursts plus transitions, were consistently classified according to place of articulation, whereas stimuli with bursts only and no transitions were not consistently identified. The acoustic attributes of the stimuli were examined to determine whether invariant properties characterized each place of atriculation independent of vowel context. It was determined that the gross shape of the spectrum sampled at the consonantal release showed a distinctive shape for each place of articulation: a prominent midfrequency spectral peak for velars, a diffuse-rising spectrum for alveolars, and a diffuse-falling spectrum for labials. These attributes are evident for stimuli containing transitions only, but are enhanced by the presence of noise bursts at the onset.},
author = {Stevens, K N and Blumstein, S E},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {nov},
number = {5},
pages = {1358--68},
pmid = {744836},
title = {{Invariant cues for place of articulation in stop consonants.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/744836},
volume = {64},
year = {1978}
}
@article{Hickok2012a,
abstract = {Speech recognition is an active process that involves some form of predictive coding. This statement is relatively uncontroversial. What is less clear is the source of the prediction. The dual-stream model of speech processing suggests that there are two possible sources of predictive coding in speech perception: the motor speech system and the lexical-conceptual system. Here I provide an overview of the dual-stream model of speech processing and then discuss evidence concerning the source of predictive coding during speech recognition. I conclude that, in contrast to recent theoretical trends, the dorsal sensory-motor stream is not a source of forward prediction that can facilitate speech recognition. Rather, it is forward prediction coming out of the ventral stream that serves this function.Learning outcomes: Readers will (1) be able to explain the dual route model of speech processing including the function of the dorsal and ventral streams in language processing, (2) describe how disruptions to certain components of the dorsal stream can cause conduction aphasia, (3) be able to explain the fundamental principles of state feedback control in motor behavior, and (4) identify the role of predictive coding in motor control and in perception and how predictive coding coming out of the two streams may have different functional consequences. {\textcopyright} 2012 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hickok, Gregory},
doi = {10.1016/j.jcomdis.2012.06.004},
eprint = {NIHMS150003},
isbn = {1873-7994 (Electronic)$\backslash$n0021-9924 (Linking)},
issn = {00219924},
journal = {Journal of Communication Disorders},
keywords = {Aphasia,Language,Motor control,Speech perception,Speech production},
number = {6},
pages = {393--402},
pmid = {22766458},
title = {{The cortical organization of speech processing: Feedback control and predictive coding the context of a dual-stream model}},
volume = {45},
year = {2012}
}
@article{Kewley-Port1983,
abstract = {Running spectral displays derived from linear prediction analysis were used to examine the initial 40 ms of stop-vowel CV syllables for possible acoustic correlates to place of articulation. Known spectral and temporal properties associated with the stop consonant release gesture were used to define a set of three-time-varying features observable in the visual displays. Judges identified place of articulation using these proposed features from running spectra of the syllables /b,d,g/paired with eight vowels produced by three talkers. Average correct identification of place was 88{\%}; identification was better for the male talkers (92{\%}) than the one female talker (78{\%}). Post hoc analyses suggested, however, that simple rules could be incorporated in the feature definitions to account for differences in vocal tract size. The nature of the information contained in linear prediction running spectra was analyzed further to take account of known properties of the peripheral auditory system. The three proposed time-varying features were shown to be displayed robustly in auditory filtered running spectra. The advantages of describing acoustic correlates for place from the dynamically varying temporal and spectral information in running spectra is discussed with regard to the static template matching approach advocated recently by Blumstein and Stevens [J. Acoust. Soc. Am. 66, 1001-1017 (1979)].},
author = {Kewley-Port, D},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {jan},
number = {1},
pages = {322--35},
pmid = {6826902},
title = {{Time-varying features as correlates of place of articulation in stop consonants.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6826902},
volume = {73},
year = {1983}
}
@article{Kewley-Port1982a,
abstract = {Formant transitions have been considered important context-dependent acoustic cues to place of articulation in stop-vowel syllables. However, the bulk of earlier research supporting their perceptual importance has been conducted primarily with synthetic speech stimuli. The present study examined the acoustic correlates of place of articulation in the voiced formant transitions from natural speech. Linear prediction analysis was used to provide detailed temporal and spectral measurements of the formant transitions for /b,d,g/ paired with eight vowels produced by one talker. Measurements of the transition onset and steady state frequencies, durations, and derived formant loci for F1, F2, and F3 are reported. Analysis of these measures showed little evidence of context invariant acoustic correlates of place. When vowel context was known, most transition parameters were not reliable acoustic correlates of place except for the F2 transition and a two-dimensional representation of F2 X F3 onset frequencies. The results indicated that the information contained in the formant transitions in these natural stop-vowel syllables was not sufficient to distinguish place across all the vowel contexts studied.},
author = {Kewley-Port, D},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {aug},
number = {2},
pages = {379--89},
pmid = {7119280},
title = {{Measurement of formant transitions in naturally produced stop consonant-vowel syllables.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7119280},
volume = {72},
year = {1982}
}
@article{Rosenblum2008,
abstract = {Speech perception is inherently multimodal. Visual speech (lip-reading) information is used by all perceivers and readily integrates with auditory speech. Imaging research suggests that the brain treats auditory and visual speech similarly. These findings have led some researchers to consider that speech perception works by extracting amodal information that takes the same form across modalities. From this perspective, speech integration is a property of the input information itself. Amodal speech information could explain the reported automaticity, immediacy, and completeness of audiovisual speech integration. However, recent findings suggest that speech integration can be influenced by higher cognitive properties such as lexical status and semantic context. Proponents of amodal accounts will need to explain these results.},
author = {Rosenblum, Lawrence D.},
doi = {10.1111/j.1467-8721.2008.00615.x},
file = {::},
issn = {0963-7214},
journal = {Current Directions in Psychological Science},
keywords = {audiovisual,lip reading,multimodal,speech},
month = {dec},
number = {6},
pages = {405--409},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Speech Perception as a Multimodal Phenomenon}},
url = {http://journals.sagepub.com/doi/10.1111/j.1467-8721.2008.00615.x},
volume = {17},
year = {2008}
}
@article{Strauss2007,
author = {Strauss, Ted J. and Harris, Harlan D. and Magnuson, James S.},
doi = {10.3758/BF03192840},
file = {::},
issn = {1554-351X},
journal = {Behavior Research Methods},
month = {feb},
number = {1},
pages = {19--30},
publisher = {Springer-Verlag},
title = {{jTRACE: A reimplementation and extension of the TRACE model of speech perception and spoken word recognition}},
url = {http://www.springerlink.com/index/10.3758/BF03192840},
volume = {39},
year = {2007}
}
@article{Ohl,
abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
issn = {1072-0502},
journal = {Learning {\&} memory (Cold Spring Harbor, N.Y.)},
number = {4},
pages = {347--62},
pmid = {10509706},
title = {{Bilateral ablation of auditory cortex in Mongolian gerbil affects discrimination of frequency modulated tones but not of pure tones.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC311295},
volume = {6}
}
@article{Kuhl1983,
abstract = {Discrimination of speech-sound pairs drawn from a computer-generated continuum in which syllables varied along the place of articulation phonetic feature (/b,d,g/) was tested with macaques. The acoustic feature that was varied along the two-formant 15-step continuum was the starting frequency of the second-formant transition. Discrimination of stimulus pairs separated by two steps was tested along the entire continuum in a same-different task. Results demonstrated that peaks in the discrimination functions occur for macaques at the "phonetic boundaries" which separate the /b-d/ and /d-g/ categories for human listeners. The data support two conclusions. First, although current theoretical accounts of place perception by human adults suggest that isolated second-formant transitions are "secondary" cues, learned by association with primary cues, the animal data are more compatible with the notion that second-formant transitions are sufficient to allow the appropriate partitioning of a place continuum in the absence of associative pairing with other more complex cues. Second, we discuss two potential roles played by audition in the evolution of the acoustics of language. One is that audition provided a set of "natural psychophysical boundaries," based on rather simple acoustic properties, which guided the selection of the phonetic repertoire but did not solely determine it; the other is that audition provided a set of rules for the formation of "natural classes" of sound and that phonetic units met those criteria. The data provided in this experiment provide support for the former. Experiments that could more clearly differentiate the two hypotheses are described.},
author = {Kuhl, P K and Padden, D M},
doi = {10.3758/BF03204208},
isbn = {0001-4966 (Print)$\backslash$r0001-4966 (Linking)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {1003--1010},
pmid = {6221040},
title = {{Enhanced discriminability at the phonetic boundaries for the place feature in macaques.}},
volume = {73},
year = {1983}
}
@article{Diehl2004,
abstract = {This chapter focuses on one of the first steps in comprehending spoken language: How do listeners extract the most fundamental linguistic elements-consonants and vowels, or the distinctive features which compose them-from the acoustic signal? We begin by describing three major theoretical perspectives on the perception of speech. Then we review several lines of research that are relevant to distinguishing these perspectives. The research topics surveyed include categorical perception, phonetic context effects, learning of speech and related nonspeech categories, and the relation between speech perception and production. Finally, we describe challenges facing each of the major theoretical perspectives on speech perception.},
author = {Diehl, Randy L. and Lotto, Andrew J. and Holt, Lori L.},
doi = {10.1146/annurev.psych.55.090902.142028},
issn = {0066-4308},
journal = {Annual Review of Psychology},
month = {feb},
number = {1},
pages = {149--179},
pmid = {14744213},
title = {{Speech Perception}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14744213 http://www.annualreviews.org/doi/10.1146/annurev.psych.55.090902.142028},
volume = {55},
year = {2004}
}
@book{Perkell1986,
abstract = {Proceedings, in honor of Kenneth Stevens, of the Symposium on Invariance and Variability of Speech Processes, held Oct. 8-10, 1983 at M.I.T.},
author = {Perkell, Joseph S. and Klatt, Dennis H. and Stevens, Kenneth N. and {Symposium on Invariance and Variability of Speech Processes (1983 : Massachusetts Institute of Technology)}},
isbn = {0898595452},
pages = {604},
publisher = {Lawrence Erlbaum Associates},
title = {{Invariance and variability in speech processes}},
year = {1986}
}
@article{Dresher2008,
abstract = {I will show that phonologists have vacillated between two different and incompatible approaches to determining whether a feature is contrastive in any particular phoneme. One approach involves extracting contrastive features from fully-specified minimal pairs. I will show that this approach is provably untenable. A second approach arrives at contrastive specifications by ordering features into a hierarchy, and splitting up the inventory by successive divisions until all phonemes have been distinguished. I will show that this hierarchical approach solves the problems encountered by the minimal-pairs method. Moreover, a hierarchical approach to contrastiveness is implicit in much descriptive phonological practice, and can be found even in the work of theorists who argue against it. Given the centrality of the issue, it is remarkable that it has received almost no attention in the literature. Recovering this missing chapter of phonological theory sheds new light on a number of controversies over contrast in phonology.},
author = {Dresher, B Elan},
doi = {10.1017/CBO9780511642005},
isbn = {9780521889735},
issn = {1718-3510},
journal = {Contrast in phonology: theory, perception, acquisition},
pages = {11},
title = {{The contrastive hierarchy in phonology}},
volume = {13},
year = {2008}
}
@article{Liberman1967,
author = {Liberman, A M and Cooper, F S and Shankweiler, D P and Studdert-Kennedy, M},
issn = {0033-295X},
journal = {Psychological review},
month = {nov},
number = {6},
pages = {431--61},
pmid = {4170865},
title = {{Perception of the speech code.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/4170865},
volume = {74},
year = {1967}
}
@article{Ohla,
abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
issn = {1072-0502},
journal = {Learning {\&} memory (Cold Spring Harbor, N.Y.)},
number = {4},
pages = {347--62},
pmid = {10509706},
title = {{Bilateral ablation of auditory cortex in Mongolian gerbil affects discrimination of frequency modulated tones but not of pure tones.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC311295},
volume = {6}
}
@book{Saussure1916,
address = {Lausanne, Paris},
author = {de Saussure, Ferdinand},
editor = {Bally, C and Sechehaye, A and Reidlinger, A},
publisher = {Payot},
title = {{Cours de linguistique générale.}},
year = {1916}
}
@book{Feynman1998,
abstract = {From 1983 to 1986, the legendary physicist and teacher Richard Feynman gave a course at Caltech called "Potentialities and Limitations of Computing Machines." Although the lectures are over ten years old, most of the material is timeless and presents a "Feynmanesque" overview of many standard and some not-so-standard topics in computer science. These include compatibility, Turing machines (or as Feynman said, "Mr. Turing's machines"), information theory, Shannon's Theorem, reversible computation, the thermodynamics of computation, the quantum limits to computation, and the physics of VLSI devices. Taken together, these lectures represent a unique exploration of the fundamental limitations of digital computers. Feynman's philosophy of learning and discovery comes through strongly in these lectures. He constantly points out the benefits of playing around with concepts and working out solutions to problems on your own - before looking at the back of the book for the answers. As Feynman says in the lectures: "If you keep proving stuff that others have done, getting confidence, increasing the complexities of your solutions - for the fun of it - then one day you'll turn around and discover that nobody actually did that one! And that's the way to become a computer scientist."},
address = {Boston, MA},
author = {Feynman, Richard P.},
editor = {Hey, J.G. and Allen, Robin W.},
isbn = {0201386283},
publisher = {Addison-Wesley Longman Publishing Co.},
title = {{Feynman Lectures on Computation}},
year = {1998}
}
@article{Buchman1986,
abstract = {Since its original description the diagnosis of word deafness has been greatly expanded. Confusion has arisen with regard to the usage of the related terms pure word deafness, auditory agnosia, and cortical deafness. Three new cases of word deafness are presented including one case with CT and necropsy correlation. These cases are compared with 34 previously reported cases of various cortical auditory disorders. Our review establishes that patients with word deafness who have had formal testing of linguistic and non-linguistic sound comprehension and musical abilities always demonstrated a more pervasive auditory agnosia. Despite the spectrum of auditory deficits and associated language abnormalities, patients with word deafness share common features including aetiology, pathology, clinical presentation and course. These common features justify inclusion of heterogeneous cortical auditory disorders under the rubric of word deafness. Despite some limitations the term "word deafness" should be retained for this syndrome, since inability to comprehend spoken words is the most distinctive clinical deficit. Word deafness is most frequently caused by cerebrovascular accidents of presumed cardiac embolisation, with bitemporal cortico-subcortical lesions. The sequence of cerebral injury is not predictive of resulting auditory deficits. Impairment of musical abilities parallels the severity of the auditory disorder.},
author = {Buchman, A S and Garron, D C and Trost-Cardamone, J E and Wichter, M D and Schwartz, M},
doi = {10.1136/jnnp.49.5.489},
isbn = {0022-3050 (Print)$\backslash$r0022-3050 (Linking)},
issn = {0022-3050},
journal = {Journal of Neurology, Neurosurgery {\&} Psychiatry},
number = {5},
pages = {489--499},
pmid = {2423648},
title = {{Word deafness: one hundred years later.}},
url = {http://jnnp.bmj.com/cgi/doi/10.1136/jnnp.49.5.489},
volume = {49},
year = {1986}
}
@article{Bizley2013,
abstract = {The fundamental perceptual unit in hearing is the 'auditory object'. Similar to visual objects, auditory objects are the computational result of the auditory system's capacity to detect, extract, segregate and group spectrotemporal regularities in the acoustic environment; the multitude of acoustic stimuli around us together form the auditory scene. However, unlike the visual scene, resolving the component objects within the auditory scene crucially depends on their temporal structure. Neural correlates of auditory objects are found throughout the auditory system. However, neural responses do not become correlated with a listener's perceptual reports until the level of the cortex. The roles of different neural structures and the contribution of different cognitive states to the perception of auditory objects are not yet fully understood.},
author = {Bizley, Jennifer K and Cohen, Yale E},
doi = {10.1038/nrn3565},
file = {::},
issn = {1471-0048},
journal = {Nature reviews. Neuroscience},
month = {oct},
number = {10},
pages = {693--707},
pmid = {24052177},
publisher = {NIH Public Access},
title = {{The what, where and how of auditory-object perception.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24052177 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4082027},
volume = {14},
year = {2013}
}
@article{Lein2007,
abstract = {Molecular approaches to understanding the functional circuitry of the nervous system promise new insights into the relationship between genes, brain and behaviour. The cellular diversity of the brain necessitates a cellular resolution approach towards understanding the functional genomics of the nervous system. We describe here an anatomically comprehensive digital atlas containing the expression patterns of approximately 20,000 genes in the adult mouse brain. Data were generated using automated high-throughput procedures for in situ hybridization and data acquisition, and are publicly accessible online. Newly developed image-based informatics tools allow global genome-scale structural analysis and cross-correlation, as well as identification of regionally enriched genes. Unbiased fine-resolution analysis has identified highly specific cellular markers as well as extensive evidence of cellular heterogeneity not evident in classical neuroanatomical atlases. This highly standardized atlas provides an open, primary data resource for a wide variety of further studies concerning brain organization and function.},
author = {Lein, Ed S and Hawrylycz, M J and Ao, N and Ayres, M and Bensinger, A and Bernard, A and Boe, A F and Boguski, M S and Brockway, K S and Byrnes, E J and Chen, L and Chen, T M and Chin, M C and Chong, J and Crook, B E and Czaplinska, A and Dang, C N and Datta, S and Dee, N R and Desaki, A L and Desta, T and Diep, E and Dolbeare, T A and Donelan, M J and Dong, H W and Dougherty, J G and Duncan, B J and Ebbert, A J and Eichele, G and Estin, L K and Faber, C and Facer, B A and Fields, R and Fischer, S R and Fliss, T P and Frensley, C and Gates, S N and Glattfelder, K J and Halverson, K R and Hart, M R and Hohmann, J G and Howell, M P and Jeung, D P and Johnson, R A and Karr, P T and Kawal, R and Kidney, J M and Knapik, R H and Kuan, C L and Lake, J H and Laramee, A R and Larsen, K D and Lau, C and Lemon, T A and Liang, A J and Liu, Y and Luong, L T and Michaels, J and Morgan, J J and Morgan, R J and Mortrud, M T and Mosqueda, N F and Ng, L L and Ng, R and Orta, G J and Overly, C C and Pak, T H and Parry, S E and Pathak, S D and Pearson, O C and Puchalski, R B and Riley, Z L and Rockett, H R and Rowland, S A and Royall, J J and Ruiz, M J and Sarno, N R and Schaffnit, K and Shapovalova, N V and Sivisay, T and Slaughterbeck, C R and Smith, S C and Smith, K A and Smith, B I and Sodt, A J and Stewart, N N and Stumpf, K R and Sunkin, S M and Sutram, M and Tam, A and Teemer, C D and Thaller, C and Thompson, C L and Varnam, L R and Visel, A and Whitlock, R M and Wohnoutka, P E and Wolkey, C K and Wong, V Y and Wood, M and Yaylaoglu, M B and Young, R C and Youngstrom, B L and Yuan, X F and Zhang, B and Zwingman, T A and Jones, A R},
doi = {10.1038/nature05453},
isbn = {1476-4687 (Electronic)},
issn = {0028-0836},
journal = {Nature},
keywords = {*Gene Expression Profiling,*Gene Expression Regulation,Animals,Brain/anatomy {\&} histology/cytology/*metabolism,Computational Biology,Genome/*genetics,Genomics,Hippocampus/anatomy {\&} histology/metabolism,Inbred C57BL,Male,Messenger/genetics/metabolism,Mice,Organ Specificity,RNA},
number = {7124},
pages = {168--176},
pmid = {17151600},
title = {{Genome-wide atlas of gene expression in the adult mouse brain}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve{\&}db=PubMed{\&}dopt=Citation{\&}list{\_}uids=17151600},
volume = {445},
year = {2007}
}
@article{Holt2010,
author = {Holt, L. L. and Lotto, A. J.},
doi = {10.3758/APP.72.5.1218},
file = {::},
issn = {1943-3921},
journal = {Attention, Perception {\&} Psychophysics},
month = {jul},
number = {5},
pages = {1218--1227},
publisher = {Springer-Verlag},
title = {{Speech perception as categorization}},
url = {http://www.springerlink.com/index/10.3758/APP.72.5.1218},
volume = {72},
year = {2010}
}
@misc{Dooling1995,
abstract = {Discrimination of three synthetic versions of a /ra–la/ speech continuum was studied in two species of birds. The stimuli used in these experiments were identical to those used in a previous study of speech perception by humans [Best et al., Percept. Psychophys. 45, 237–250 (1989)]. Budgerigars and zebra finches were trained using operant conditioning and tested on three different series of acoustic stimuli: three‐formant synthetic speech, sinewave versions of those tokens, and isolated F3 tones from the sinewave speech. Both species showed enhanced discrimination performance near the /l/–/r/ boundary in the full‐formant speech continuum, whereas for the F3 continuum, neither species showed a peak near this boundary. These results are similar to human discrimination of the same continua. Budgerigars also showed a peak in discrimination of the sinewave analog continuum paralleling that for full‐formant syllables, similar to humans who are induced to perceive sinewave speech as speech. Zebra finches, by contrast, showed a relatively flat function mirroring their performance for F3 sinewaves, similar to humans who are induced to perceive sinewave speech as nonspeech. These data provide new evidence of species similarities and differences in the discrimination of speech and speechlike sounds. These data also strengthen and refine previous findings on the sensitivities of the vertebrate auditory system to the acoustic distinctions between speechsound categories.},
author = {Dooling, Robert J and Best, Catherine T. and Brown, Susan D.},
booktitle = {The Journal of the Acoustical Society of America},
doi = {10.1121/1.412058},
isbn = {0001-4966 (Print)},
issn = {00014966},
number = {3},
pages = {1839--1846},
pmid = {7699165},
title = {{Discrimination of synthetic full-formant and sinewave /ra–la/ continua by budgerigars (Melopsittacus undulatus) and zebra finches (Taeniopygia guttata)}},
url = {http://scitation.aip.org/content/asa/journal/jasa/97/3/10.1121/1.412058},
volume = {97},
year = {1995}
}
@article{Rauschecker2009a,
abstract = {Speech and language are considered uniquely human abilities: animals have communication systems, but they do not match human linguistic skills in terms of recursive structure and combinatorial power. Yet, in evolution, spoken language must have emerged from neural mechanisms at least partially available in animals. In this paper, we will demonstrate how our understanding of speech perception, one important facet of language, has profited from findings and theory in nonhuman primate studies. Chief among these are physiological and anatomical studies showing that primate auditory cortex, across species, shows patterns of hierarchical structure, topographic mapping and streams of functional processing. We will identify roles for different cortical areas in the perceptual processing of speech and review functional imaging work in humans that bears on our understanding of how the brain decodes and monitors speech. A new model connects structures in the temporal, frontal and parietal lobes linking speech perception and production.},
author = {Rauschecker, Josef P and Scott, Sophie K},
doi = {10.1038/nn.2331},
file = {::},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Anatomy,Animals,Auditory Cortex,Auditory Cortex: anatomy {\&} histology,Auditory Cortex: physiology,Auditory Pathways,Auditory Pathways: anatomy {\&} histology,Auditory Pathways: physiology,Biological Evolution,Brain Mapping,Comparative,Humans,Models,Nerve Net,Nerve Net: anatomy {\&} histology,Nerve Net: physiology,Neurological,Primates,Primates: anatomy {\&} histology,Primates: physiology,Speech Perception,Speech Perception: physiology},
month = {jun},
number = {6},
pages = {718--724},
pmid = {19471271},
title = {{Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing}},
url = {http://www.nature.com/doifinder/10.1038/nn.2331},
volume = {12},
year = {2009}
}
@article{Wang2005a,
abstract = {It has been well documented that neurons in the auditory cortex of anaesthetized animals generally display transient responses to acoustic stimulation, and typically respond to a brief stimulus with one or fewer action potentials. The number of action potentials evoked by each stimulus usually does not increase with increasing stimulus duration. Such observations have long puzzled researchers across disciplines and raised serious questions regarding the role of the auditory cortex in encoding ongoing acoustic signals. Contrary to these long-held views, here we show that single neurons in both primary (area A1) and lateral belt areas of the auditory cortex of awake marmoset monkeys (Callithrix jacchus) are capable of firing in a sustained manner over a prolonged period of time, especially when they are driven by their preferred stimuli. In contrast, responses become more transient or phasic when auditory cortex neurons respond to non-preferred stimuli. These findings suggest that when the auditory cortex is stimulated by a sound, a particular population of neurons fire maximally throughout the duration of the sound. Responses of other, less optimally driven neurons fade away quickly after stimulus onset. This results in a selective representation of the sound across both neuronal population and time.},
author = {Wang, Xiaoqin and Lu, Thomas and Snider, Ross K and Liang, Li},
doi = {10.1038/nature03565},
file = {::;::},
isbn = {1476-4687 (Electronic)$\backslash$n0028-0836 (Linking)},
issn = {1476-4687},
journal = {Nature},
keywords = {Acoustic Stimulation,Action Potentials,Action Potentials: physiology,Animals,Auditory Cortex,Auditory Cortex: cytology,Auditory Cortex: physiology,Auditory Perception,Auditory Perception: physiology,Callithrix,Callithrix: physiology,Models,Neurological,Neurons,Neurons: physiology,Sound,Time Factors,Wakefulness,Wakefulness: physiology},
number = {7040},
pages = {341--6},
pmid = {15902257},
title = {{Sustained firing in auditory cortex evoked by preferred stimuli.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15902257},
volume = {435},
year = {2005}
}
@article{Goldinger2003,
abstract = {Although speech signals are continuous and variable, listeners experience segmentation and linguistic structure in perception. For years, researchers have tried to identify the basic building-block of speech perception. In that time, experimental methods have evolved, constraints on stimulus materials have evolved, sources of variance have been identified, and computational models have been advanced. As a result, the slate of candidate units has increased, each with its own empirical support. In this article, we endorse Grossberg's adaptive resonance theory (ART), proposing that speech units are emergent properties of perceptual dynamics. By this view, units only "exist" when disparate features achieve resonance, a level of perceptual coherence that allows conscious encoding. We outline basic principles of ART, then summarize five experiments. Three experiments assessed the power of social influence to affect phonemesyllable competitions. Two other experiments assessed repetition effects in monitoring data. Together the data suggest that "primary" speech units are strongly and symmetrically affected by bottom-up and top-down knowledge sources. ?? 2003 Elsevier Ltd. All rights reserved.},
author = {Goldinger, Stephen D. and Azuma, Tamiko},
doi = {10.1016/S0095-4470(03)00030-5},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Goldinger, Azuma - 2003 - Puzzle-solving science The quixotic quest for units in speech perception(2).pdf:pdf},
isbn = {0095-4470},
issn = {00954470},
journal = {Journal of Phonetics},
number = {3-4},
pages = {305--320},
pmid = {18292779},
title = {{Puzzle-solving science: The quixotic quest for units in speech perception}},
volume = {31},
year = {2003}
}
@article{LIBERMAN1957,
author = {LIBERMAN, A M and HARRIS, K S and HOFFMAN, H S and GRIFFITH, B C},
issn = {0022-1015},
journal = {Journal of experimental psychology},
keywords = {HEARING,SPEECH},
month = {nov},
number = {5},
pages = {358--68},
pmid = {13481283},
title = {{The discrimination of speech sounds within and across phoneme boundaries.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/13481283},
volume = {54},
year = {1957}
}
@article{Howard2009,
author = {Howard, James D and Plailly, Jane and Grueschow, Marcus and Haynes, John-Dylan and Gottfried, Jay A},
doi = {10.1038/nn.2324},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {jul},
number = {7},
pages = {932--938},
publisher = {Nature Publishing Group},
title = {{Odor quality coding and categorization in human posterior piriform cortex}},
url = {http://www.nature.com/doifinder/10.1038/nn.2324},
volume = {12},
year = {2009}
}
@incollection{Farnetani1990,
address = {Dordrecht},
author = {Farnetani, E.},
booktitle = {Speech Production and Speech Modelling},
doi = {10.1007/978-94-009-2037-8_5},
file = {::},
pages = {93--130},
publisher = {Springer Netherlands},
title = {{V-C-V Lingual Coarticulation and Its Spatiotemporal Domain}},
url = {http://www.springerlink.com/index/10.1007/978-94-009-2037-8{\_}5},
year = {1990}
}
@misc{Stevens1989,
abstract = {This is a review of regularities which we have observed in the analysis of text reading, mostly Swedish, directed to the timing of vowels and consonants, syllables, inter-stress intervals and pauses. We have found tendencies of quantal aspects of temporal structure, superimposed on more gradual variations, which add quasi-rhythmical elements to speech. A local average of inter-stress intervals of the order of 0.5 sec. appears to function as a reference quantum for the planning of pause durations. A recent study, confirming our previous findings of multiple peaks with about 0.5 sec. spacing in histograms of pause durations, provides support for this model. It is well established that pause durations tend to increase with increasing syntactic level of boundaries. However, these variations tend to be quantally scaled, even within a specific boundary category, e.g. between sentences or between paragraphs. Relatively short pauses, as between phrases or clauses, show durations in complementary relation to terminal lengthening. There are indications of approximately 1, 1/2, 1/4 and 1/8 ratios of the average durations of inter-stress intervals, stressed syllables, unstressed syllables and phoneme segments, which add to the observed regularities. The timing of syllables and phonetic segments, with due regard to relative distinctiveness and reading speed, is discussed, and also tempo variations within a sentence},
author = {Stevens, K. N},
booktitle = {Journal of Phonetics},
doi = {10.1109/ICSLP.1996.607202},
isbn = {0-7803-3555-4},
issn = {00954470},
pages = {3--45},
pmid = {982},
title = {{On the quantal nature of speech}},
volume = {17},
year = {1989}
}
@article{Newell2002,
abstract = {We report three experiments where the categorical perception of familiar, three-dimensional objects was investigated. A continuum of shape change between 15 pairs of objects was created and the images along the continuum were used as stimuli. In Experiment 1 participants were first required to discriminate pairs of images of objects that lay along the shape continuum. Then participants were asked to classify each morph-image into one of two pre-specified classes. We found evidence for categorical perception in some but not all of our object pairs. In Experiment 2 we varied the viewpoint of the objects in the discrimination task and found that effects of categorical perception generalized across changes in view. In Experiment 3 similarity ratings for each object pair were collected. These similarity scores correlated with the degree of perceptual categorization found for the object pairs. Our findings suggest that some familiar objects are perceived categorically and that categorical perception is closely tied to inter-object perceptual similarity.},
author = {Newell, Fiona N and B{\"{u}}lthoff, Heinrich H},
doi = {10.1016/S0010-0277(02)00104-X},
file = {::},
issn = {00100277},
journal = {Cognition},
number = {2},
pages = {113--143},
title = {{Categorical perception of familiar objects}},
volume = {85},
year = {2002}
}
@article{Davis2007,
abstract = {This paper focuses on the cognitive and neural mechanisms of speech perception: the rapid, and highly automatic processes by which complex time-varying speech signals are perceived as sequences of meaningful linguistic units. We will review four processes that contribute to the perception of speech: perceptual grouping, lexical segmentation, perceptual learning and categorical perception, in each case presenting perceptual evidence to support highly interactive processes with top-down information flow driving and constraining interpretations of spoken input. The cognitive and neural underpinnings of these interactive processes appear to depend on two distinct representations of heard speech: an auditory, echoic representation of incoming speech, and a motoric/somatotopic representation of speech as it would be produced. We review the neuroanatomical system supporting these two key properties of speech perception and discuss how this system incorporates interactive processes and two parallel echoic and somato-motoric representations, drawing on evidence from functional neuroimaging studies in humans and from comparative anatomical studies. We propose that top-down interactive mechanisms within auditory networks play an important role in explaining the perception of spoken language. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Davis, Matthew H. and Johnsrude, Ingrid S.},
doi = {10.1016/j.heares.2007.01.014},
file = {::},
isbn = {0378-5955 (Print)$\backslash$r0378-5955 (Linking)},
issn = {03785955},
journal = {Hearing Research},
keywords = {Auditory cortex,Categorical perception,Feedback,Frontal lobe,Lexical segmentation,Perceptual grouping,Perceptual learning,Speech perception,Temporal lobe,fMRI},
number = {1-2},
pages = {132--147},
pmid = {17317056},
title = {{Hearing speech sounds: Top-down influences on the interface between audition and speech perception}},
volume = {229},
year = {2007}
}
@unpublished{Knight2014,
abstract = {The tactile surface forms a continuous sheet covering the body. And yet, the perceived distance between two touches varies across stimulation sites. Perceived tactile distance is larger when stimuli cross over the wrist, compared to when both fall on either the hand or the forearm. This effect could reflect a categorical distortion of tactile space across body-part boundaries (in which stimuli crossing the wrist boundary are perceptually elongated) or may simply reflect a localised increased in acuity surrounding anatomical landmarks (in which stimuli near the wrist are perceptually elongated). We tested these two interpretations across two experiments, by comparing a well-documented bias to perceive mediolateral tactile distances across the forearm/hand as larger than proximodistal ones along the forearm/hand at three different sites (hand, wrist, and forearm). According to the ‘categorical' interpretation, tactile distances should be elongated selectively in the proximodistal axis thus reducing the anisotropy. According to the ‘localised acuity' interpretation, distances will be perceptually elongated in the vicinity of the wrist regardless of orientation, leading to increased overall size without affecting anisotropy. Consistent with the categorical account, we found a reduction in the magnitude of anisotropy at the wrist, with no evidence of a corresponding localised increase in precision. These findings demonstrate that we reference touch to a representation of the body that is categorically segmented into discrete parts, which consequently influences the perception of tactile distance.},
author = {Knight, Frances Le Cornu and Longo, Matthew R. and Bremner, Andrew J.},
booktitle = {Cognition},
doi = {10.1016/j.cognition.2014.01.005},
file = {::},
issn = {00100277},
number = {2},
pages = {254--262},
title = {{Categorical perception of tactile distance}},
volume = {131},
year = {2014}
}
@article{Repp1989,
abstract = {This study focuses on the initial component of the stop consonant release burst, the release transient. In theory, the transient, because of its impulselike source, should contain much information about the vocal tract configuration at release, but it is usually weak in intensity and difficult to isolate from the accompanying frication in natural speech. For this investigation, a human talker produced isolated release transients of /b,d,g/ in nine vocalic contexts by whispering these syllables very quietly. He also produced the corresponding CV syllables with regular phonation for comparison. Spectral analyses showed the isolated transients to have a clearly defined formant structure, which was not seen in natural release bursts, whose spectra were dominated by the frication noise. The formant frequencies varied systematically with both consonant place of articulation and vocalic context. Perceptual experiments showed that listeners can identify both consonants and vowels from isolated transients, though not very accurately. Knowing one of the two segments in advance did not help, but when the transients were followed by a compatible synthetic, steady-state vowel, consonant identification improved somewhat. On the whole, isolated transients, despite their clear formant structure, provided only partial information for consonant identification, but no less so, it seems, than excerpted natural release bursts. The information conveyed by artificially isolated transients and by natural (frication-dominated) release bursts appears to be perceptually equivalent.},
author = {Repp, B H and Lin, H B},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {jan},
number = {1},
pages = {379--96},
pmid = {2921420},
title = {{Acoustic properties and perception of stop consonant release transients.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2921420},
volume = {85},
year = {1989}
}
@book{James1890,
address = {New York},
author = {James, William},
publisher = {H. Holt},
title = {{The Principles of Psychology, Vol. 1}},
year = {1890}
}
@incollection{Harnad1987,
abstract = {Categorization is a very basic cognitive activity. It is involved in any task that calls for differential responding, from operant discrimination to pattern recognition to naming and describing objects and states-of-affairs. Explanations of categorization range from nativist theories denying that any nontrivial categories are acquired by learning to inductivist theories claiming that most categories are learned. "Categorical perception" (CP) is the name given to a suggestive perceptual phenomenon that may serve as a useful model for categorization in general: For certain perceptual categories, within-category differences look much smaller than between-category differences even when they are of the same size physically. For example, in color perception, differences between reds and differences between yellows look much smaller than equal-sized differences that cross the red/yellow boundary; the same is true of the phoneme categories /ba/ and /da/. Indeed, the effect of the category boundary is not merely quantitative, but qualitative.},
author = {Harnad, Stevan},
booktitle = {Categorical Perception: The Groundwork of Cognition},
pages = {1--52},
publisher = {Cambridge University Press},
title = {{Psychophysical and cognitive aspects of categorical perception: A critical overview}},
year = {1987}
}
@book{Collins2003,
abstract = {Routledge English Language Introductions cover core areas of language study and are one-stop resources for students.Assuming no prior knowledge, books in the series offer an accessible overview of the subject, with activities, study questions, sample analyses, commentaries and key readings—all in the same volume. The innovative and flexible 'two-dimensional' structure is built around four sections—introduction, development, exploration and extension—which offer self-contained stages for study. Each topic can also be read across these sections, enabling the reader to build gradually on the knowledge gained. Revised and updated throughout, this third edition of Practical Phonetics and Phonology:presents the essentials of the subject and their day-to-day applications in an engaging and accessible manner covers all the core concepts of speech science, such as the phoneme, syllable structure, production of speech, vowel and consonant possibilities, glottal settings, stress, rhythm, intonation and the surprises of connected speechincorporates classic readings from key names in the discipline including David Abercrombie, David Crystal, Dennis Fry, Daniel Jones, Peter Ladefoged, Peter Trudgill and John Wellsincludes an audio CD containing a collection of samples provided by genuine speakers of 25 accent varieties from Britain, Ireland, the USA, Canada, Australia, New Zealand, South Africa, India, Singapore and West Africagives outlines of the sound systems of six key languages from around the worldcontains over a hundred activity exercises, many accompanied by audio materialis accompanied by a brand new companion website featuring additional guidance, audio files, keys to activities in the book, further exercises and activities, and extra practice in phonemic transcriptionNew features of this edition include an additional reading on teaching pronunciation, phonetic descriptions of three more languages (Japanese, Polish and Italian), expanded material on spelling/sound relationships, more information on acquiring the pronunciation of a foreign language, additional suggestions for further reading and much new illustrative material. Written by authors who are experienced teachers and researchers, this best-selling textbook will appeal to all students of English language and linguistics and those training for a certificate in TEFL.},
author = {Collins, Beverley and Mees, Inger},
booktitle = {Routledge English language introductions series},
isbn = {0415261333 (cased) 0415261341 (pbk.)},
keywords = {English language Phonetics.},
pages = {46--61},
title = {{Practical phonetics and phonology : a resource book for students}},
year = {2003}
}
@article{Fugate2013,
abstract = {Categorical perception (CP) refers to how similar things look different depending on whether they are classified as the same category. Many studies demonstrate that adult humans show CP for human emotional faces. It is widely debated whether the effect can be accounted for solely by perceptual differences (structural differences among emotional faces) or whether additional perceiver-based conceptual knowledge is required. In this review, I discuss the phenomenon of CP and key studies showing CP for emotional faces. I then discuss a new model of emotion which highlights how perceptual and conceptual knowledge interact to explain how people see discrete emotions in others' faces. In doing so, I discuss how language (emotion words included in the paradigm) contribute to CP.},
author = {Fugate, Jennifer M. B.},
doi = {10.1177/1754073912451350},
file = {::},
issn = {1754-0739},
journal = {Emotion Review},
keywords = {categorical perception,emotional faces,language},
month = {jan},
number = {1},
pages = {84--89},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Categorical Perception for Emotional Faces}},
url = {http://journals.sagepub.com/doi/10.1177/1754073912451350},
volume = {5},
year = {2013}
}
@article{Tabain2000,
abstract = {Using electropalatographic (EPG) data, the following hypothesis is tested: the slope value generated by a locus equation (LE) analysis of the F 2 transition in CV syllables is an accurate reflection of the degree of coarticulation between the consonant and the vowel in that syllable. The consonants studied are /∂ z z d g n n l r/. Comparisons between EPG and LE data suggest that the LE analysis provides an accurate reflection of the degree of coarticulation for the stop and nasal classes in English, which have an alveolar and a velar place of articulation amongst the lingual consonants. By contrast, the correlation between EPG and LE data for the fricative consonants is very poor. Two explanations are offered for the poorer results for the fricatives: (1) the fricative noise following consonant release obscures the F 2 transition, rendering formant measurement less accurate, and (2) the LE is capable of encoding gross differences in degree of coarticulation (such as that between an alveolar and a velar) but not more subtle differences such as those between the various coronal articulations.},
author = {Tabain, Marija},
doi = {10.1006/jpho.2000.0110},
file = {::},
issn = {00954470},
journal = {Journal of Phonetics},
number = {2},
pages = {137--159},
title = {{Coarticulation in CV syllables: a comparison of Locus Equation and EPG data}},
volume = {28},
year = {2000}
}
@article{Bidelman2013,
abstract = {Speech perception requires the effortless mapping from smooth, seemingly continuous changes in sound features into discrete perceptual units, a conversion exemplified in the phenomenon of categorical perception. Explaining how/when the human brain performs this acoustic-phonetic transformation remains an elusive problem in current models and theories of speech perception. In previous attempts to decipher the neural basis of speech perception, it is often unclear whether the alleged brain correlates reflect an underlying percept or merely changes in neural activity that covary with parameters of the stimulus. Here, we recorded neuroelectric activity generated at both cortical and subcortical levels of the auditory pathway elicited by a speech vowel continuum whose percept varied categorically from /u/ to /a/. This integrative approach allows us to characterize how various auditory structures code, transform, and ultimately render the perception of speech material as well as dissociate brain responses reflecting changes in stimulus acoustics from those that index true internalized percepts. We find that activity from the brainstem mirrors properties of the speech waveform with remarkable fidelity, reflecting progressive changes in speech acoustics but not the discrete phonetic classes reported behaviorally. In comparison, patterns of late cortical evoked activity contain information reflecting distinct perceptual categories and predict the abstract phonetic speech boundaries heard by listeners. Our findings demonstrate a critical transformation in neural speech representations between brainstem and early auditory cortex analogous to an acoustic-phonetic mapping necessary to generate categorical speech percepts. Analytic modeling demonstrates that a simple nonlinearity accounts for the transformation between early (subcortical) brain activity and subsequent cortical/behavioral responses to speech ({\textgreater}. 150-200. ms) thereby describing a plausible mechanism by which the brain achieves its acoustic-to-phonetic mapping. Results provide evidence that the neurophysiological underpinnings of categorical speech are present cortically by {\~{}}. 175. ms after sound enters the ear. ?? 2013 Elsevier Inc..},
author = {Bidelman, Gavin M. and Moreno, Sylvain and Alain, Claude},
doi = {10.1016/j.neuroimage.2013.04.093},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Bidelman, Moreno, Alain - 2013 - Tracing the emergence of categorical speech perception in the human auditory system(2).pdf:pdf},
isbn = {1053-8119},
issn = {10538119},
journal = {NeuroImage},
keywords = {Auditory event-related potentials (ERP),Brainstem response,Categorical perception,Neural computation,Speech perception},
pages = {201--212},
pmid = {23648960},
publisher = {Elsevier Inc.},
title = {{Tracing the emergence of categorical speech perception in the human auditory system}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2013.04.093},
volume = {79},
year = {2013}
}
@article{Feldman2009,
author = {Feldman, Naomi H. and Griffiths, Thomas L. and Morgan, James L.},
doi = {10.1037/a0017196},
file = {::},
issn = {1939-1471},
journal = {Psychological Review},
keywords = {Bayesian inference,categorical perception,noise,perceptual magnet effect,phonetic categorization,rational analysis,speech perception},
number = {4},
pages = {752--782},
publisher = {American Psychological Association},
title = {{The influence of categories on perception: Explaining the perceptual magnet effect as optimal statistical inference.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0017196},
volume = {116},
year = {2009}
}
@article{Howard2000,
author = {Howard, M.A. and Volkov, I.O. and Mirsky, R. and Garell, P.C. and Noh, M.D. and Granner, M. and Damasio, H. and Steinschneider, M. and Reale, R.A. and Hind, J.E. and Brugge, J.F.},
doi = {10.1002/(SICI)1096-9861(20000103)416:1<79::AID-CNE6>3.0.CO;2-2},
file = {::},
issn = {0021-9967},
journal = {The Journal of Comparative Neurology},
keywords = {audition,auditory cortex,hearing},
month = {jan},
number = {1},
pages = {79--92},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Auditory cortex on the human posterior superior temporal gyrus}},
url = {http://doi.wiley.com/10.1002/{\%}28SICI{\%}291096-9861{\%}2820000103{\%}29416{\%}3A1{\%}3C79{\%}3A{\%}3AAID-CNE6{\%}3E3.0.CO{\%}3B2-2},
volume = {416},
year = {2000}
}
@article{Boersma2001,
abstract = {See, stats, and : http : / / www . researchgate . net / publication / 208032992 PRAAT , a computer ARTICLE CITATIONS 942 DOWNLOADS 870 VIEWS 1 , 365 2 : Paul University 104 , 780 SEE David University 19 , 721 SEE Available : Paul Retrieved : 28},
author = {Boersma, Paul},
doi = {10.1097/AUD.0b013e31821473f7},
isbn = {1381-3439},
issn = {0196-0202},
journal = {Glot International},
number = {9/10},
pages = {341--347},
pmid = {61},
title = {{Praat, a system for doing phonetics by computer}},
volume = {5},
year = {2001}
}
@article{Kronrod2016a,
author = {Kronrod, Yakov and Coppess, Emily and Feldman, Naomi H.},
doi = {10.3758/s13423-016-1049-y},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
month = {dec},
number = {6},
pages = {1681--1712},
publisher = {Springer US},
title = {{A unified account of categorical effects in phonetic perception}},
url = {http://link.springer.com/10.3758/s13423-016-1049-y},
volume = {23},
year = {2016}
}
@article{Beale1995,
abstract = {These studies suggest categorical perception effects may be much more general than has commonly been believed and can occur in apparently similar ways at dramatically different levels of processing. To test the nature of individual face representations, a linear continuum of “morphed” faces was generated between individual exemplars of familiar faces. In separate categorization, discrimination and “better-likeness” tasks, subjects viewed pairs of faces from these continua. Subjects discriminate most accurately when face-pairs straddle apparent category boundaries; thus individual faces are perceived categorically. A high correlation is found between the familiarity of a face-pair and the magnitude of the categorization effect. Categorical perception therefore is not limited to low-level perceptual continua, but can occur at higher levels and may be acquired through experience as well.},
author = {Beale, James M. and Keil, Frank C.},
doi = {10.1016/0010-0277(95)00669-X},
file = {::},
issn = {00100277},
journal = {Cognition},
number = {3},
pages = {217--239},
title = {{Categorical effects in the perception of faces}},
volume = {57},
year = {1995}
}
@article{Grossberg2013,
abstract = {Adaptive Resonance Theory, or ART, is a cognitive and neural theory of how the brain autonomously learns to categorize, recognize, and predict objects and events in a changing world. This article reviews classical and recent developments of ART, and provides a synthesis of concepts, principles, mechanisms, architectures, and the interdisciplinary data bases that they have helped to explain and predict. The review illustrates that ART is currently the most highly developed cognitive and neural theory available, with the broadest explanatory and predictive range. Central to ART's predictive power is its ability to carry out fast, incremental, and stable unsupervised and supervised learning in response to a changing world. ART specifies mechanistic links between processes of consciousness, learning, expectation, attention, resonance, and synchrony during both unsupervised and supervised learning. ART provides functional and mechanistic explanations of such diverse topics as laminar cortical circuitry; invariant object and scenic gist learning and recognition; prototype, surface, and boundary attention; gamma and beta oscillations; learning of entorhinal grid cells and hippocampal place cells; computation of homologous spatial and temporal mechanisms in the entorhinal-hippocampal system; vigilance breakdowns during autism and medial temporal amnesia; cognitive-emotional interactions that focus attention on valued objects in an adaptively timed way; item-order-rank working memories and learned list chunks for the planning and control of sequences of linguistic, spatial, and motor information; conscious speech percepts that are influenced by future context; auditory streaming in noise during source segregation; and speaker normalization. Brain regions that are functionally described include visual and auditory neocortex; specific and nonspecific thalamic nuclei; inferotemporal, parietal, prefrontal, entorhinal, hippocampal, parahippocampal, perirhinal, and motor cortices; frontal eye fields; supplementary eye fields; amygdala; basal ganglia: cerebellum; and superior colliculus. Due to the complementary organization of the brain, ART does not describe many spatial and motor behaviors whose matching and learning laws differ from those of ART. ART algorithms for engineering and technology are listed, as are comparisons with other types of models. ?? 2012 Elsevier Ltd.},
author = {Grossberg, Stephen},
doi = {10.1016/j.neunet.2012.09.017},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Grossberg - 2013 - Adaptive Resonance Theory How a brain learns to consciously attend, learn, and recognize a changing world(2).pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Adaptive Resonance Theory,Adaptive timing,Amygdala,Attention,Basal ganglia,Consciousness,Entorhinal cortex,Expectation,Gamma and beta oscillations,Hippocampal cortex,Inferotemporal cortex,Learning,Parietal cortex,Prefrontal cortex,Recognition,Reinforcement learning,Speech perception,Synchrony,Working memory},
pages = {1--47},
pmid = {23149242},
publisher = {Elsevier Ltd},
title = {{Adaptive Resonance Theory: How a brain learns to consciously attend, learn, and recognize a changing world}},
url = {http://dx.doi.org/10.1016/j.neunet.2012.09.017},
volume = {37},
year = {2013}
}
@article{Bird2014,
abstract = {The areas of the brain that encode color categorically have not yet been reliably identified. Here, we used functional MRI adaptation to identify neuronal populations that represent color categories irrespective of metric differences in color. Two colors were successively presented within a block of trials. The two colors were either from the same or different categories (e.g., "blue 1 and blue 2" or "blue 1 and green 1"), and the size of the hue difference was varied. Participants performed a target detection task unrelated to the difference in color. In the middle frontal gyrus of both hemispheres and to a lesser extent, the cerebellum, blood-oxygen level-dependent response was greater for colors from different categories relative to colors from the same category. Importantly, activation in these regions was not modulated by the size of the hue difference, suggesting that neurons in these regions represent color categorically, regardless of metric color difference. Representational similarity analyses, which investigated the similarity of the pattern of activity across local groups of voxels, identified other regions of the brain (including the visual cortex), which responded to metric but not categorical color differences. Therefore, categorical and metric hue differences appear to be coded in qualitatively different ways and in different brain regions. These findings have implications for the long-standing debate on the origin and nature of color categories, and also further our understanding of how color is processed by the brain.},
author = {Bird, Chris M and Berens, Samuel C and Horner, Aidan J and Franklin, Anna},
doi = {10.1073/pnas.1315275111},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {categorization,chromatic,functional magnetic resonance imaging},
month = {mar},
number = {12},
pages = {4590--5},
pmid = {24591602},
publisher = {National Academy of Sciences},
title = {{Categorical encoding of color in the brain.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24591602 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3970503},
volume = {111},
year = {2014}
}
@article{Belin2000a,
abstract = {The human voice contains in its acoustic structure a wealth of information on the speaker's identity and emotional state which we perceive with remarkable ease and accuracy. Although the perception of speaker-related features of voice plays a major role in human communication, little is known about its neural basis. Here we show, using functional magnetic resonance imaging in human volunteers, that voice-selective regions can be found bilaterally along the upper bank of the superior temporal sulcus (STS). These regions showed greater neuronal activity when subjects listened passively to vocal sounds, whether speech or non-speech, than to non-vocal environmental sounds. Central STS regions also displayed a high degree of selectivity by responding significantly more to vocal sounds than to matched control stimuli, including scrambled voices and amplitude-modulated noise. Moreover, their response to stimuli degraded by frequency filtering paralleled the subjects' behavioural performance in voice-perception tasks that used these stimuli. The voice-selective areas in the STS may represent the counterpart of the face-selective areas in human visual cortex; their existence sheds new light on the functional architecture of the human auditory cortex.},
author = {Belin, P and Zatorre, R J and Lafaille, P and Ahad, P and Pike, B},
doi = {10.1038/35002078},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Belin et al. - 2000 - Voice-selective areas in human auditory cortex(3).pdf:pdf;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Belin et al. - 2000 - Voice-selective areas in human auditory cortex(4).pdf:pdf},
isbn = {0028-0836 (Print)},
issn = {0028-0836},
journal = {Nature},
number = {6767},
pages = {309--312},
pmid = {10659849},
title = {{Voice-selective areas in human auditory cortex.}},
volume = {403},
year = {2000}
}
@misc{Ghazanfar1999,
abstract = {In this article, we review behavioral and neurobiological studies of the perception and use of species-specific vocalizations by non-human primates. At the behavioral level, primate vocal perception shares many features with speech perception by humans. These features include a left-hemisphere bias towards conspecific vocalizations, the use of temporal features for identifying different calls, and the use of calls to refer to objects and events in the environment. The putative neural bases for some of these behaviors have been revealed by recent studies of the primate auditory and prefrontal cortices. These studies also suggest homologies with the human language circuitry. Thus, a synthesis of cognitive, ethological and neurobiological approaches to primate vocal behavior is likely to yield the richest understanding of the neural bases of speech perception, and might also shed light on the evolutionary precursors to language.},
author = {Ghazanfar, Asif A. and Hauser, Marc D.},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/S1364-6613(99)01379-0},
isbn = {1364-6613},
issn = {13646613},
number = {10},
pages = {377--384},
pmid = {10498928},
title = {{The neuroethology of primate vocal communication: Substrates for the evolution of speech}},
volume = {3},
year = {1999}
}
@book{Stevens1998,
abstract = {Content Description {\#}Includes bibliographical references (p.) and index. Preface -- 1. Anatomy and physiology of speech production -- 2. Source mechanisms -- 3. Basic acoustics of vocal tract resonators -- 4. Auditory processing of speechlike sounds -- 5. Phonological representation of utterances -- 6. Vowels : acoustic events with a relatively open vocal tract -- 7. The basic stop consonants : bursts and formant transitions -- 8. Obstruent consonants -- 9. Sonorant consonants -- 10. Some influences of context on speech sound production.},
author = {Stevens, Kenneth N.},
isbn = {0262692503},
pages = {607},
publisher = {MIT Press},
title = {{Acoustic phonetics}},
url = {https://books.google.com/books?id=Gej94hCGrLMC{\&}source=gbs{\_}navlinks{\_}s},
year = {1998}
}
@article{Schwartz2012,
abstract = {Vowels are by far the best understood units in human sound systems, and are well characterized at the articulatory, acoustic, and perceptual levels. This has permitted explanations of vowel systems as structured by perception, and has led to effective substance-based theories. By contrast, stops are far less thoroughly understood. In this paper we use an articulatory-acoustic model of the vocal tract to examine stop consonant place in terms of both articulation and formant values. This allows us to locate each place of articulation in the F1-F2-F3 space, and to demonstrate in "articulatory nomograms" how formants evolve while closure is displaced from the front to the back of the vocal tract. Then, in the framework of the "Perception for Action Control Theory" that we have developed in recent years, we show that the near universal labial-coronal-velar stop series (i.e., /b d g/ or /p t k/) is a perceptually optimal structure for stops just as /i a u/ is for vowels, provided that it is embedded in a suitable perceptuo-motor framework. ?? 2011 Elsevier Ltd.},
author = {Schwartz, Jean-Luc and Bo{\"{e}}, Louis-Jean and Badin, Pierre and Sawallis, Thomas R.},
doi = {10.1016/j.wocn.2011.10.004},
file = {::},
isbn = {0095-4470},
issn = {00954470},
journal = {Journal of Phonetics},
number = {1},
pages = {20--36},
title = {{Grounding stop place systems in the perceptuo-motor substance of speech: On the universality of the labial–coronal–velar stop series}},
volume = {40},
year = {2012}
}
@article{Dorman1977,
author = {Dorman, M. F. and Studdert-Kennedy, M. and Raphael, L. J.},
doi = {10.3758/BF03198744},
file = {::},
issn = {0031-5117},
journal = {Perception {\&} Psychophysics},
month = {mar},
number = {2},
pages = {109--122},
publisher = {Springer-Verlag},
title = {{Stop-consonant recognition: Release bursts and formant transitions as functionally equivalent, context-dependent cues}},
url = {http://www.springerlink.com/index/10.3758/BF03198744},
volume = {22},
year = {1977}
}
@article{Steinschneider2003,
abstract = {Voice onset time (VOT) signifies the interval between consonant onset and the start of rhythmic vocal-cord vibrations. Differential perception of consonants such as /d/ and /t/ is categorical in American English, with the boundary generally lying at a VOT of 20-40 ms. This study tests whether previously identified response patterns that differentially reflect VOT are maintained in large-scale population activity within primary auditory cortex (A1) of the awake monkey. Multiunit activity and current source density patterns evoked by the syllables /da/ and /ta/ with variable VOTs are examined. Neural representation is determined by the tonotopic organization. Differential response patterns are restricted to lower best-frequency regions. Response peaks time-locked to both consonant and voicing onsets are observed for syllables with a 40- and 60-ms VOT, whereas syllables with a 0- and 20-ms VOT evoke a single response time-locked only to consonant onset. Duration of aspiration noise is represented in higher best-frequency regions. Representation of VOT and aspiration noise in discrete tonotopic areas of A1 suggest that integration of these phonetic cues occurs in secondary areas of auditory cortex. Findings are consistent with the evolving concept that complex stimuli are encoded by synchronized activity in large-scale neuronal ensembles.},
author = {Steinschneider, Mitchell and Fishman, Yonatan I and Arezzo, Joseph C},
doi = {10.1121/1.1582449},
file = {::},
isbn = {00014966},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {307--321},
pmid = {12880043},
title = {{Representation of the voice onset time (VOT) speech parameter in population responses within primary auditory cortex of the awake monkey.}},
volume = {114},
year = {2003}
}
@article{Lindblom2012,
abstract = {A programmatic series of studies aimed at expanding our understanding of coarticulation in V 1{\textperiodcentered}CV 2 sequences is presented. The common thread was examining coarticulatory dynamics through the prism of locus equations (LEs). Multiple experimental methodologies (articulatory synthesis, X-ray film, Principal Component Analysis, and extraction of time constants for F2 transitions), guided by a few theoretical assumptions about speech motor planning and control, were used to uncover the articulatory underpinnings responsible for the trademark acoustic form of LE scatterplots. Specific findings were: (1) the concept of a stop consonantal 'target' was quantitatively derived as a vowel-neutral, 'deactivated,' tongue contour; (2) the linearity of LEs is significantly enhanced by the uniformity of F2 transition time constants, which normalize with respect to F2 transition extents, and an inherent linear bias created by the smaller frequency range of [F2 onset-F2 vowel] relative to F2 vowel frequencies; (3) realistic LE slopes and y-intercepts were derived by modeling different extents of V 2 overlap onto stop consonantal target shapes at closure; and (4) a conceptually simple model, viz. interpolation between successive articulatory target shapes, followed by derivation of their formant values expressed as LEs, came surprisingly close to matching actual LEs obtained from our speaker. {\textcopyright} 2011 Elsevier Ltd.},
author = {Lindblom, Bj{\"{o}}rn and Sussman, Harvey M.},
doi = {10.1016/j.wocn.2011.09.005},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Lindblom, Sussman - 2012 - Dissecting coarticulation How locus equations happen(2).pdf:pdf},
isbn = {0095-4470},
issn = {00954470},
journal = {Journal of Phonetics},
number = {1},
pages = {1--19},
title = {{Dissecting coarticulation: How locus equations happen}},
volume = {40},
year = {2012}
}
@article{Chang2010,
abstract = {Speech perception requires the rapid and effortless extraction of meaningful phonetic information from a highly variable acoustic signal. A powerful example of this phenomenon is categorical speech perception, in which a continuum of acoustically varying sounds is transformed into perceptually distinct phoneme categories. We found that the neural representation of speech sounds is categorically organized in the human posterior superior temporal gyrus. Using intracranial high-density cortical surface arrays, we found that listening to synthesized speech stimuli varying in small and acoustically equal steps evoked distinct and invariant cortical population response patterns that were organized by their sensitivities to critical acoustic features. Phonetic category boundaries were similar between neurometric and psychometric functions. Although speech-sound responses were distributed, spatially discrete cortical loci were found to underlie specific phonetic discrimination. Our results provide direct evidence for acoustic-to-higher order phonetic level encoding of speech sounds in human language receptive cortex.},
author = {Chang, Edward F and Rieger, Jochem W and Johnson, Keith and Berger, Mitchel S and Barbaro, Nicholas M and Knight, Robert T},
doi = {10.1038/nn.2641},
file = {::},
issn = {1546-1726},
journal = {Nature neuroscience},
month = {nov},
number = {11},
pages = {1428--32},
pmid = {20890293},
publisher = {NIH Public Access},
title = {{Categorical speech representation in human superior temporal gyrus.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20890293 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2967728},
volume = {13},
year = {2010}
}
@article{Sussman1998,
abstract = {Neuroethological investigations of mammalian and avian auditory systems have documented species-specific specializations for processing complex acoustic signals that could, if viewed in abstract terms, have an intriguing and striking relevance for human speech sound categorization and representation. Each species forms biologically relevant categories based on combinatorial analysis of information-bearing parameters within the complex input signal. This target article uses known neural models from the mustached bat and barn owl to develop, by analogy, a conceptualization of human processing of consonant plus vowel sequences that offers a partial solution to the noninvariance dilemma--the nontransparent relationship between the acoustic waveform and the phonetic segment. Critical input sound parameters used to establish species-specific categories in the mustached bat and barn owl exhibit high correlation and linearity due to physical laws. A cue long known to be relevant to the perception of stop place of articulation is the second formant (F2) transition. This article describes an empirical phenomenon--the locus equations--that describes the relationship between the F2 of a vowel and the F2 measured at the onset of a consonant-vowel (CV) transition. These variables, F2 onset and F2 vowel within a given place category, are consistently and robustly linearly correlated across diverse speakers and languages, and even under perturbation conditions as imposed by bite blocks. A functional role for this category-level extreme correlation and linearity (the "orderly output constraint") is hypothesized based on the notion of an evolutionarily conserved auditory-processing strategy. High correlation and linearity between critical parameters in the speech signal that help to cue place of articulation categories might have evolved to satisfy a preadaptation by mammalian auditory systems for representing tightly correlated, linearly related components of acoustic signals.},
author = {Sussman, Harvey M and Fruchter, David and Hilbert, Jon and Sirosh, Joseph},
doi = {10.1017/S0140525X98001174},
file = {::;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Sussman et al. - 1998 - Linear correlates in the speech signal the orderly output constraint(3).pdf:pdf},
isbn = {0140-525X},
issn = {0140-525X},
journal = {The Behavioral and brain sciences},
keywords = {Humans,Phonetics,Speech,Speech Acoustics,Speech Perception,Speech Perception: physiology,Speech: physiology,acoustic,and frustration,because they,categories,linearity,locus equations,must,must not tolerate is,neuroethology,noninvariance,perception,phoneme,place of articulation,scientists do tolerate uncertainty,sound,speech signal,the one thing that,they do not and},
number = {2},
pages = {241--59; discussion 260--99},
pmid = {10097014},
publisher = {University of Oregon Library},
title = {{Linear correlates in the speech signal: the orderly output constraint.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10097014},
volume = {21},
year = {1998}
}
@article{Kuhl1992,
author = {Kuhl, PK and Williams, KA and Lacerda, F and Stevens, KN and Lindblom, B},
journal = {Science},
number = {5044},
title = {{Linguistic experience alters phonetic perception in infants by 6 months of age}},
volume = {255},
year = {1992}
}
@article{Syka2002,
abstract = {Gap detection threshold (GDT) was measured in adult female pigmented rats (strain Long–Evans) by an operant conditioning technique with food reinforcement, before and after bilateral ablation of the auditory cortex. GDT was dependent on the frequency spectrum and intensity of the continuously present noise in which the gaps were embedded. The mean values of GDT for gaps embedded in white noise or low-frequency noise (upper cutoff frequency 3 kHz) at 70 dB sound pressure level (SPL) were 1.57±0.07 ms and 2.9±0.34 ms, respectively. Decreasing noise intensity from 80 dB SPL to 20 dB SPL produced a significant increase in GDT. The increase in GDT was relatively small in the range of 80–50 dB SPL for white noise and in the range of 80–60 dB for low-frequency noise. The minimal intensity level of the noise that enabled GDT measurement was 20 dB SPL for white noise and 30 dB SPL for low-frequency noise. Mean GDT values at these intensities were 10.6±3.9 ms and 31.3±4.2 ms, respectively. Bilateral ablation of the primary auditory cortex (complete destruction of the Te1 and partial destruction of the Te2 and Te3 areas) resulted in an increase in GDT values. The fifth day after surgery, the rats were able to detect gaps in the noise. The values of GDT observed at this time were 4.2±1.1 ms for white noise and 7.4±3.1 ms for low-frequency noise at 70 dB SPL. During the first month after cortical ablation, recovery of GDT was observed. However, 1 month after cortical ablation GDT still remained slightly higher than in controls (1.8±0.18 for white noise, 3.22±0.15 for low-frequency noise, P{\textless}0.05). A decrease in GDT values during the subsequent months was not observed.},
author = {Syka, J and Rybalko, N and Mazelov{\'{a}}, J and Druga, R},
doi = {10.1016/S0378-5955(02)00578-6},
issn = {03785955},
journal = {Hearing Research},
number = {1},
pages = {151--159},
title = {{Gap detection threshold in the rat before and after auditory cortex ablation}},
url = {http://www.sciencedirect.com/science/article/pii/S0378595502005786},
volume = {172},
year = {2002}
}
@article{Ohl1999a,
abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
doi = {10.1101/LM.6.4.347},
file = {::},
issn = {1072-0502},
journal = {Learning {\&} memory (Cold Spring Harbor, N.Y.)},
number = {4},
pages = {347--62},
pmid = {10509706},
publisher = {Cold Spring Harbor Laboratory Press},
title = {{Bilateral ablation of auditory cortex in Mongolian gerbil affects discrimination of frequency modulated tones but not of pure tones.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC311295},
volume = {6},
year = {1999}
}
@article{Stilp2010,
abstract = {Speech sounds are traditionally divided into consonants and vowels.$\backslash$r$\backslash$nWhen only vowels or only consonants are replaced by noise, listeners$\backslash$r$\backslash$nare more accurate understanding sentences in which consonants are$\backslash$r$\backslash$nreplaced but vowels remain. From such data, vowels have been$\backslash$r$\backslash$nsuggested to be more important for understanding sentences;$\backslash$r$\backslash$nhowever, such conclusions are mitigated by the fact that replaced$\backslash$r$\backslash$nconsonant segments were roughly one-third shorter than vowels.$\backslash$r$\backslash$nWe report two experiments that demonstrate listener performance$\backslash$r$\backslash$nto be better predicted by simple psychoacoustic measures of cochleascaled$\backslash$r$\backslash$nspectral change across time. First, listeners identified sentences$\backslash$r$\backslash$nin which portions of consonants (C), vowels (V), CV transitions,$\backslash$r$\backslash$nor VC transitions were replaced by noise. Relative intelligibility was$\backslash$r$\backslash$nnot well accounted for on the basis of Cs, Vs, or their transitions. In$\backslash$r$\backslash$na second experiment, distinctions between Cs and Vs were abandoned.$\backslash$r$\backslash$nInstead, portions of sentences were replaced on the basis of$\backslash$r$\backslash$ncochlea-scaled spectral entropy (CSE). Sentence segments having$\backslash$r$\backslash$nrelatively high, medium, or low entropy were replaced with noise.$\backslash$r$\backslash$nIntelligibility decreased linearly as the amount of replaced CSE$\backslash$r$\backslash$nincreased. Duration of signal replaced and proportion of consonants/vowels$\backslash$r$\backslash$nreplaced fail to account for listener data. CSE corresponds$\backslash$r$\backslash$nclosely with the linguistic construct of sonority (or vowellikeness)$\backslash$r$\backslash$nthat is useful for describing phonological systematicity,$\backslash$r$\backslash$nespecially syllable composition. Results challenge traditional distinctions$\backslash$r$\backslash$nbetween consonants and vowels. Speech intelligibility is better$\backslash$r$\backslash$npredicted by nonlinguistic sensory measures of uncertainty (potential$\backslash$r$\backslash$ninformation) than by orthodox physical acoustic measures or$\backslash$r$\backslash$nlinguistic constructs.},
author = {Stilp, Christian E. and Kluender, Keith R.},
doi = {10.1073/pnas.0913625107},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Stilp, Kluender - 2010 - Cochlea-scaled entropy, not consonants, vowels, or time, best predicts speech intelligibility(2).pdf:pdf},
isbn = {0027-8424$\backslash$r1091-6490},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {27},
pages = {12387--12392},
pmid = {20566842},
title = {{Cochlea-scaled entropy, not consonants, vowels, or time, best predicts speech intelligibility}},
volume = {107},
year = {2010}
}
@article{Hickok2007,
author = {Hickok, Gregory and Poeppel, David},
doi = {10.1038/nrn2113},
file = {::},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
month = {may},
number = {5},
pages = {393--402},
publisher = {Nature Publishing Group},
title = {{The cortical organization of speech processing}},
url = {http://www.nature.com/doifinder/10.1038/nrn2113},
volume = {8},
year = {2007}
}
@article{Gaskell1997,
abstract = {We present a new distributed connectionist model of the perception of spoken words. The model employs a representation of speech that combines lexical information with abstract phonological information, with lexical access modelled as a direct mapping onto this single distributed representation. We first examine the integration of partial cues to phonological identity, showing that the model provides a sound basis for simulating phonetic and lexical decision data from Marslen-Wilson and Warren (1994). We then investigate the time course of lexical access, and argue that the process of competition between word candidates during lexical access can be interpreted in terms of interference between distributed lexical representations. The relation between our model and other models of spoken word recognition is discussed.},
author = {Gaskell, M. Gareth and Marslen-Wilson, William D.},
doi = {10.1080/016909697386646},
issn = {0169-0965},
journal = {Language and Cognitive Processes},
month = {oct},
number = {5-6},
pages = {613--656},
publisher = { Taylor {\&} Francis Group },
title = {{Integrating Form and Meaning: A Distributed Model of Speech Perception}},
url = {http://www.tandfonline.com/doi/abs/10.1080/016909697386646},
volume = {12},
year = {1997}
}
@article{Hefner1986,
author = {Hefner, H. E. and Heffner, R. S.},
journal = {Journal of Neurophysiology},
number = {3},
title = {{Effect of unilateral and bilateral auditory cortex lesions on the discrimination of vocalizations by Japanese macaques}},
url = {http://jn.physiology.org/content/56/3/683.short},
volume = {56},
year = {1986}
}
@article{Ohl1999,
abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
doi = {10.1101/LM.6.4.347},
file = {::},
issn = {1072-0502},
journal = {Learning {\&} memory (Cold Spring Harbor, N.Y.)},
number = {4},
pages = {347--62},
pmid = {10509706},
publisher = {Cold Spring Harbor Laboratory Press},
title = {{Bilateral ablation of auditory cortex in Mongolian gerbil affects discrimination of frequency modulated tones but not of pure tones.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC311295},
volume = {6},
year = {1999}
}
@article{Engineer2015,
abstract = {Speech sounds evoke unique neural activity patterns in primary auditory cortex (A1). Extensive speech sound discrimination training alters A1 responses. While the neighboring auditory cortical fields each contain information about speech sound identity, each field processes speech sounds differently. We hypothesized that while all fields would exhibit training-induced plasticity following speech training, there would be unique differences in how each field changes. In this study, rats were trained to discriminate speech sounds by consonant or vowel in quiet and in varying levels of background speech-shaped noise. Local field potential and multiunit responses were recorded from four auditory cortex fields in rats that had received 10 weeks of speech discrimination training. Our results reveal that training alters speech evoked responses in each of the auditory fields tested. The neural response to consonants was significantly stronger in anterior auditory field (AAF) and A1 following speech training. The neural response to vowels following speech training was significantly weaker in ventral auditory field (VAF) and posterior auditory field (PAF). This differential plasticity of consonant and vowel sound responses may result from the greater paired pulse depression, expanded low frequency tuning, reduced frequency selectivity, and lower tone thresholds, which occurred across the four auditory fields. These findings suggest that alterations in the distributed processing of behaviorally relevant sounds may contribute to robust speech discrimination.},
author = {Engineer, Crystal T. and Rahebi, Kimiya C. and Buell, Elizabeth P. and Fink, Melyssa K. and Kilgard, Michael P.},
doi = {10.1016/j.bbr.2015.03.044},
file = {::},
isbn = {1872-7549 (Electronic)
0166-4328 (Linking)},
issn = {18727549},
journal = {Behavioural Brain Research},
keywords = {Auditory processing,Map reorganization,Receptive field plasticity,Speech therapy},
pages = {256--264},
pmid = {25827927},
publisher = {Elsevier B.V.},
title = {{Speech training alters consonant and vowel responses in multiple auditory cortex fields}},
url = {http://dx.doi.org/10.1016/j.bbr.2015.03.044},
volume = {287},
year = {2015}
}
@article{Ohl2001,
author = {Ohl, F. W. and Scheich, H. and Freeman, W. J.},
doi = {10.1038/35089076},
issn = {0028-0836},
journal = {Nature},
month = {aug},
number = {6848},
pages = {733--736},
publisher = {Nature Publishing Group},
title = {{Change in pattern of ongoing cortical activity with auditory category learning}},
url = {http://www.nature.com/doifinder/10.1038/35089076},
volume = {412},
year = {2001}
}
@article{Mesgarani2014,
abstract = {During speech perception, linguistic elements such as consonants and vowels are extracted from a complex acoustic speech signal. The superior temporal gyrus (STG) participates in high-order auditory processing of speech, but how it encodes phonetic information is poorly understood. We used high-density direct cortical surface recordings in humans while they listened to natural, continuous speech to reveal the STG representation of the entire English phonetic inventory. At single electrodes, we found response selectivity to distinct phonetic features. Encoding of acoustic properties was mediated by a distributed population response. Phonetic features could be directly related to tuning for spectrotemporal acoustic cues, some of which were encoded in a nonlinear fashion or by integration of multiple cues. These findings demonstrate the acoustic-phonetic representation of speech in human STG.},
author = {Mesgarani, Nima and Cheung, Connie and Johnson, Keith and Chang, Edward F},
doi = {10.1126/science.1245994},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Mesgarani et al. - 2014 - Phonetic feature encoding in human superior temporal gyrus(2).pdf:pdf},
isbn = {0036-8075},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Auditory Cortex,Auditory Cortex: anatomy {\&} histology,Auditory Cortex: physiology,Female,Humans,Magnetic Resonance Imaging,Male,Phonetics,Speech Acoustics,Speech Perception},
number = {6174},
pages = {1006--10},
pmid = {24482117},
title = {{Phonetic feature encoding in human superior temporal gyrus.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4350233{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {343},
year = {2014}
}
@article{Rutishauser2015,
abstract = {Previous explanations of computations performed by recurrent networks have focused on symmetrically connected saturating neurons and their convergence toward attractors. Here we analyze the behavior of asymmetrical connected networks of linear threshold neurons, whose positive response is unbounded. We show that, for a wide range of parameters, this asymmetry brings interesting and computationally useful dynamical properties. When driven by input, the network explores potential solutions through highly unstable 'expansion' dynamics. This expansion is steered and constrained by negative divergence of the dynamics, which ensures that the dimensionality of the solution space continues to reduce until an acceptable solution manifold is reached. Then the system contracts stably on this manifold towards its final solution trajectory. The unstable positive feedback and cross inhibition that underlie expansion and divergence are common motifs in molecular and neuronal networks. Therefore we propose that very simple organizational constraints that combine these motifs can lead to spontaneous computation and so to the spontaneous modification of entropy that is characteristic of living systems.},
author = {Rutishauser, Ueli and Slotine, Jean Jacques and Douglas, Rodney},
doi = {10.1371/journal.pcbi.1004039},
file = {::},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {1},
pages = {e1004039},
pmid = {25617645},
title = {{Computation in Dynamically Bounded Asymmetric Systems}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1004039},
volume = {11},
year = {2015}
}
@article{Bailey1980,
abstract = {A series of experiments is reported that investigated the pattern of acoustic information specifying place and manner of stop consonants in medial position after [s]. In both production and perception, information for stop place includes the spectrum of the fricative at offset, the duration of the silent closure interval, the spectral relationship between the frequency of the stop release burst and the following periodically excited formants, and the spectral and temporal characteristics of the first formant transition. Similarly, the information for stop manner includes the duration of silent closure, the frequency of the first formant at the release, the magnitude of the first formant transition, and the proximity of the second and third formants at release. A relationship was shown to exist in perception between the spectral characteristics of the first formant and the duration of the silent closure required to hear a stop. This appears to reciprocate the covariation of these parameters in production across different places of articulation and different vocalic contexts. The existence of perceptual sensitivity to a wide range of the acoustic consequences of production questions the efficacy of accounts of speech perception in terms of the fractionation of the signal into elemental acoustic cues, which are then integrated to yield a phonetic percept. It is argued that it is inappropriate to ascribe a psychological status to cues whose only reality is their operational role as physical parameters whose manipulation can change the phenotic interpretation of a signal. It is suggested that the metric of the information for phonetic perception cannot be that of the cues; rather, a metric should be sought in which acoustic and articulatory dynamics are isomorphic.},
author = {Bailey, P J and Summerfield, Q},
doi = {10.1037/0096-1523.6.3.536},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Bailey, Summerfield - 1980 - Information in speech observations on the perception of s-stop clusters(2).pdf:pdf},
isbn = {0096-1523},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
keywords = {Humans,Phonetics,Psychoacoustics,Speech Percept},
month = {aug},
number = {3},
pages = {536--563},
pmid = {6447767},
title = {{Information in speech: observations on the perception of [s]-stop clusters}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6447767},
volume = {6},
year = {1980}
}
@article{Liberman1985,
abstract = {A motor theory of speech perception, initially proposed to account for results of early experiments with synthetic speech, is now extensively revised to accommodate recent findings, and to relate the assumptions of the theory to those that might be made about other perceptual modes. According to the revised theory, phonetic information is perceived in a biologically distinct system, a 'module' specialized to detect the intended gestures of the speaker that are the basis for phonetic categories. Built into the structure of this module is the unique but lawful relationship between the gestures and the acoustic patterns in which they are variously overlapped. In consequence, the module causes perception of phonetic structure without translation from preliminary auditory impressions. Thus, it is comparable to such other modules as the one that enables an animal to localize sound. Peculiar to the phonetic module are the relation between perception and production it incorporates and the fact that it must compete with other modules for the same stimulus variations. ?? 1985.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Liberman, Alvin M. and Mattingly, Ignatius G.},
doi = {10.1016/0010-0277(85)90021-6},
eprint = {NIHMS150003},
isbn = {0010-0277, 0010-0277},
issn = {00100277},
journal = {Cognition},
month = {oct},
number = {1},
pages = {1--36},
pmid = {4075760},
title = {{The motor theory of speech perception revised}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/4075760},
volume = {21},
year = {1985}
}
@article{Kosem2016,
author = {K{\"{o}}sem, Anne and Basirat, Anahita and Azizi, Leila and van Wassenhove, Virginie},
doi = {10.1152/jn.00074.2016},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
number = {6},
pages = {2497--2512},
title = {{High-frequency neural activity predicts word parsing in ambiguous speech streams}},
url = {http://jn.physiology.org/lookup/doi/10.1152/jn.00074.2016},
volume = {116},
year = {2016}
}
@article{Ohala1996,
abstract = {Three types of evidence are reviewed which cast doubt on claims that recovery of the speaker's articulations is an inherent part of speech perception: (a) Phonological data (e.g., universal tendencies of languages' segment inventories, phonotactic patterns, sound changes, etc.) show unmistakably that the acoustic-auditory properties of speech sounds, not their articulations, are the primary determinant of their behavior. (b) Infants and various nonhuman species can differentiate certain sound contrasts in human speech even though it is highly unlikely that they can deduce the vocal tract movements generating the sounds. (c) Humans can differentiate many nonspeech sounds almost as complex as speech, e.g., music, machine noises, as well as bird and monkey vocalizations, where there is little or no possibility of recovering the mechanisms producing the sounds.},
author = {Ohala, J J},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {mar},
number = {3},
pages = {1718--25},
pmid = {8819861},
title = {{Speech perception is hearing sounds, not tongues.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8819861},
volume = {99},
year = {1996}
}
@article{Bub2001,
abstract = {It is generally accepted, following Landauer and Bennett, that the process of measurement involves no minimum entropy cost, but the erasure of information in resetting the memory register of a computer to zero requires dissipating heat into the environment. This thesis has been challenged recently in a two-part article by Earman and Norton. I review some relevant observations in the thermodynamics of computation and argue that Earman and Norton are mistaken: there is in principle no entropy cost to the acquisition of information, but the destruction of information does involve an irreducible entropy cost. ?? 2001 Elsevier Science Ltd.},
archivePrefix = {arXiv},
arxivId = {quant-ph/0203017},
author = {Bub, Jeffrey},
doi = {10.1016/S1355-2198(01)00023-5},
eprint = {0203017},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Bub - 2001 - Maxwell's Demon and the Thermodynamics of Computation(2).pdf:pdf},
isbn = {1355-2198},
issn = {13552198},
journal = {Studies in History and Philosophy of Science Part B - Studies in History and Philosophy of Modern Physics},
keywords = {Information,Maxwell's Demon,Measurement,Thermodynamics of Computation},
number = {4},
pages = {569--579},
primaryClass = {quant-ph},
title = {{Maxwell's Demon and the Thermodynamics of Computation}},
volume = {32},
year = {2001}
}
@article{Idemaru2011,
author = {Idemaru, Kaori and Holt, Lori L.},
doi = {10.1037/a0025641},
file = {::},
issn = {1939-1277},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
keywords = {dimension-based learning,perceptual learning,speech perception,statistical learning,talker adaptation,word recognition},
number = {6},
pages = {1939--1956},
publisher = {American Psychological Association},
title = {{Word recognition reflects dimension-based statistical learning.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0025641},
volume = {37},
year = {2011}
}
@incollection{Kluender2013a,
abstract = {Predicated upon principles of information theory, efficient coding has proven valuable for understanding visual perception. Here, we illustrate how efficient coding provides a powerful explanatory framework for understanding speech perception. This framework dissolves debates about objects of perception, instead focusing on the objective of perception: optimizing information transmis-sion between the environment and perceivers. A simple measure of physiologically significant information is shown to predict intelligibility of variable-rate speech and discriminability of vowel sounds. Reliable covariance between acoustic attributes in complex sounds, both speech and nonspeech, is demonstrated to be amply available in natural sounds and efficiently coded by listeners. An efficient coding framework provides a productive approach to answer questions concerning perception of vowel sounds (including vowel inherent spectral change), perception of speech, and perception most broadly.},
annote = {From Duplicate 1 (The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception - Bartlett, Edward L.)

From Duplicate 1 (The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception. - Bartlett, Edward L.)

10.1016/j.bandl.2013.03.003},
author = {Kluender, Keith R and Stilp, Christian E and Kiefte, Michael and Kluender, K R and Stilp, C E and Kiefte, M},
booktitle = {Vowel Inherent Spectral Change},
doi = {10.1007/978-3-642-14209-3_6},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Kluender et al. - 2013 - Perception of Vowel Sounds Within a Biologically Realistic Model of Efficient Coding(2).pdf:pdf},
isbn = {9783642142093},
pages = {117--151},
title = {{Perception of Vowel Sounds Within a Biologically Realistic Model of Efficient Coding}},
year = {2013}
}
@inproceedings{JiaDeng2009,
author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206848},
file = {::},
isbn = {978-1-4244-3992-8},
month = {jun},
pages = {248--255},
publisher = {IEEE},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://ieeexplore.ieee.org/document/5206848/},
year = {2009}
}
@misc{Maechler2017,
author = {Maechler, Martin and Rousseeuw, Peter and Struyf, Anja and Hubert, Mia and Hornik, Kurt},
title = {{cluster: Cluster Analysis Basics and Extensions}},
year = {2017}
}
@article{Stevens,
abstract = {The consonantal segments that underlie an utterance are manifested in the acoustic signal by abrupt discontinuities or dislocations in the spectral pattern. There are potentially two such discontinuities for each consonant, corresponding to the formation and release of a constriction in the oral cavity by the lips, the tongue blade, or the tongue body. Acoustic cues for the various consonant features of place, voicing and nasality reside in the signal in quite different forms on the two sides of each acoustic discontinuity. Examples of these diverse cues and their origin in acoustic theory are reviewed, with special attention to place features and features related to the laryngeal state and to nasalization. A listener appears to have the ability to integrate these diverse, brief acoustic cues for the features of consonants, although the mechanism for this integration process is unclear.},
author = {Stevens, K N},
doi = {28468},
issn = {0031-8388},
journal = {Phonetica},
number = {2-4},
pages = {139--51},
pmid = {10992135},
title = {{Diverse acoustic cues at consonantal landmarks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10992135},
volume = {57}
}
@article{Clevert2015,
abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10{\%} classification error for a single crop, single model network.},
archivePrefix = {arXiv},
arxivId = {1511.07289},
author = {Clevert, Djork-Arn{\'{e}} and Unterthiner, Thomas and Hochreiter, Sepp},
eprint = {1511.07289},
file = {::},
month = {nov},
title = {{Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}},
url = {http://arxiv.org/abs/1511.07289},
year = {2015}
}
@article{Kleinschmidt2016,
abstract = {When a listener hears many good examples of a /b/ in a row, they are less likely to classify other sounds on, e.g., a /b/-to-/d/ continuum as /b/. This phenomenon is known as selective adaptation and is a well-studied property of speech perception. Traditionally, selective adaptation is seen as a mechanistic property of the speech perception system, and attributed to fatigue in acoustic-phonetic feature detectors. However, recent developments in our understanding of non-linguistic sensory adaptation and higher-level adaptive plasticity in speech perception and language comprehension suggest that it is time to re-visit the phenomenon of selective adaptation. We argue that selective adaptation is better thought of as a computational property of the speech perception system. Drawing on a common thread in recent work on both non-linguistic sensory adaptation and plasticity in language comprehension, we furthermore propose that selective adaptation can be seen as a consequence of distributional learning across multiple levels of representation. This proposal opens up new questions for research on selective adaptation itself, and also suggests that selective adaptation can be an important bridge between work on adaptation in low-level sensory systems and the complicated plasticity of the adult language comprehension system.},
author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
doi = {10.3758/s13423-015-0943-z},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
keywords = {Computational models,Perceptual learning,Speech perception,Statistical inference},
month = {jun},
number = {3},
pages = {678--691},
pmid = {26438255},
title = {{Re-examining selective adaptation: Fatiguing feature detectors, or distributional learning?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26438255 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4821823 http://link.springer.com/10.3758/s13423-015-0943-z},
volume = {23},
year = {2016}
}
@article{Moczulska2013,
abstract = {Long-lasting changes in synaptic connections induced by relevant experiences are believed to represent the physical correlate of memories. Here, we combined chronic in vivo two-photon imaging of dendritic spines with auditory-cued classical conditioning to test if the formation of a fear memory is associated with structural changes of synapses in the mouse auditory cortex. We find that paired conditioning and unpaired conditioning induce a transient increase in spine formation or spine elimination, respectively. A fraction of spines formed during paired conditioning persists and leaves a long-lasting trace in the network. Memory recall triggered by the reexposure of mice to the sound cue did not lead to changes in spine dynamics. Our findings provide a synaptic mechanism for plasticity in sound responses of auditory cortex neurons induced by auditory-cued fear conditioning; they also show that retrieval of an auditory fear memory does not lead to a recapitulation of structural plasticity in the auditory cortex as observed during initial memory consolidation.},
author = {Moczulska, Kaja Ewa and Tinter-Thiede, Juliane and Peter, Manuel and Ushakova, Lyubov and Wernle, Tanja and Bathellier, Brice and Rumpel, Simon},
doi = {10.1073/pnas.1312508110},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Moczulska et al. - 2013 - Dynamics of dendritic spines in the mouse auditory cortex during memory formation and memory recall(3).pdf:pdf;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Moczulska et al. - 2013 - Dynamics of dendritic spines in the mouse auditory cortex during memory formation and memory recall(4).pdf:pdf},
isbn = {1091-6490 (Electronic)$\backslash$r0027-8424 (Linking)},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {auditory fear conditioning,learning,reconsolidation},
number = {45},
pages = {18315--20},
pmid = {24151334},
title = {{Dynamics of dendritic spines in the mouse auditory cortex during memory formation and memory recall.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24151334{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3831433},
volume = {110},
year = {2013}
}
@misc{Lisker1977,
abstract = {InAmerican English, initial /bdg/ often lack the acoustic feature takenas the defining feature of voiced stops; intervocalically before unstressedvowel /ptk/ lack aspiration, without which initial stops are notlabeled /ptk/. Initially, the two categories differ in the timingof vocal fold adduction and onset of fold vibration, andseveral acoustic cues, all tied to the VOT difference, havebeen studied. Medially there is also a difference in themanagement of the larynx, though it results in a phoneticallysimpler contrast, one of voicing with no accompanying aspiration difference.Acoustically, however, the list of features that play, or mightplausibly play a role is quite large. The word pairrapid-rabid, for example, might be affected by the following: (1)presence/absence of low-frequency buzz during the closure interval, (2) durationof closure, (3) F1 offset frequency before closure, (4) F1offset transition duration, (5) F1 onset frequency following closure, (6)F1 onset transition duration, (7) {\ae} duration, (8) F1 cut-backbefore closure, (9) F1 cutback following closure, (10) VOT cutbackbefore closure, (11) VOT delay after closure, (12) F0 contourbefore closure, (13) F0 contour after closure, (14) amplitude ofi relative to {\ae}, (15) decay time of glottal signalpreceding closure, (16) intensity of burst following closure. Even ifsome of these should turn out to be perceptually negligible,enough of them surely have cue value to make ita formidable task to justify preferring an acoustic to anarticulatory account of the distinction between the two English words.The support of the National Institute of Child Health andHuman Development is gratefully acknowledged. 1977 Acoustical Society of America},
author = {Lisker, Leigh},
booktitle = {The Journal of the Acoustical Society of America},
doi = {10.1121/1.2016377},
isbn = {0001-4966},
issn = {00014966},
number = {S1},
pages = {S77},
title = {{Rapid versus rabid: A catalogue of acoustic features that may cue the distinction}},
volume = {62},
year = {1977}
}
@article{Centanni2013,
abstract = {We have developed a classifier capable of locating and identifying speech sounds using activity from rat auditory cortex with an accuracy equivalent to behavioral performance and without the need to specify the onset time of the speech sounds. This classifier can identify speech sounds from a large speech set within 40. ms of stimulus presentation. To compare the temporal limits of the classifier to behavior, we developed a novel task that requires rats to identify individual consonant sounds from a stream of distracter consonants. The classifier successfully predicted the ability of rats to accurately identify speech sounds for syllable presentation rates up to 10 syllables per second (up to 17.9 ± 1.5 bits/s), which is comparable to human performance. Our results demonstrate that the spatiotemporal patterns generated in primary auditory cortex can be used to quickly and accurately identify consonant sounds from a continuous speech stream without prior knowledge of the stimulus onset times. Improved understanding of the neural mechanisms that support robust speech processing in difficult listening conditions could improve the identification and treatment of a variety of speech-processing disorders. {\textcopyright} 2013 IBRO.},
author = {Centanni, T. M. and Sloan, A. M. and Reed, A. C. and Engineer, C. T. and Rennaker, R. L. and Kilgard, M. P.},
doi = {10.1016/j.neuroscience.2013.11.030},
file = {::},
isbn = {0306-4522},
issn = {03064522},
journal = {Neuroscience},
keywords = {Auditory cortex,Classifier,Coding,Rat,Temporal patterns},
pages = {292--306},
pmid = {24286757},
publisher = {IBRO},
title = {{Detection and identification of speech sounds using cortical activity patterns}},
url = {http://dx.doi.org/10.1016/j.neuroscience.2013.11.030},
volume = {258},
year = {2013}
}
@article{Theunissen2014,
abstract = {We might be forced to listen to a high-frequency tone at our audiologist's office or we might enjoy falling asleep with a white-noise machine, but the sounds that really matter to us are the voices of our companions or music from our favourite radio station. The auditory system has evolved to process behaviourally relevant natural sounds. Research has shown not only that our brain is optimized for natural hearing tasks but also that using natural sounds to probe the auditory system is the best way to understand the neural computations that enable us to comprehend speech or appreciate music.},
author = {Theunissen, Fr{\'{e}}d{\'{e}}ric E and Elie, Julie E},
doi = {10.1038/nrn3731},
isbn = {1471-0048 (Electronic)$\backslash$r1471-003X (Linking)},
issn = {1471-0048},
journal = {Nature reviews. Neuroscience},
keywords = {Acoustic Stimulation,Animals,Auditory Pathways,Auditory Pathways: physiology,Auditory Perception,Auditory Perception: physiology,Brain Mapping,Hearing,Hearing: physiology,Humans,Music,Sound},
number = {6},
pages = {355--66},
pmid = {24840800},
title = {{Neural processing of natural sounds.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24840800},
volume = {15},
year = {2014}
}
@article{Carbonell2014,
author = {Carbonell, Kathy M. and Lotto, Andrew J.},
doi = {10.3389/fpsyg.2014.00427},
file = {::},
issn = {1664-1078},
journal = {Frontiers in psychology},
keywords = {as is apparent from,auditory processing,motor theory,multisensory i,multisensory integration,of nearly any research,of speech,or review article,reading the first line,sensorimotor effects on perception,specialness,speech perception,the},
month = {jun},
number = {June},
pages = {427},
pmid = {24917830},
publisher = {Frontiers},
title = {{Speech is not special{\ldots} again.}},
url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00427/abstract http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4042079{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {5},
year = {2014}
}
@misc{Brenowitz1997,
abstract = {This special issue of the Journal of Neurobiology is devoted to a consideration of the avian song control system. In the 20 years that have passed since Nottebohm et al. 1976) first identified forebrain circuits that control song in birds, the song system has emerged as a leading model in behavioral neuroscience. To mark the beginning of the third decade of study of this model, we invited several leading investigators to contribute to this volume. We set two goals for the authors: to review progress in their area of study and, more important, to identify critical directions for future research. The modern study of birdsong began with the work of William Thorpe (1958, 1961). He showed that chaffinches (Fringilla coelebs) collected as nestlings and reared in the laboratory in isolation from conspecific adult males produced very abnormal songs. If these young birds were exposed to tape recordings of wild chaffinch songs, however, they eventually produced normal songs that matched those heard on the recordings. These studies demonstrated for the first time that young birds must learn the song of their species by listening to adult conspecifics. Thorpe's student Peter Marler greatly expanded upon this early work. Marler and his colleagues demonstrated the existence of local geographic song "dialects," that song learning is characterized by early sensitive periods, and that birds have innate predispositions to learn the song of their species. Masakazu Konishi, while a student with Marler, showed that birds must be able to hear themselves sing to develop song normally. Fernando Nottebohm, also while a student with Marler, showed that the peripheral control of song production is lateralized. Nottebohm and his colleagues subsequently identified neural circuits in the avian forebrain that control song behavior. This important discovery paved the way for many investigators who have subsequently contributed to our understanding of song behavior and its neural control. The birdsong system offers several advantages as a model for identifying neural mechanisms that underlie biologically relevant behavior: 1. Song is a learned behavior that is controlled by discrete neural circuits. 2. There are distinct phases in the development of song, with well-defined sensitive periods. One can relate the ontogeny of song behavior to the development of the underlying neural circuits. 3. Song is the product of stereotyped motor programs, with hierarchical organization of the premotor and motor nuclei. 4. Song behavior and the associated neural circuits are sexually dimorphic in most species. 5. Gonadal steroid hormones have pronounced effects on the development and adult function of the song control circuits, as well as on song behavior. 6. There is extensive plasticity of the adult song system, including ongoing neurogenesis and seasonal changes in morphology. 7. There is pronounced species diversity in different aspects of song behavior, including the timing of vocal learning, sex patterns of song production, number of songs that are learned, and seasonality of song behavior. This diversity provides opportunities for comparative studies of the song control system.},
author = {Brenowitz, E. A. and Margoliash, D. and Nordeen, K. W.},
booktitle = {Journal of Neurobiology},
isbn = {0022-3034},
issn = {00223034},
number = {5},
pages = {495--500},
pmid = {9369455},
title = {{An introduction to birdsong and the avian song system}},
volume = {33},
year = {1997}
}
@article{Fritz2003,
abstract = {Listening is an active process in which attentive focus on salient acoustic features in auditory tasks can influence receptive field properties of cortical neurons. Recent studies showing rapid task-related changes in neuronal spectrotemporal receptive fields (STRFs) in primary auditory cortex of the behaving ferret are reviewed in the context of current research on cortical plasticity. Ferrets were trained on spectral tasks, including tone detection and two-tone discrimination, and on temporal tasks, including gap detection and click-rate discrimination. STRF changes could be measured on-line during task performance and occurred within minutes of task onset. During spectral tasks, there were specific spectral changes (enhanced response to tonal target frequency in tone detection and discrimination, suppressed response to tonal reference frequency in tone discrimination). However, only in the temporal tasks, the STRF was changed along the temporal dimension by sharpening temporal dynamics. In ferrets trained on multiple tasks, distinctive and task-specific STRF changes could be observed in the same cortical neurons in successive behavioral sessions. These results suggest that rapid task-related plasticity is an ongoing process that occurs at a network and single unit level as the animal switches between different tasks and dynamically adapts cortical STRFs in response to changing acoustic demands. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Fritz, Jonathan and Elhilali, Mounya and Shamma, Shihab},
doi = {10.1016/j.heares.2005.01.015},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Fritz, Elhilali, Shamma - 2005 - Active listening Task-dependent plasticity of spectrotemporal receptive fields in primary auditory c(2).pdf:pdf},
isbn = {0378-5955 (Print)$\backslash$r0378-5955 (Linking)},
issn = {03785955},
journal = {Hearing Research},
keywords = {Adaptive,Attention,Auditory,Behavior,Cortex,Plasticity},
number = {1-2},
pages = {159--176},
pmid = {16081006},
title = {{Active listening: Task-dependent plasticity of spectrotemporal receptive fields in primary auditory cortex}},
url = {http://www.nature.com/doifinder/10.1038/nn1141},
volume = {206},
year = {2005}
}
@inproceedings{Schouten2003,
abstract = {Comparing phoneme classification and discrimination (or "categorical perception") of a stimulus continuum has for a long time been regarded as a useful method for investigating the storage and retrieval of phoneme categories in long-term memory. The closeness of the relationship between the two tasks, i.e. the degree of categorical perception, depends on a number of factors, some of which are unknown or random. One very important factor, however, seems to be the degree of bias (in the signal-detection sense of the term) in the discrimination task. When the task is such (as it is in 2IFC, for example) that the listener has to rely heavily on an internal, subjective, criterion, discrimination can seem to be almost perfectly categorical, if the stimuli are natural enough. Presenting the same stimuli in a much less biasing task, however, leads to discrimination results that are completely unrelated to phoneme classification. Even the otherwise ubiquitous peak at the phoneme boundary has disappeared. The traditional categorical-perception experiment measures the bias inherent in the discrimination task; if we want to know how speech sounds are categorized, we will have to look elsewhere. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Schouten, Bert and Gerrits, Ellen and {Van Hessen}, Arjan},
booktitle = {Speech Communication},
doi = {10.1016/S0167-6393(02)00094-8},
isbn = {0167-6393},
issn = {01676393},
keywords = {Categorical perception},
number = {1},
pages = {71--80},
title = {{The end of categorical perception as we know it}},
volume = {41},
year = {2003}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org/},
year = {2016}
}
@article{Hillenbrand1994,
abstract = {This study was designed as a replication and extension of the classic study of vowel acoustics by Peterson and Barney (PB) [J. Acoust. Soc. Am. 24, 175–184 (1952)]. Recordings were made of 50 men, 50 women, and 50 children producing the vowels /i, i, eh, {\ae}, hooked backward eh, inverted vee), a, open oh, u, u/ in h–V–d syllables. Formant contours for F1–F4 were measured from LPC spectra using a custom interactive editing tool. For comparison with the PB data, formant patterns were sampled at a time that was judged by visual inspection to be maximally steady. Preliminary analysis shows numerous differences between the present data and those of PB, both in terms of average formant frequencies for vowels, and the degree of overlap among adjacent vowels. As with the original study, listening tests showed that the signals were nearly always identified as the vowel intended by the talker.},
author = {Hillenbrand, James and Getty, Laura A. and Wheeler, Kimberlee and Clark, Michael J.},
doi = {10.1121/1.409456},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {COMPARATIVE EVALUATIONS,PERFORMANCE TESTING,SPEECH RECOGNITION,VOWELS},
month = {may},
number = {5},
pages = {2875--2875},
publisher = {Acoustical Society of America},
title = {{Acoustic characteristics of American English vowels}},
url = {http://asa.scitation.org/doi/10.1121/1.409456},
volume = {95},
year = {1994}
}
@article{Lotto1997,
author = {Lotto, AJ and Kluender, KR and Holt, LL},
journal = {Chicago Linguistic Society},
title = {{Animal models of speech perception phenomena}},
url = {https://www.researchgate.net/profile/Keith{\_}Kluender/publication/237280984{\_}(from{\_}K.{\_}Singer{\_}R.{\_}Eggert{\_}{\_}G.{\_}Anderson{\_}(Eds.){\_}Chicago{\_}Linguistic{\_}Society{\_}Volume{\_}33{\_}(Chicago{\_}Linguistic{\_}Society{\_}Chicago).{\_}pp.{\_}357-367{\_}(1997).){\_}Animal{\_}models{\_}of{\_}speech{\_}perception{\_}phen},
year = {1997}
}
@article{Weiss2001,
abstract = {In this article we analyze the effect of class distribution on classifier learning. We begin by describing the different ways in which class distribution affects learning and how it affects the evaluation of learned classifiers. We then present the results of two comprehensive experimental studies. The first study compares the performance of classifiers generated from unbalanced data sets with the performance of classifiers generated from balanced versions of the same data sets. This...},
author = {Weiss, Gm and Provost, Foster},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Weiss, Provost - 2001 - The effect of class distribution on classifier learning an empirical study(2).pdf:pdf},
journal = {Rutgers Univ},
title = {{The effect of class distribution on classifier learning: an empirical study}},
url = {ftp://ftp.cs.rutgers.edu/http/cs/cs/pub/technical-reports/work/ml-tr-44.pdf},
year = {2001}
}
@article{Kleinschmidt2016a,
author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
doi = {10.3758/s13423-015-0943-z},
file = {::},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
month = {jun},
number = {3},
pages = {678--691},
publisher = {Springer US},
title = {{Re-examining selective adaptation: Fatiguing feature detectors, or distributional learning?}},
url = {http://link.springer.com/10.3758/s13423-015-0943-z},
volume = {23},
year = {2016}
}
@article{Estes2015,
abstract = {To learn from their environments, infants must detect structure behind pervasive variation. This presents substantial and largely untested learning challenges in early language acquisition. The current experiments address whether infants can use statistical learning mechanisms to segment words when the speech signal contains acoustic variation produced by changes in speakers' voices. In Experiment 1, 8- and 10-month-old infants listened to a continuous stream of novel words produced by 8 different female voices. The voices alternated frequently, potentially interrupting infants' detection of transitional probability patterns that mark word boundaries. Infants at both ages successfully segmented words in the speech stream. In Experiment 2, 8-month-olds demonstrated the ability to generalize their learning about the speech stream when presented with a new, acoustically distinct voice during testing. However, in Experiments 3 and 4, when the same speech stream was produced by only 2 female voices, infants failed to segment the words. The results of these experiments indicate that low acoustic variation may interfere with infants' efficiency in segmenting words from continuous speech, but that infants successfully use statistical cues to segment words in conditions of high acoustic variation. These findings contribute to our understanding of whether statistical learning mechanisms can scale up to meet the demands of natural learning environments.},
author = {Estes, Katharine Graf and Lew-Williams, Casey},
doi = {10.1037/a0039725},
issn = {1939-0599},
journal = {Developmental Psychology},
month = {nov},
number = {11},
pages = {1517--1528},
pmid = {26389607},
title = {{Listening through voices: Infant statistical word segmentation across multiple speakers.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26389607 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4631842 http://doi.apa.org/getdoi.cfm?doi=10.1037/a0039725},
volume = {51},
year = {2015}
}
@article{Kluender2000,
abstract = {Broadly speaking, nonhuman animal models contribute to understanding speech perception by humans in two ways—by analogy and by homology. The former is generally easier and examples are more abundant. Because demonstrating homology requires deeper explication of underlying mechanisms, claims can be more precarious but carry potentially greater explanatory payoff. When studying nonhuman organisms as an analogy, the emphasis is typically upon how animal physiological or behavioral processes have adapted to fulfill requirements of particular ecological niches. By contrast, study of animals as homology often violates ecology in search of common underlying processes, and the animal becomes a method more than an object of study. Examples of findings for animal analogies and homologies will be reviewed. Data will be presented from experiments in which nonhuman subjects play the role of homology in revealing both foundational sensory processes and more plastic processes of perceptual development. Animal models pro...},
author = {Kluender, Keith R.},
doi = {10.1121/1.429153},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {may},
number = {5},
pages = {2835--2835},
publisher = {Acoustical Society of AmericaASA},
title = {{Contributions of nonhuman animal models to understanding human speech perception}},
url = {http://asa.scitation.org/doi/10.1121/1.429153},
volume = {107},
year = {2000}
}
@misc{Sundar2014,
abstract = {Constructs confidence intervals on the probability of success in a binomial experiment via several parameterizations},
author = {Sundar, Dorai-Raj},
pages = {R package version 1.1--1},
title = {{binom: Binomial Confidence Intervals For Several Parameterizations}},
year = {2014}
}
@article{Eimas1973,
abstract = {Using a selective adaptation procedure, evidence was obtained for the existence of linguistic feature detectors, analogous to visual feature detectors. These detectors are each sensitive to a restricted range of voice onset times, the physical continuum underlying the perceived phonetic distinctions between voiced and voiceless stop consonants. The sensitivity of a particular detector can be reduced selectively by repetitive presentation of its adequate stimulus. This results in a shift in the locus of the phonetic boundary separating the voiced and voiceless stops.},
author = {Eimas, Peter D. and Corbit, John D.},
doi = {10.1016/0010-0285(73)90006-6},
issn = {00100285},
journal = {Cognitive Psychology},
number = {1},
pages = {99--109},
title = {{Selective adaptation of linguistic feature detectors}},
url = {http://www.sciencedirect.com/science/article/pii/0010028573900066},
volume = {4},
year = {1973}
}
@article{Pedregosa2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Froemke2007,
abstract = {Receptive fields of sensory cortical neurons are plastic, changing in response to alterations of neural activity or sensory experience. In this way, cortical representations of the sensory environment can incorporate new information about the world, depending on the relevance or value of particular stimuli. Neuromodulation is required for cortical plasticity, but it is uncertain how subcortical neuromodulatory systems, such as the cholinergic nucleus basalis, interact with and refine cortical circuits. Here we determine the dynamics of synaptic receptive field plasticity in the adult primary auditory cortex (also known as AI) using in vivo whole-cell recording. Pairing sensory stimulation with nucleus basalis activation shifted the preferred stimuli of cortical neurons by inducing a rapid reduction of synaptic inhibition within seconds, which was followed by a large increase in excitation, both specific to the paired stimulus. Although nucleus basalis was stimulated only for a few minutes, reorganization of synaptic tuning curves progressed for hours thereafter: inhibition slowly increased in an activity-dependent manner to rebalance the persistent enhancement of excitation, leading to a retuned receptive field with new preference for the paired stimulus. This restricted period of disinhibition may be a fundamental mechanism for receptive field plasticity, and could serve as a memory trace for stimuli or episodes that have acquired new behavioural significance.},
author = {Froemke, Robert C and Merzenich, Michael M and Schreiner, Christoph E},
doi = {10.1038/nature06289},
file = {::},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
keywords = {Afferent,Animals,Auditory Cortex,Excitatory Postsynaptic Pote,Female,Memory,Neuronal Plasticity,Neurons,Rats,Sprague-Dawley,Synapses,Time Factors,cytology/physiology,metabolism,physiology},
number = {7168},
pages = {425--429},
pmid = {18004384},
title = {{A synaptic memory trace for cortical receptive field plasticity.}},
url = {http://dx.doi.org/10.1038/nature06289},
volume = {450},
year = {2007}
}
@article{Holt2001,
abstract = {For stimuli modeling stop consonants varying in the acoustic correlates of voice onset time (VOT), human listeners are more likely to perceive stimuli with lower f0's as voiced consonants—a pattern of perception that follows regularities in English speech production. The present study examines the basis of this observation. One hypothesis is that lower f0's enhance perception of voiced stops by virtue of perceptual interactions that arise from the operating characteristics of the auditory system. A second hypothesis is that this perceptual pattern develops as a result of experience with f0-voicing covariation. In a test of these hypotheses, Japanese quail learned to respond to stimuli drawn from a series varying in VOT through training with one of three patterns of f0-voicing covariation. Voicing and f0 varied in the natural pattern (shorter VOT, lower f0), in an inverse pattern (shorter VOT, higher f0), or in a random pattern (no f0-voicing covariation). Birds trained with stimuli that had no f0-voicing ...},
author = {Holt, Lori L. and Lotto, Andrew J. and Kluender, Keith R.},
doi = {10.1121/1.1339825},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {speech intelligibility},
month = {feb},
number = {2},
pages = {764--774},
publisher = {Acoustical Society of AmericaASA},
title = {{Influence of fundamental frequency on stop-consonant voicing perception: A case of learned covariation or auditory enhancement?}},
url = {http://asa.scitation.org/doi/10.1121/1.1339825},
volume = {109},
year = {2001}
}
@article{Fitch2000,
abstract = {The evolution of speech can be studied independently of the evolution of language, with the advantage that most aspects of speech acoustics, physiology and neural control are shared with animals, and thus open to empirical investigation. At least two changes were necessary prerequisites for modern human speech abilities: (1) modification of vocal tract morphology, and (2) development of vocal imitative ability. Despite an extensive literature, attempts to pinpoint the timing of these changes using fossil data have proven inconclusive. However, recent comparative data from nonhuman primates have shed light on the ancestral use of formants (a crucial cue in human speech) to identify individuals and gauge body size. Second, comparative analysis of the diverse vertebrates that have evolved vocal imitation (humans, cetaceans, seals and birds) provides several distinct, testable hypotheses about the adaptive function of vocal mimicry. These developments suggest that, for understanding the evolution of speech, comparative analysis of living species provides a viable alternative to fossil data. However, the neural basis for vocal mimicry and for mimesis in general remains unknown.},
author = {Fitch, W.Tecumseh},
doi = {10.1016/S1364-6613(00)01494-7},
file = {::},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {7},
pages = {258--267},
title = {{The evolution of speech: a comparative review}},
volume = {4},
year = {2000}
}
@article{Blank2016,
abstract = {Successful perception depends on combining sensory input with prior knowledge. However, the underlying mechanism by which these two sources of information are combined is unknown. In speech perception, as in other domains, two functionally distinct coding schemes have been proposed for how expectations influence representation of sensory evidence. Traditional models suggest that expected features of the speech input are enhanced or sharpened via interactive activation (Sharpened Signals). Conversely, Predictive Coding suggests that expected features are suppressed so that unexpected features of the speech input (Prediction Errors) are processed further. The present work is aimed at distinguishing between these two accounts of how prior knowledge influences speech perception. By combining behavioural, univariate, and multivariate fMRI measures of how sensory detail and prior expectations influence speech perception with computational modelling, we provide evidence in favour of Prediction Error computations. Increased sensory detail and informative expectations have additive behavioural and univariate neural effects because they both improve the accuracy of word report and reduce the BOLD signal in lateral temporal lobe regions. However, sensory detail and informative expectations have interacting effects on speech representations shown by multivariate fMRI in the posterior superior temporal sulcus. When prior knowledge was absent, increased sensory detail enhanced the amount of speech information measured in superior temporal multivoxel patterns, but with informative expectations, increased sensory detail reduced the amount of measured information. Computational simulations of Sharpened Signals and Prediction Errors during speech perception could both explain these behavioural and univariate fMRI observations. However, the multivariate fMRI observations were uniquely simulated by a Prediction Error and not a Sharpened Signal model. The interaction between prior expectation and sensory detail provides evidence for a Predictive Coding account of speech perception. Our work establishes methods that can be used to distinguish representations of Prediction Error and Sharpened Signals in other perceptual domains.},
author = {Blank, Helen and Davis, Matthew H},
doi = {10.1371/journal.pbio.1002577},
editor = {Zatorre, Robert},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Blank, Davis - 2016 - Prediction Errors but Not Sharpened Signals Simulate Multivoxel fMRI Patterns during Speech Perception(2).pdf:pdf},
issn = {1545-7885},
journal = {PLoS Biology},
month = {nov},
number = {11},
pages = {e1002577},
pmid = {27846209},
publisher = {Public Library of Science},
title = {{Prediction Errors but Not Sharpened Signals Simulate Multivoxel fMRI Patterns during Speech Perception.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27846209},
volume = {14},
year = {2016}
}
@article{Peterson1952,
abstract = {Relationships between a listener's identification of a spoken vowel and its properties as revealed from acoustic measurement of its sound wave have been a subject of study by many investigators.Both the utterance and the identification of a vowel depend upon the language and dialectal backgrounds and the vocal and auditory characteristics of the individuals concerned.The purpose of this paper is to discuss some of the control methods that have been used in the evaluation of these effects in a vowel study program at Bell Telephone Laboratories.The plan of the study, calibration of recording and measureing equipment, and methods for checking the performance of both speakers and listeners are described.The methods are illustrated from results of tests involving some 76 speakers and 70 listerners.},
author = {Peterson, Gordon E and Barney, Harold L},
doi = {10.1121/1.1906875},
isbn = {0001-4966},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {2},
pages = {175--184},
title = {{Control methods used in a study of the vowels}},
volume = {24},
year = {1952}
}
@article{Gourevitch2009,
abstract = {In order to investigate how the auditory scene is analyzed and perceived, auditory spectrotemporal receptive fields (STRFs) are generally used as a convenient way to describe how frequency and temporal sound information is encoded. However, using broadband sounds to estimate STRFs imperfectly reflects the way neurons process complex stimuli like conspecific vocalizations insofar as natural sounds often show limited bandwidth. Using recordings in the primary auditory cortex of anesthetized cats, we show that presentation of narrowband stimuli not including the best frequency of neurons provokes the appearance of residual peaks and increased firing rate at some specific spectral edges of stimuli compared with classical STRFs obtained from broadband stimuli. This result is the same for STRFs obtained from both spikes and local field potentials. Potential mechanisms likely involve release from inhibition. We thus emphasize some aspects of context dependency of STRFs, that is, how the balance of inhibitory and excitatory inputs is able to shape the neural response from the spectral content of stimuli.},
author = {Gour{\'{e}}vitch, Boris and Nore{\~{n}}a, Arnaud and Shaw, Gregory and Eggermont, Jos J.},
doi = {10.1093/cercor/bhn184},
issn = {1460-2199},
journal = {Cerebral Cortex},
month = {jun},
number = {6},
pages = {1448--1461},
pmid = {18854580},
title = {{Spectrotemporal Receptive Fields in Anesthetized Cat Primary Auditory Cortex Are Context Dependent}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18854580 https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhn184},
volume = {19},
year = {2009}
}
@article{Erickson1998,
abstract = {Psychological theories of categorization generally focus on either rule- or exemplar-based$\backslash$r$\backslash$nexplanations. We present 2 experiments that show evidence of both rule induction and$\backslash$r$\backslash$nexemplar encoding as well as a connectionist model, ATRn.rM, that specifies a mechanism for$\backslash$r$\backslash$ncombining rule- and exemplar-based representation. In 2 experiments participants learned to$\backslash$r$\backslash$nclassify items, most of which followed a simple rule, although there were a few frequently$\backslash$r$\backslash$noccurring exceptions. Experiment 1 examined how people extrapolate beyond the range of$\backslash$r$\backslash$ntraining. Experiment 2 examined the effect of instance frequency on generalization.$\backslash$r$\backslash$nCategorization behavior was well described by the model, in which exemplar representation is$\backslash$r$\backslash$nused for both rule and exception processing. A key element in correctly modeling these results$\backslash$r$\backslash$nwas capturing the interaction between the rule- and exemplar-based representations by using$\backslash$r$\backslash$nshifts of attention between rules and exemplars.},
author = {Erickson, Michael A. and Kruschke, John K.},
doi = {10.1037/0096-3445.127.2.107},
isbn = {0096-3445; 1939-2222},
issn = {0096-3445},
journal = {Journal of Experimental Psychology: General},
number = {2},
pages = {107--140},
pmid = {9622910},
title = {{Rules and Exemplars in Category Learning}},
url = {http://www.indiana.edu/{~}kruschke/articles/EricksonK1998.pdf},
volume = {127},
year = {1998}
}
@inproceedings{Chetouani2002,
author = {Chetouani, M. and Gas, B. and Zarader, J.L. and Chavy, C.},
booktitle = {Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)},
doi = {10.1109/IJCNN.2002.1005585},
isbn = {0-7803-7278-6},
pages = {852--857 vol.1},
publisher = {IEEE},
title = {{Discriminative training for neural predictive coding applied to speech features extraction}},
url = {http://ieeexplore.ieee.org/document/1005585/},
year = {2002}
}
@article{Ranasinghe2013,
abstract = {Neurons at higher stations of each sensory system are responsive to feature combinations not present at lower levels. As a result, the activity of these neurons becomes less redundant than lower levels. We recorded responses to speech sounds from the inferior colliculus and the primary auditory cortex neurons of rats, and tested the hypothesis that primary auditory cortex neurons are more sensitive to combinations of multiple acoustic parameters compared to inferior colliculus neurons. We independently eliminated periodicity information, spectral information and temporal information in each consonant and vowel sound using a noise vocoder. This technique made it possible to test several key hypotheses about speech sound processing. Our results demonstrate that inferior colliculus responses are spatially arranged and primarily determined by the spectral energy and the fundamental frequency of speech, whereas primary auditory cortex neurons generate widely distributed responses to multiple acoustic parameters, and are not strongly influenced by the fundamental frequency of speech. We found no evidence that inferior colliculus or primary auditory cortex was specialized for speech features such as voice onset time or formants. The greater diversity of responses in primary auditory cortex compared to inferior colliculus may help explain how the auditory system can identify a wide range of speech sounds across a wide range of conditions without relying on any single acoustic cue. ?? 2013 IBRO.},
author = {Ranasinghe, K. G. and Vrana, W. A. and Matney, C. J. and Kilgard, M. P.},
doi = {10.1016/j.neuroscience.2013.08.005},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Ranasinghe et al. - 2013 - Increasing diversity of neural responses to speech sounds across the central auditory pathway(2).pdf:pdf},
isbn = {1873-7544 (Electronic)
0306-4522 (Linking)},
issn = {03064522},
journal = {Neuroscience},
keywords = {Multiple acoustic parameters,Neural response diversity,Noise-vocoded speech,Rat auditory system,Redundancy reduction},
pages = {80--97},
pmid = {23954862},
publisher = {IBRO},
title = {{Increasing diversity of neural responses to speech sounds across the central auditory pathway}},
url = {http://dx.doi.org/10.1016/j.neuroscience.2013.08.005},
volume = {252},
year = {2013}
}
@book{Wittgenstein1958,
address = {Oxford, UK},
author = {Wittgenstein, Ludwig},
editor = {Blackwell, Basil},
pages = {30--36},
publisher = {Basil Blackwell Ltd.},
title = {{Philosophical Investigations}},
year = {1958}
}
@article{Jayaraman2015,
author = {Jayaraman, Swapnaa and Fausey, Caitlin M. and Smith, Linda B.},
doi = {10.1371/journal.pone.0123780},
editor = {Nardini, Marko},
file = {::},
issn = {1932-6203},
journal = {PLOS ONE},
month = {may},
number = {5},
pages = {e0123780},
publisher = {Public Library of Science},
title = {{The Faces in Infant-Perspective Scenes Change over the First Year of Life}},
url = {http://dx.plos.org/10.1371/journal.pone.0123780},
volume = {10},
year = {2015}
}
@article{Norris2016,
abstract = {Speech perception involves prediction, but how is that prediction implemented? In cognitive models prediction has often been taken to imply that there is feedback of activation from lexical to pre-lexical processes as implemented in interactive-activation models (IAMs). We show that simple activation feedback does not actually improve speech recognition. However, other forms of feedback can be beneficial. In particular, feedback can enable the listener to adapt to changing input, and can potentially help the listener to recognise unusual input, or recognise speech in the presence of competing sounds. The common feature of these helpful forms of feedback is that they are all ways of optimising the performance of speech recognition using Bayesian inference. That is, listeners make predictions about speech because speech recognition is optimal in the sense captured in Bayesian models.},
author = {Norris, Dennis and McQueen, James M and Cutler, Anne},
doi = {10.1080/23273798.2015.1081703},
file = {::},
issn = {2327-3798},
journal = {Language, cognition and neuroscience},
keywords = {Bayesian inference,Speech recognition,feedback,prediction},
month = {jan},
number = {1},
pages = {4--18},
pmid = {26740960},
publisher = {Taylor {\&} Francis},
title = {{Prediction, Bayesian inference and feedback in speech recognition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26740960 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4685608},
volume = {31},
year = {2016}
}
@article{Ison2007a,
abstract = {Auditory brainstem-evoked response (ABR) thresholds were obtained in a longitudinal study of C57BL/6J mice between 10 and 53 weeks old, with repeated testing every 2 weeks. On alternate weeks, acoustic startle reflex (ASR) amplitudes were measured, elicited by tone pips with stimulus frequencies of 3, 6, 12, and 24 kHz, and intensities from subthreshold up to 110 dB sound pressure level. The increase in ABR thresholds for 3 and 6 kHz test stimuli followed a linear time course with increasing age from 10 to 53 weeks, with a slope of about 0.7 dB/week, and for 48 kHz a second linear time course, but beginning at 10 weeks with a slope of about 2.3 dB/week. ABR thresholds for 12, 24, and 32 kHz increased after one linear segment with a 0.7 dB slope, then after a variable delay related to the test frequency, shifted to a second segment having slopes of 3-5 dB/week. Hearing loss initially reduced the ASR for all eliciting stimuli, but at about 6 months of age, the response elicited by intense 3 and 6 kHz stimuli began to increase to reach values about three times above normal, and previously subthreshold stimuli came to elicit vigorous responses seen at first only for the intense stimuli. This hyperacusis-like effect appeared in all mice but was especially pronounced in mice with more serious hearing loss. These ABR data, together with a review of histopathological data in the C57BL/6 literature, suggest that the non-frequency-specific slow time course of hearing loss results from pathology in the lateral wall of the cochlea, whereas the stimulus-specific hearing loss with a rapid time course results from hair cell loss. Delayed exaggeration of the ASR with hearing loss reveals a deficit in centrifugal inhibitory control over the afferent reflex pathways after central neural reorganization, suggesting that this mouse may provide a useful model of age-related tinnitus and associated hyperacusis.},
annote = {From Duplicate 2 (Age-related hearing loss in C57BL/6J mice has both frequency-specific and non-frequency-specific components that produce a hyperacusis-like exaggeration of the acoustic startle reflex - Ison, James R.; Allen, Paul D.; O'Neill, William E.)

10.1007/s10162-007-0098-3},
author = {Ison, James R. and Allen, Paul D. and O'Neill, William E.},
doi = {10.1007/s10162-007-0098-3},
isbn = {1525-3961 (Print)$\backslash$n1438-7573 (Linking)},
issn = {15253961},
journal = {JARO - Journal of the Association for Research in Otolaryngology},
keywords = {Aging,Hearing loss,Mixed strial/sensory presbycusis,Plasticity,Startle,Tinnitus/hyperacusis},
month = {nov},
number = {4},
pages = {539--550},
pmid = {17952509},
publisher = {Springer-Verlag},
title = {{Age-related hearing loss in C57BL/6J mice has both frequency-specific and non-frequency-specific components that produce a hyperacusis-like exaggeration of the acoustic startle reflex}},
url = {http://link.springer.com/10.1007/s10162-007-0098-3 http://dx.doi.org/10.1007/s10162-007-0098-3},
volume = {8},
year = {2007}
}
@article{Nosofsky1988,
author = {Nosofsky, Robert M. and M., Robert},
doi = {10.1037/0278-7393.14.1.54},
file = {::},
issn = {0278-7393},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
keywords = {adults,classification {\&} typicality ratings,stimulus similarity {\&} frequency},
number = {1},
pages = {54--65},
publisher = {American Psychological Association},
title = {{Similarity, frequency, and category representations.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.14.1.54},
volume = {14},
year = {1988}
}
@article{Malone2015,
author = {Malone, Brian J. and Scott, Brian H. and Semple, Malcolm N.},
journal = {Journal of Neurophysiology},
number = {7},
title = {{Diverse cortical codes for scene segmentation in primate auditory cortex}},
url = {http://jn.physiology.org/content/113/7/2934},
volume = {113},
year = {2015}
}
@article{Clerkin2016,
author = {Clerkin, Elizabeth M. and Hart, Elizabeth and Rehg, James M. and Yu, Chen and Smith, Linda B.},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
number = {1711},
title = {{Real-world visual statistics and infants' first-learned object names}},
url = {http://rstb.royalsocietypublishing.org/content/372/1711/20160055.article-info},
volume = {372},
year = {2016}
}
@incollection{Flemming2005,
address = {Oxford, UK},
author = {Flemming, Edward},
booktitle = {The Handbook of Speech Perception},
doi = {10.1002/9780470757024.ch7},
file = {::},
isbn = {9780470757024},
keywords = {optimality theory,phonetic descriptions,phonological contrast,phonological patterns,speech perception},
pages = {156--181},
publisher = {Blackwell Publishing Ltd},
title = {{Speech Perception and Phonological Contrast}},
url = {http://doi.wiley.com/10.1002/9780470757024.ch7},
year = {2005}
}
@article{Kobler1985,
author = {Kobler, J.B. and Wilson, B.S. and Henson, O.W. and Bishop, A.L.},
doi = {10.1016/0378-5955(85)90161-3},
issn = {03785955},
journal = {Hearing Research},
month = {jan},
number = {2},
pages = {99--108},
title = {{Echo intensity compensation by echolocating bats}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0378595585901613},
volume = {20},
year = {1985}
}
@article{Arnal2012,
abstract = {Many theories of perception are anchored in the central notion that the brain continuously updates an internal model of the world to infer the probable causes of sensory events. In this framework, the brain needs not only to predict the causes of sensory input, but also when they are most likely to happen. In this article, we review the neurophysiological bases of sensory predictions of “what' (predictive coding) and ‘when' (predictive timing), with an emphasis on low-level oscillatory mechanisms. We argue that neural rhythms offer distinct and adapted computational solutions to predicting ‘what' is going to happen in the sensory environment and ‘when'.},
author = {Arnal, Luc H. and Giraud, Anne-Lise},
doi = {10.1016/j.tics.2012.05.003},
file = {::},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {7},
pages = {390--398},
title = {{Cortical oscillations and sensory predictions}},
volume = {16},
year = {2012}
}
@article{Sadagopan2009,
abstract = {In the auditory cortex of awake animals, a substantial number of neurons do not respond to pure tones. These neurons have historically been classified as "unresponsive" and even been speculated as being nonauditory. We discovered, however, that many of these neurons in the primary auditory cortex (A1) of awake marmoset monkeys were in fact highly selective for complex sound features. We then investigated how such selectivity might arise from the tone-tuned inputs that these neurons likely receive. We found that these non-tone responsive neurons exhibited nonlinear combination-sensitive responses that require precise spectral and temporal combinations of two tone pips. The nonlinear spectrotemporal maps derived from these neurons were correlated with their selectivity for complex acoustic features. These non-tone responsive and nonlinear neurons were commonly encountered at superficial cortical depths in A1. Our findings demonstrate how temporally and spectrally specific nonlinear integration of putative tone-tuned inputs might underlie a diverse range of high selectivity of A1 neurons in awake animals. We propose that describing A1 neurons with complex response properties in terms of tone-tuned input channels can conceptually unify a wide variety of observed neural selectivity to complex sounds into a lower dimensional description.},
author = {Sadagopan, S. and Wang, X.},
doi = {10.1523/JNEUROSCI.1286-09.2009},
issn = {0270-6474},
journal = {Journal of Neuroscience},
month = {sep},
number = {36},
pages = {11192--11202},
pmid = {19741126},
title = {{Nonlinear Spectrotemporal Interactions Underlying Selectivity for Complex Sounds in Auditory Cortex}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19741126 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2757444 http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1286-09.2009},
volume = {29},
year = {2009}
}
@article{Shepard1987,
abstract = {A psychological space is established for any set of stimuli by determining metric distances between the stimuli such that the probability that a response learned to any stimulus will generalize to any other is an invariant monotonic function of the distance between them. To a good approximation, this probability of generalization (i) decays exponentially with this distance, and (ii) does so in accordance with one of two metrics, depending on the relation between the dimensions along which the stimuli vary. These empirical regularities are mathematically derivable from universal principles of natural kinds and probabilistic geometry that may, through evolutionary internalization, tend to govern the behaviors of all sentient organisms.},
author = {Shepard, R.},
doi = {10.1126/science.3629243},
isbn = {0036-8075 (Print)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science},
number = {4820},
pages = {1317--1323},
pmid = {3629243},
title = {{Toward a universal law of generalization for psychological science}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.3629243},
volume = {237},
year = {1987}
}
@article{Bartlett2013,
abstract = {The auditory thalamus, or medial geniculate body (MGB), is the primary sensory input to auditory cortex. Therefore, it plays a critical role in the complex auditory processing necessary for robust speech perception. This review will describe the functional organization of the thalamus as it relates to processing acoustic features important for speech perception, focusing on thalamic nuclei that relate to auditory representations of language sounds. The MGB can be divided into three main subdivisions, the ventral, dorsal, and medial subdivisions, each with different connectivity, auditory response properties, neuronal properties, and synaptic properties. Together, the MGB subdivisions actively and dynamically shape complex auditory processing and form ongoing communication loops with auditory cortex and subcortical structures. Copyright {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
annote = {From Duplicate 1 (The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception - Bartlett, Edward L.)

From Duplicate 1 (The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception. - Bartlett, Edward L.)

10.1016/j.bandl.2013.03.003},
author = {Bartlett, Edward L.},
doi = {10.1016/j.bandl.2013.03.003},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Bartlett - 2013 - The organization and physiology of the auditory thalamus and its role in processing acoustic features important for(2).pdf:pdf},
issn = {10902155},
journal = {Brain and language},
month = {jul},
number = {1},
pages = {29--48},
pmid = {23725661},
title = {{The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception.}},
url = {http://dx.doi.org/10.1016/j.bandl.2013.03.003 http://linkinghub.elsevier.com/retrieve/pii/S0093934X13000722},
volume = {126},
year = {2013}
}
@article{Clauset2007a,
abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.},
archivePrefix = {arXiv},
arxivId = {0706.1062},
author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, M. E. J.},
doi = {10.1137/070710111},
eprint = {0706.1062},
file = {::},
month = {jun},
title = {{Power-law distributions in empirical data}},
url = {http://arxiv.org/abs/0706.1062 http://dx.doi.org/10.1137/070710111},
year = {2007}
}


@article{ahrensNonlinearitiesContextualInfluences2008,
  title = {Nonlinearities and Contextual Influences in Auditory Cortical Responses Modeled with Multilinear Spectrotemporal Methods},
  author = {Ahrens, Misha B. and Linden, Jennifer F. and Sahani, Maneesh},
  year = {2008},
  month = feb,
  journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  volume = {28},
  number = {8},
  pages = {1929--1942},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.3377-07.2008},
  abstract = {The relationship between a sound and its neural representation in the auditory cortex remains elusive. Simple measures such as the frequency response area or frequency tuning curve provide little insight into the function of the auditory cortex in complex sound environments. Spectrotemporal receptive field (STRF) models, despite their descriptive potential, perform poorly when used to predict auditory cortical responses, showing that nonlinear features of cortical response functions, which are not captured by STRFs, are functionally important. We introduce a new approach to the description of auditory cortical responses, using multilinear modeling methods. These descriptions simultaneously account for several nonlinearities in the stimulus-response functions of auditory cortical neurons, including adaptation, spectral interactions, and nonlinear sensitivity to sound level. The models reveal multiple inseparabilities in cortical processing of time lag, frequency, and sound level, and suggest functional mechanisms by which auditory cortical neurons are sensitive to stimulus context. By explicitly modeling these contextual influences, the models are able to predict auditory cortical responses more accurately than are STRF models. In addition, they can explain some forms of stimulus dependence in STRFs that were previously poorly understood.},
  langid = {english},
  pmcid = {PMC6671443},
  pmid = {18287509},
  keywords = {_tablet,Acoustic Stimulation,Animals,Auditory Cortex,Mice,Models; Biological,Nonlinear Dynamics,Rats,Time Factors},
  file = {/Users/jonny/Dropbox/papers/zotero/A/AhrensM/ahrens_2008_nonlinearities_and_contextual_influences_in_auditory_cortical_responses_modeled.pdf;/Users/jonny/Zotero/storage/5Q7EPMDH/ahrens_2008_nonlinearities_and_contextual_influences_in_auditory_cortical_responses_modeled.pdf}
}

@article{Alain2015,
  title = {Auditory {{Scene Analysis}}: {{Tales}} from {{Cognitive Neurosciences}}},
  author = {Alain, Claude and Bernstein, Lori J},
  year = {2015},
  month = sep,
  journal = {Music Perception: An Interdisciplinary Journal},
  volume = {33},
  number = {1},
  pages = {70--82},
  issn = {0730-7829},
  doi = {10.1525/mp.2015.33.1.70},
  keywords = {_tablet,attention,erp,neuroimaging,streaming},
  file = {/Users/jonny/Dropbox/papers/zotero/A/AlainC/alain_2015_auditory_scene_analysis.pdf}
}

@misc{alammarIllustratedTransformer,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  abstract = {Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Chinese (Simplified), French, Japanese, Korean, Russian, Spanish Watch: MIT's Deep Learning State of the Art lecture referencing this post In the previous post, we looked at Attention \textendash{} a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \textendash{} a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud's recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let's try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard's NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. 2020 Update: I've created a ``Narrated Transformer'' video which is a gentler approach to the topic: A High-Level Look Let's begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
  howpublished = {https://jalammar.github.io/illustrated-transformer/}
}

@misc{alammarVisualizingNeuralMachine,
  title = {Visualizing {{A Neural Machine Translation Model}} ({{Mechanics}} of {{Seq2seq Models With Attention}})},
  author = {Alammar, Jay},
  abstract = {Translations: Chinese (Simplified), Japanese, Korean, Russian, Turkish Watch: MIT's Deep Learning State of the Art lecture referencing this post May 25th update: New graphics (RNN animation, word embedding graph), color coding, elaborated on the final attention example. Note: The animations below are videos. Touch or hover on them (if you're using a mouse) to get play controls so you can pause if needed. Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014). I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That's what I aim to do in this post. You'll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post). A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images\ldots etc) and outputs another sequence of items. A trained model would work like this:      Your browser does not support the video tag.},
  howpublished = {https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/}
}

@article{angeloniContextualModulationSound2018,
  title = {Contextual Modulation of Sound Processing in the Auditory Cortex},
  author = {Angeloni, C and Geffen, MN},
  year = {2018},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  series = {Neurobiology of {{Behavior}}},
  volume = {49},
  pages = {8--15},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2017.10.012},
  abstract = {In everyday acoustic environments, we navigate through a maze of sounds that possess a complex spectrotemporal structure, spanning many frequencies and exhibiting temporal modulations that differ within frequency bands. Our auditory system needs to efficiently encode the same sounds in a variety of different contexts, while preserving the ability to separate complex sounds within an acoustic scene. Recent work in auditory neuroscience has made substantial progress in studying how sounds are represented in the auditory system under different contexts, demonstrating that auditory processing of seemingly simple acoustic features, such as frequency and time, is highly dependent on co-occurring acoustic and behavioral stimuli. Through a combination of electrophysiological recordings, computational analysis and behavioral techniques, recent research identified the interactions between external spectral and temporal context of stimuli, as well as the internal behavioral state.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/A/AngeloniC/angeloni_2018_contextual_modulation_of_sound_processing_in_the_auditory_cortex.pdf;/Users/jonny/Zotero/storage/K67RBJ93/angeloni_2018_contextual_modulation_of_sound_processing_in_the_auditory_cortex.pdf}
}

@article{aravamudhanPerceptualContextEffects2008,
  title = {Perceptual Context Effects of Speech and Nonspeech Sounds: The Role of Auditory Categories},
  shorttitle = {Perceptual Context Effects of Speech and Nonspeech Sounds},
  author = {Aravamudhan, Radhika and Lotto, Andrew J. and Hawks, John W.},
  year = {2008},
  month = sep,
  journal = {The Journal of the Acoustical Society of America},
  volume = {124},
  number = {3},
  pages = {1695--1703},
  issn = {1520-8524},
  doi = {10.1121/1.2956482},
  abstract = {Williams [(1986). "Role of dynamic information in the perception of coarticulated vowels," Ph.D. thesis, University of Connecticut, Standford, CT] demonstrated that nonspeech contexts had no influence on pitch judgments of nonspeech targets, whereas context effects were obtained when instructed to perceive the sounds as speech. On the other hand, Holt et al. [(2000). "Neighboring spectral content influences vowel identification," J. Acoust. Soc. Am. 108, 710-722] showed that nonspeech contexts were sufficient to elicit context effects in speech targets. The current study was to test a hypothesis that could explain the varying effectiveness of nonspeech contexts: Context effects are obtained only when there are well-established perceptual categories for the target stimuli. Experiment 1 examined context effects in speech and nonspeech signals using four series of stimuli: steady-state vowels that perceptually spanned from /inverted ohm/-/I/ in isolation and in the context of /w/ (with no steady-state portion) and two nonspeech sine-wave series that mimicked the acoustics of the speech series. In agreement with previous work context effects were obtained for speech contexts and targets but not for nonspeech analogs. Experiment 2 tested predictions of the hypothesis by testing for nonspeech context effects after the listeners had been trained to categorize the sounds. Following training, context-dependent categorization was obtained for nonspeech stimuli in the training group. These results are presented within a general perceptual-cognitive framework for speech perception research.},
  langid = {english},
  pmcid = {PMC2601703},
  pmid = {19045660},
  keywords = {_tablet,Acoustic Stimulation,Adult,Auditory Perception,Auditory Threshold,Cognition,Cues,Humans,Pitch Perception,Signal Detection; Psychological,Sound,Sound Spectrography,Speech Acoustics,Speech Discrimination Tests,Speech Perception,Time Factors,Young Adult},
  file = {/Users/jonny/Dropbox/papers/zotero/A/AravamudhanR/aravamudhan_2008_perceptual_context_effects_of_speech_and_nonspeech_sounds.pdf}
}

@techreport{ashwoodMiceAlternateDiscrete2020,
  type = {Preprint},
  title = {Mice Alternate between Discrete Strategies during Perceptual Decision-Making},
  author = {Ashwood, Zoe C. and Roy, Nicholas A. and Stone, Iris R. and {The International Brain Laboratory} and Churchland, Anne K. and Pouget, Alexandre and Pillow, Jonathan W.},
  year = {2020},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.10.19.346353},
  abstract = {Classical models of perceptual decision-making assume that animals use a single, consistent strategy to integrate sensory evidence and form decisions during an experiment. Here we provide analyses showing that this common view is incorrect. We use a latent variable modeling framework to show that decision-making behavior in mice reflects an interplay between different strategies that alternate on a timescale of tens to hundreds of trials. This model provides a powerful alternate explanation for ``lapses'' commonly observed during psychophysical experiments. Formally, our approach consists of a Hidden Markov Model (HMM) with states corresponding to different decision-making strategies, each parameterized by a distinct Bernoulli generalized linear model (GLM). We fit the resulting model (GLM-HMM) to choice data from two large cohorts of mice in different perceptual decision-making tasks. For both datasets, we found that mouse decision-making was far better described by a GLM-HMM with 3 or 4 states than by a traditional psychophysical model with lapses. The identified states were highly consistent across animals, consisting of a single ``engaged'' state, in which the strategy relied heavily on the sensory stimulus, and multiple biased or disengaged states in which accuracy was low. These states persisted for many trials, suggesting that lapses were not independent, but reflected state dynamics in which animals were relatively engaged or disengaged for extended periods of time. We found that for most animals, response times and violation rates were positively correlated with disengagement, providing independent correlates of the identified changes in strategy. The GLM-HMM framework thus provides a powerful lens for the analysis of decision-making, and suggests that standard measures of psychophysical performance mask the presence of slow but dramatic alternations in strategy across trials.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/A/AshwoodZ/ashwood_2020_mice_alternate_between_discrete_strategies_during_perceptual_decision-making.pdf}
}

@article{atencioCooperativeNonlinearitiesAuditory2008,
  title = {Cooperative {{Nonlinearities}} in {{Auditory Cortical Neurons}}},
  author = {Atencio, Craig A. and Sharpee, Tatyana O. and Schreiner, Christoph E.},
  year = {2008},
  month = jun,
  journal = {Neuron},
  volume = {58},
  number = {6},
  pages = {956--966},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2008.04.026},
  abstract = {Cortical receptive fields represent the signal preferences of sensory neurons. Receptive fields are thought to provide a representation of sensory experience from which the cerebral cortex may make interpretations. While it is essential to determine a neuron's receptive field, it remains unclear which features of the acoustic environment are specifically represented by neurons in the primary auditory cortex (AI). We characterized cat AI spectrotemporal receptive fields (STRFs) by finding both the spike-triggered average (STA) and stimulus dimensions that maximized the mutual information between response and stimulus. We derived a nonlinearity relating spiking to stimulus projection onto two maximally informative dimensions (MIDs). The STA was highly correlated with the first MID. Generally, the nonlinearity for the first MID was asymmetric and often monotonic in shape, while the second MID nonlinearity was symmetric and nonmonotonic. The joint nonlinearity for both MIDs revealed that most first and second MIDs were synergistic and thus should be considered conjointly. The difference between the nonlinearities suggests different possible roles for the MIDs in auditory processing.},
  langid = {english},
  keywords = {_tablet,SYSNEURO},
  file = {/Users/jonny/Dropbox/papers/zotero/A/AtencioC/atencio_2008_cooperative_nonlinearities_in_auditory_cortical_neurons.pdf;/Users/jonny/Zotero/storage/PHZKBLYJ/atencio_2008_cooperative_nonlinearities_in_auditory_cortical_neurons.pdf}
}

@article{atencioMultidimensionalReceptiveField2017,
  title = {Multidimensional Receptive Field Processing by Cat Primary Auditory Cortical Neurons},
  author = {Atencio, Craig A. and Sharpee, Tatyana O.},
  year = {2017},
  month = sep,
  journal = {Neuroscience},
  volume = {359},
  pages = {130--141},
  issn = {1873-7544},
  doi = {10.1016/j.neuroscience.2017.07.003},
  abstract = {The receptive fields of many auditory cortical neurons are multidimensional and are best represented by more than one stimulus feature. The number of these dimensions, their characteristics, and how they differ with stimulus context have been relatively unexplored. Standard methods that are often used to characterize multidimensional stimulus selectivity, such as spike-triggered covariance (STC) or maximally informative dimensions (MIDs), are either limited to Gaussian stimuli or are only able to recover a small number of stimulus features due to data limitations. An information theoretic extension of STC, the maximum noise entropy (MNE) model, can be used with non-Gaussian stimulus distributions to find an arbitrary number of stimulus dimensions. When we applied the MNE model to auditory cortical neurons, we often found more than two stimulus features that influenced neuronal firing. Excitatory and suppressive features coded different acoustic contexts: excitatory features encoded higher temporal and spectral modulations, while suppressive features had lower modulation frequency preferences. We found that the excitatory and suppressive features themselves were sensitive to stimulus context when we employed two stimuli that differed only in their short-term correlation structure: while the linear features were similar, the secondary features were strongly affected by stimulus statistics. These results show that multidimensional receptive field processing is influenced by feature type and stimulus context.},
  langid = {english},
  pmcid = {PMC5600511},
  pmid = {28694174},
  keywords = {_tablet,Acoustic Stimulation,Animals,Auditory Cortex,Auditory Perception,cat,Cats,Female,Information Theory,Models; Neurological,modulation,Neurons,primary auditory cortex,spectrotemporal,stimulus statistics,strf},
  file = {/Users/jonny/Dropbox/papers/zotero/A/AtencioC/atencio_2017_multidimensional_receptive_field_processing_by_cat_primary_auditory_cortical.pdf;/Users/jonny/Zotero/storage/6NUPN3A4/atencio_2017_multidimensional_receptive_field_processing_by_cat_primary_auditory_cortical.pdf}
}

@misc{AttentionAttention2018,
  title = {Attention? {{Attention}}!},
  shorttitle = {Attention?},
  year = {2018},
  month = jun,
  journal = {Lil'Log},
  abstract = {Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL.},
  howpublished = {https://lilianweng.github.io/2018/06/24/attention-attention.html},
  langid = {english}
}

@article{Bailey1980,
  title = {Information in Speech: Observations on the Perception of [s]-Stop Clusters},
  author = {Bailey, P J and Summerfield, Q},
  year = {1980},
  month = aug,
  journal = {Journal of experimental psychology. Human perception and performance},
  volume = {6},
  number = {3},
  pages = {536--563},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.6.3.536},
  abstract = {A series of experiments is reported that investigated the pattern of acoustic information specifying place and manner of stop consonants in medial position after [s]. In both production and perception, information for stop place includes the spectrum of the fricative at offset, the duration of the silent closure interval, the spectral relationship between the frequency of the stop release burst and the following periodically excited formants, and the spectral and temporal characteristics of the first formant transition. Similarly, the information for stop manner includes the duration of silent closure, the frequency of the first formant at the release, the magnitude of the first formant transition, and the proximity of the second and third formants at release. A relationship was shown to exist in perception between the spectral characteristics of the first formant and the duration of the silent closure required to hear a stop. This appears to reciprocate the covariation of these parameters in production across different places of articulation and different vocalic contexts. The existence of perceptual sensitivity to a wide range of the acoustic consequences of production questions the efficacy of accounts of speech perception in terms of the fractionation of the signal into elemental acoustic cues, which are then integrated to yield a phonetic percept. It is argued that it is inappropriate to ascribe a psychological status to cues whose only reality is their operational role as physical parameters whose manipulation can change the phenotic interpretation of a signal. It is suggested that the metric of the information for phonetic perception cannot be that of the cues; rather, a metric should be sought in which acoustic and articulatory dynamics are isomorphic.},
  isbn = {0096-1523},
  pmid = {6447767},
  keywords = {_tablet,Humans,Phonetics,Psychoacoustics,Speech Percept},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BaileyP/bailey_1980_information_in_speech.pdf;/Users/jonny/Dropbox/papers/zotero/B/BaileyP/false}
}

@article{bakerPhilosophicalUnderstandingRepresentation2021,
  title = {A {{Philosophical Understanding}} of {{Representation}} for {{Neuroscience}}},
  author = {Baker, Ben and Lansdell, Benjamin and Kording, Konrad},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.06592 [q-bio]},
  eprint = {2102.06592},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {Typical systems neuroscience experiments correlate the activity of recorded neurons with controlled variations in stimuli or behavior. Such experiments are commonly taken to reveal that the neural activity is a representation of the stimulus or behavior. But what does this statement mean? Representation is a philosophical concept with a rich history going back at least to Aristotle. A careful reading of contemporary philosophy suggests four criteria of representation as they relate to neuroscience. First, a representation needs to correlate with the object in reality that it represents. Second, a representation needs to have some causal impact on behavior. Third, a representation should have its causal role exclusively -- the absence of the representation should block the behavior it enables. Fourth, a representation should have teleological justification that specifies the cases in which it is a misrepresentation. We argue that not taking all four criteria seriously leads to statements that are neither neuroscientifically nor philosophically satisfying. We discuss how neuroscience can move towards a deeper notion of representation.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Quantitative Biology - Neurons and Cognition},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BakerB/baker_2021_a_philosophical_understanding_of_representation_for_neuroscience.pdf}
}

@article{barlowSingleUnitsSensation1972,
  title = {Single {{Units}} and {{Sensation}}: {{A Neuron Doctrine}} for {{Perceptual Psychology}}?},
  shorttitle = {Single {{Units}} and {{Sensation}}},
  author = {Barlow, H B},
  year = {1972},
  month = dec,
  journal = {Perception},
  volume = {1},
  number = {4},
  pages = {371--394},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0301-0066},
  doi = {10.1068/p010371},
  abstract = {The problem discussed is the relationship between the firing of single neurons in sensory pathways and subjectively experienced sensations. The conclusions are formulated as the following five dogmas:, , , The development of the concepts leading up to these speculative dogmas, their experimental basis, and some of their limitations are discussed.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BarlowH/barlow_1972_single_units_and_sensation.pdf}
}

@article{bartlettOrganizationPhysiologyAuditory2013c,
  title = {The Organization and Physiology of the Auditory Thalamus and Its Role in Processing Acoustic Features Important for Speech Perception},
  author = {Bartlett, Edward L.},
  year = {2013},
  month = jul,
  journal = {Brain and Language},
  volume = {126},
  number = {1},
  pages = {29--48},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2013.03.003},
  abstract = {The auditory thalamus, or medial geniculate body (MGB), is the primary sensory input to auditory cortex. Therefore, it plays a critical role in the complex auditory processing necessary for robust speech perception. This review will describe the functional organization of the thalamus as it relates to processing acoustic features important for speech perception, focusing on thalamic nuclei that relate to auditory representations of language sounds. The MGB can be divided into three main subdivisions, the ventral, dorsal, and medial subdivisions, each with different connectivity, auditory response properties, neuronal properties, and synaptic properties. Together, the MGB subdivisions actively and dynamically shape complex auditory processing and form ongoing communication loops with auditory cortex and subcortical structures.},
  keywords = {Amplitude modulation,Calbindin,Corticothalamic,Inferior colliculus,Marmoset,Monotonic,Thalamic reticular nucleus,Thalamocortical,Vocalizations},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BartlettE/false}
}

@article{battistonNetworksPairwiseInteractions2020,
  title = {Networks beyond Pairwise Interactions: Structure and Dynamics},
  shorttitle = {Networks beyond Pairwise Interactions},
  author = {Battiston, Federico and Cencetti, Giulia and Iacopini, Iacopo and Latora, Vito and Lucas, Maxime and Patania, Alice and Young, Jean-Gabriel and Petri, Giovanni},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.01764 [cond-mat, physics:nlin, physics:physics, q-bio]},
  eprint = {2006.01764},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:nlin, physics:physics, q-bio},
  abstract = {The complexity of many biological, social and technological systems stems from the richness of the interactions among their units. Over the past decades, a great variety of complex systems has been successfully described as networks whose interacting pairs of nodes are connected by links. Yet, in face-to-face human communication, chemical reactions and ecological systems, interactions can occur in groups of three or more nodes and cannot be simply described just in terms of simple dyads. Until recently, little attention has been devoted to the higher-order architecture of real complex systems. However, a mounting body of evidence is showing that taking the higher-order structure of these systems into account can greatly enhance our modeling capacities and help us to understand and predict their emerging dynamical behaviors. Here, we present a complete overview of the emerging field of networks beyond pairwise interactions. We first discuss the methods to represent higher-order interactions and give a unified presentation of the different frameworks used to describe higher-order systems, highlighting the links between the existing concepts and representations. We review the measures designed to characterize the structure of these systems and the models proposed in the literature to generate synthetic structures, such as random and growing simplicial complexes, bipartite graphs and hypergraphs. We introduce and discuss the rapidly growing research on higher-order dynamical systems and on dynamical topology. We focus on novel emergent phenomena characterizing landmark dynamical processes, such as diffusion, spreading, synchronization and games, when extended beyond pairwise interactions. We elucidate the relations between higher-order topology and dynamical properties, and conclude with a summary of empirical applications, providing an outlook on current modeling and conceptual frontiers.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Social and Information Networks,Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Physics - Physics and Society,Quantitative Biology - Neurons and Cognition},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BattistonF/battiston_2020_networks_beyond_pairwise_interactions.pdf;/Users/jonny/Zotero/storage/G3LSSNH5/2006.html}
}

@article{beckMarginalizationNeuralCircuits2011,
  title = {Marginalization in {{Neural Circuits}} with {{Divisive Normalization}}},
  author = {Beck, Jeffrey M. and Latham, Peter E. and Pouget, Alexandre},
  year = {2011},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {31},
  number = {43},
  pages = {15310--15319},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1706-11.2011},
  abstract = {A wide range of computations performed by the nervous system involves a type of probabilistic inference known as marginalization. This computation comes up in seemingly unrelated tasks, including causal reasoning, odor recognition, motor control, visual tracking, coordinate transformations, visual search, decision making, and object recognition, to name just a few. The question we address here is: how could neural circuits implement such marginalizations? We show that when spike trains exhibit a particular type of statistics\textemdash associated with constant Fano factors and gain-invariant tuning curves, as is often reported in vivo\textemdash some of the more common marginalizations can be achieved with networks that implement a quadratic nonlinearity and divisive normalization, the latter being a type of nonlinear lateral inhibition that has been widely reported in neural circuits. Previous studies have implicated divisive normalization in contrast gain control and attentional modulation. Our results raise the possibility that it is involved in yet another, highly critical, computation: near optimal marginalization in a remarkably wide range of tasks.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2011 the authors 0270-6474/11/3115310-10\$15.00/0},
  langid = {english},
  pmid = {22031877},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BeckJ/beck_2011_marginalization_in_neural_circuits_with_divisive_normalization.pdf}
}

@article{beiranShapingDynamicsMultiple2020,
  title = {Shaping Dynamics with Multiple Populations in Low-Rank Recurrent Networks},
  author = {Beiran, Manuel and Dubreuil, Alexis and Valente, Adrian and Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2020},
  month = nov,
  journal = {arXiv:2007.02062 [q-bio]},
  eprint = {2007.02062},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {An emerging paradigm proposes that neural computations can be understood at the level of dynamical systems that govern low-dimensional trajectories of collective neural activity. How the connectivity structure of a network determines the emergent dynamical system however remains to be clarified. Here we consider a novel class of models, Gaussian-mixture low-rank recurrent networks, in which the rank of the connectivity matrix and the number of statistically-defined populations are independent hyper-parameters. We show that the resulting collective dynamics form a dynamical system, where the rank sets the dimensionality and the population structure shapes the dynamics. In particular, the collective dynamics can be described in terms of a simplified effective circuit of interacting latent variables. While having a single, global population strongly restricts the possible dynamics, we demonstrate that if the number of populations is large enough, a rank-R network can approximate any R-dimensional dynamical system.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Quantitative Biology - Neurons and Cognition},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BeiranM/beiran_2020_shaping_dynamics_with_multiple_populations_in_low-rank_recurrent_networks.pdf;/Users/jonny/Zotero/storage/PZQIA8YX/2007.html}
}

@article{belinVoiceselectiveAreasHuman2000b,
  title = {Voice-Selective Areas in Human Auditory Cortex},
  author = {Belin, Pascal and Zatorre, Robert J. and Lafaille, Philippe and Ahad, Pierre and Pike, Bruce},
  year = {2000},
  month = jan,
  journal = {Nature},
  volume = {403},
  number = {6767},
  pages = {309--312},
  issn = {0028-0836},
  doi = {10.1038/35002078},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BelinP/belin_2000_voice-selective_areas_in_human_auditory_cortex.pdf;/Users/jonny/Papers/BelinP/2000/Belin_2000_Voice-selective areas in human auditory cortex2.pdf}
}

@article{benchenaneCoherentThetaOscillations2010,
  title = {Coherent Theta Oscillations and Reorganization of Spike Timing in the Hippocampal- Prefrontal Network upon Learning},
  author = {Benchenane, Karim and Peyrache, Adrien and Khamassi, Mehdi and Tierney, Patrick L. and Gioanni, Yves and Battaglia, Francesco P. and Wiener, Sidney I.},
  year = {2010},
  month = jun,
  journal = {Neuron},
  volume = {66},
  number = {6},
  pages = {921--936},
  issn = {1097-4199},
  doi = {10.1016/j.neuron.2010.05.013},
  abstract = {To study the interplay between hippocampus and medial prefrontal cortex (Pfc) and its importance for learning and memory consolidation, we measured the coherence in theta oscillations between these two structures in rats learning new rules on a Y maze. Coherence peaked at the choice point, most strongly after task rule acquisition. Simultaneously, Pfc pyramidal neurons reorganized their phase, concentrating at hippocampal theta trough, and synchronous cell assemblies emerged. This synchronous state may result from increased inhibition exerted by interneurons on pyramidal cells, as measured by cross-correlation, and could be modulated by dopamine: we found similar hippocampal-Pfc theta coherence increases and neuronal phase shifts following local administration of dopamine in Pfc of anesthetized rats. Pfc cell assemblies emerging during high coherence were preferentially replayed during subsequent sleep, concurrent with hippocampal sharp waves. Thus, hippocampal/prefrontal coherence could lead to synchronization of reward predicting activity in prefrontal networks, tagging it for subsequent memory consolidation.},
  langid = {english},
  pmid = {20620877},
  keywords = {_tablet,Action Potentials,Animals,Behavior; Animal,Dopamine,Hippocampus,Male,Maze Learning,Neural Inhibition,Neural Pathways,Neurons,Periodicity,Prefrontal Cortex,Principal Component Analysis,Rats,Rats; Long-Evans,Reward,Spectrum Analysis,Time Factors},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BenchenaneK/benchenane_2010_coherent_theta_oscillations_and_reorganization_of_spike_timing_in_the.pdf}
}

@article{bieszczadRepresentationalGainCortical2010,
  title = {Representational Gain in Cortical Area Underlies Increase of Memory Strength},
  author = {Bieszczad, Kasia M. and Weinberger, Norman M.},
  year = {2010},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {107},
  number = {8},
  pages = {3793--3798},
  issn = {1091-6490},
  doi = {10.1073/pnas.1000159107},
  abstract = {Neuronal plasticity that develops in the cortex during learning is assumed to represent memory content, but the functions of such plasticity are actually unknown. The shift in spectral tuning in primary auditory cortex (A1) to the frequency of a tone signal is a compelling candidate for a substrate of memory because it has all of the cardinal attributes of associative memory: associativity, specificity, rapid induction, consolidation, and long-term retention. Tuning shifts increase the representational area of the signal in A1, as an increasing function of performance level, suggesting that area encodes the magnitude of acquired stimulus significance. The present study addresses the question of the specific function of learning-induced associative representational plasticity. We tested the hypothesis that specific increases in A1 representational area for an auditory signal serve the mnemonic function of enhancing memory strength for that signal. Rats were trained to bar-press for reward contingent on the presence of a signal tone (5.0 kHz), and assessed for memory strength during extinction. The amount of representational area gain for the signal frequency band was significantly positively correlated with resistance to extinction to the signal frequency in two studies that spanned the range of task difficulty. These findings indicate that specific gain in cortical representational area underlies the strength of the behaviorally-relevant contents of memory. Thus, mnemonic functions of cortical plasticity are determinable.},
  langid = {english},
  pmcid = {PMC2840533},
  pmid = {20133679},
  keywords = {_tablet,Animals,Auditory Cortex,Brain Mapping,Learning,Male,Memory,Neuronal Plasticity,Rats,Rats; Sprague-Dawley},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BieszczadK/bieszczad_2010_representational_gain_in_cortical_area_underlies_increase_of_memory_strength.pdf;/Users/jonny/Zotero/storage/VZBPW2D8/bieszczad_2010_representational_gain_in_cortical_area_underlies_increase_of_memory_strength.pdf}
}

@article{bizleyInterdependentEncodingPitch2009b,
  title = {Interdependent Encoding of Pitch, Timbre, and Spatial Location in Auditory Cortex},
  author = {Bizley, Jennifer K. and Walker, Kerry M. M. and Silverman, Bernard W. and King, Andrew J. and Schnupp, Jan W. H.},
  year = {2009},
  month = feb,
  journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  volume = {29},
  number = {7},
  pages = {2064--2075},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.4755-08.2009},
  abstract = {Because we can perceive the pitch, timbre, and spatial location of a sound source independently, it seems natural to suppose that cortical processing of sounds might separate out spatial from nonspatial attributes. Indeed, recent studies support the existence of anatomically segregated "what" and "where" cortical processing streams. However, few attempts have been made to measure the responses of individual neurons in different cortical fields to sounds that vary simultaneously across spatial and nonspatial dimensions. We recorded responses to artificial vowels presented in virtual acoustic space to investigate the representations of pitch, timbre, and sound source azimuth in both core and belt areas of ferret auditory cortex. A variance decomposition technique was used to quantify the way in which altering each parameter changed neural responses. Most units were sensitive to two or more of these stimulus attributes. Although indicating that neural encoding of pitch, location, and timbre cues is distributed across auditory cortex, significant differences in average neuronal sensitivity were observed across cortical areas and depths, which could form the basis for the segregation of spatial and nonspatial cues at higher cortical levels. Some units exhibited significant nonlinear interactions between particular combinations of pitch, timbre, and azimuth. These interactions were most pronounced for pitch and timbre and were less commonly observed between spatial and nonspatial attributes. Such nonlinearities were most prevalent in primary auditory cortex, although they tended to be small compared with stimulus main effects.},
  langid = {english},
  pmcid = {PMC2663390},
  pmid = {19228960},
  keywords = {_tablet,Acoustic Stimulation,Action Potentials,Animals,Auditory Cortex,Auditory Pathways,Brain Mapping,Electrophysiology,Female,Ferrets,Nerve Net,Neurons,Nonlinear Dynamics,Orientation,Pitch Perception,Signal Processing; Computer-Assisted,Sound Localization,Space Perception},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BizleyJ/bizley_2009_interdependent_encoding_of_pitch,_timbre,_and_spatial_location_in_auditory.pdf;/Users/jonny/Zotero/storage/LUGDH9GT/bizley_2009_interdependent_encoding_of_pitch,_timbre,_and_spatial_location_in_auditory.pdf}
}

@article{bizleyWhatWhereHow2013a,
  title = {The What, Where and How of Auditory-Object Perception},
  author = {Bizley, Jennifer K. and Cohen, Yale E.},
  year = {2013},
  month = oct,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {10},
  pages = {693--707},
  issn = {1471-0048},
  doi = {10.1038/nrn3565},
  abstract = {The fundamental perceptual unit in hearing is the 'auditory object'. Similar to visual objects, auditory objects are the computational result of the auditory system's capacity to detect, extract, segregate and group spectrotemporal regularities in the acoustic environment; the multitude of acoustic stimuli around us together form the auditory scene. However, unlike the visual scene, resolving the component objects within the auditory scene crucially depends on their temporal structure. Neural correlates of auditory objects are found throughout the auditory system. However, neural responses do not become correlated with a listener's perceptual reports until the level of the cortex. The roles of different neural structures and the contribution of different cognitive states to the perception of auditory objects are not yet fully understood.},
  copyright = {2013 Nature Publishing Group},
  langid = {english},
  file = {/Users/jonny/Papers/BizleyJ/2013/Bizley_2013_The what, where and how of auditory-object perception3.pdf}
}

@book{bregmanAuditorySceneAnalysis1994,
  title = {Auditory {{Scene Analysis}}: {{The Perceptual Organization}} of {{Sound}}},
  shorttitle = {Auditory {{Scene Analysis}}},
  author = {Bregman, Albert S.},
  year = {1994},
  publisher = {{MIT Press}},
  abstract = {Auditory Scene Analysis addresses the problem of hearing complex auditory environments, using a series of creative analogies to describe the process required of the human auditory system as it analyzes mixtures of sounds to recover descriptions of individual sounds. In a unified and comprehensive way, Bregman establishes a theoretical framework that integrates his findings with an unusually wide range of previous research in psychoacoustics, speech perception, music theory and composition, and computer modeling.},
  googlebooks = {jI8muSpAC5AC},
  isbn = {978-0-262-52195-6},
  langid = {english},
  keywords = {Psychology / Cognitive Psychology \& Cognition}
}

@article{brembsBrainDynamicallyActive2020,
  title = {The Brain as a Dynamically Active Organ},
  author = {Brembs, Bj{\"o}rn},
  year = {2020},
  month = dec,
  journal = {Biochemical and Biophysical Research Communications},
  issn = {0006-291X},
  doi = {10.1016/j.bbrc.2020.12.011},
  abstract = {Nervous systems are typically described as static networks passively responding to external stimuli (i.e., the `sensorimotor hypothesis'). However, for more than a century now, evidence has been accumulating that this passive-static perspective is wrong. Instead, evidence suggests that nervous systems dynamically change their connectivity and actively generate behavior so their owners can achieve goals in the world, some of which involve controlling their sensory feedback. This review provides a brief overview of the different historical perspectives on general brain function and details some select modern examples falsifying the sensorimotor hypothesis.},
  langid = {english},
  keywords = {_tablet,Active-dynamic,Behavior,Cognition,Evolution,Neuroscience,Passive-static},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BrembsB/brembs_2020_the_brain_as_a_dynamically_active_organ.pdf;/Users/jonny/Zotero/storage/HEJS8ZDD/brembs_2020_the_brain_as_a_dynamically_active_organ.pdf}
}

@article{broderickMoreWordsNeurophysiological2020,
  title = {More than {{Words}}: {{Neurophysiological Correlates}} of {{Semantic Dissimilarity Depend}} on {{Comprehension}} of the {{Speech Narrative}}},
  shorttitle = {More than {{Words}}},
  author = {Broderick, Michael P. and Zuk, Nathaniel J. and Anderson, Andrew J. and Lalor, Edmund C.},
  year = {2020},
  month = dec,
  journal = {bioRxiv},
  pages = {2020.12.14.422789},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.14.422789},
  abstract = {{$<$}p{$>$}Speech comprehension relies on the ability to understand the meaning of words within a coherent context. Recent studies have attempted to obtain electrophysiological indices of this process by modelling how brain activity is affected by a word9s semantic dissimilarity to preceding words. While the resulting indices appear robust and are strongly modulated by attention, it remains possible that, rather than capturing the contextual understanding of words, they may actually reflect word-to-word changes in semantic content without the need for a narrative-level understanding on the part of the listener. To test this possibility, we recorded EEG from subjects who listened to speech presented in either its original, narrative form, or after scrambling the word order by varying amounts. This manipulation affected the ability of subjects to comprehend the narrative content of the speech, but not the ability to recognize the individual words. Neural indices of semantic understanding and low-level acoustic processing were derived for each scrambling condition using the temporal response function (TRF) approach. Signatures of semantic processing were observed for conditions where speech was unscrambled or minimally scrambled and subjects were able to understand the speech. The same markers were absent for higher levels of scrambling when speech comprehension dropped below chance. In contrast, word recognition remained high and neural measures related to envelope tracking did not vary significantly across the different scrambling conditions. This supports the previous claim that electrophysiological indices based on the semantic dissimilarity of words to their context reflect a listener9s understanding of those words relative to that context. It also highlights the relative insensitivity of neural measures of low-level speech processing to speech comprehension.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BroderickM/broderick_2020_more_than_words.pdf;/Users/jonny/Zotero/storage/G6MAKP8H/2020.12.14.html}
}

@article{Carbonell2014,
  title = {Speech Is Not Special\ldots{} Again.},
  author = {Carbonell, Kathy M. and Lotto, Andrew J.},
  year = {2014},
  month = jun,
  journal = {Frontiers in psychology},
  volume = {5},
  number = {June},
  pages = {427},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00427},
  pmid = {24917830},
  keywords = {_tablet,as is apparent from,auditory processing,motor theory,multisensory i,multisensory integration,of nearly any research,of speech,or review article,reading the first line,sensorimotor effects on perception,specialness,speech perception,the},
  file = {/Users/jonny/Dropbox/papers/zotero/C/CarbonellK/carbonell_2014_speech_is_not_special…_again.pdf;/Users/jonny/Zotero/storage/XCQ9XMBD/Carbonell, Lotto - 2014 - Speech is not special… again(3).pdf}
}

@article{carrascoEvidenceHierarchicalProcessing2009,
  title = {Evidence for {{Hierarchical Processing}} in {{Cat Auditory Cortex}}: {{Nonreciprocal Influence}} of {{Primary Auditory Cortex}} on the {{Posterior Auditory Field}}},
  shorttitle = {Evidence for {{Hierarchical Processing}} in {{Cat Auditory Cortex}}},
  author = {Carrasco, Andres and Lomber, Stephen G.},
  year = {2009},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {29},
  number = {45},
  pages = {14323--14333},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2905-09.2009},
  abstract = {The auditory cortex of the cat is composed of 13 distinct fields that have been defined on the basis of anatomy, physiology, and behavior. Although an anatomically based hierarchical processing scheme has been proposed in auditory cortex, few functional studies have examined how these areas influence one another. The purpose of the present study was to examine the bidirectional processing contributions between primary auditory cortex (A1) and the nonprimary posterior auditory field (PAF). Multiunit acute recording techniques in eight mature cats were used to measure neuronal responses to tonal stimuli in A1 or PAF while synaptic activity from PAF or A1 was suppressed with reversible cooling deactivation techniques. Specifically, in four animals, electrophysiological recordings in A1 were conducted before, during, and after deactivation of PAF. Similarly, in the other four animals, PAF activity was measured before, during, and after deactivation of A1. The characteristic frequency, bandwidth, and neuronal threshold were calculated at each receptive field collected and the response strength and response latency measures were calculated from cumulative peristimulus time histograms. Two major changes in PAF response properties were observed during A1 deactivation: a decrease in response strength and a reduction in receptive field bandwidths. In comparison, we did not identify any significant changes in A1 neuronal responses during deactivation of PAF neurons. These findings support proposed models of hierarchal processing in cat auditory cortex.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2009 Society for Neuroscience 0270-6474/09/2914323-11\$15.00/0},
  langid = {english},
  pmid = {19906979},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/CarrascoA/carrasco_2009_evidence_for_hierarchical_processing_in_cat_auditory_cortex.pdf;/Users/jonny/Zotero/storage/CF3RLGCN/carrasco_2009_evidence_for_hierarchical_processing_in_cat_auditory_cortex.pdf}
}

@article{carrascoNeuronalActivationTimes2011,
  title = {Neuronal Activation Times to Simple, Complex, and Natural Sounds in Cat Primary and Nonprimary Auditory Cortex},
  author = {Carrasco, Andres and Lomber, Stephen G.},
  year = {2011},
  month = jun,
  journal = {Journal of Neurophysiology},
  volume = {106},
  number = {3},
  pages = {1166--1178},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.00940.2010},
  abstract = {Interactions between living organisms and the environment are commonly regulated by accurate and timely processing of sensory signals. Hence, behavioral response engagement by an organism is typically constrained by the arrival time of sensory information to the brain. While psychophysical response latencies to acoustic information have been investigated, little is known about how variations in neuronal response time relate to sensory signal characteristics. Consequently, the primary objective of the present investigation was to determine the pattern of neuronal activation induced by simple (pure tones), complex (noise bursts and frequency modulated sweeps), and natural (conspecific vocalizations) acoustic signals of different durations in cat auditory cortex. Our analysis revealed three major cortical response characteristics. First, latency measures systematically increase in an antero-dorsal to postero-ventral direction among regions of auditory cortex. Second, complex acoustic stimuli reliably provoke faster neuronal response engagement than simple stimuli. Third, variations in neuronal response time induced by changes in stimulus duration are dependent on acoustic spectral features. Collectively, these results demonstrate that acoustic signals, regardless of complexity, induce a directional pattern of activation in auditory cortex.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/CarrascoA/carrasco_2011_neuronal_activation_times_to_simple,_complex,_and_natural_sounds_in_cat_primary.pdf;/Users/jonny/Zotero/storage/P2A6W7W2/carrasco_2011_neuronal_activation_times_to_simple,_complex,_and_natural_sounds_in_cat_primary.pdf}
}

@article{carruthersEmergenceInvariantRepresentation2015c,
  title = {Emergence of Invariant Representation of Vocalizations in the Auditory Cortex},
  author = {Carruthers, Isaac M. and Laplagne, Diego A. and Jaegle, Andrew and Briguglio, John J. and {Mwilambwe-Tshilobo}, Laetitia and Natan, Ryan G. and Geffen, Maria N.},
  year = {2015},
  month = aug,
  journal = {Journal of Neurophysiology},
  volume = {114},
  number = {5},
  pages = {2726--2740},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.00095.2015},
  abstract = {An essential task of the auditory system is to discriminate between different communication signals, such as vocalizations. In everyday acoustic environments, the auditory system needs to be capable of performing the discrimination under different acoustic distortions of vocalizations. To achieve this, the auditory system is thought to build a representation of vocalizations that is invariant to their basic acoustic transformations. The mechanism by which neuronal populations create such an invariant representation within the auditory cortex is only beginning to be understood. We recorded the responses of populations of neurons in the primary and nonprimary auditory cortex of rats to original and acoustically distorted vocalizations. We found that populations of neurons in the nonprimary auditory cortex exhibited greater invariance in encoding vocalizations over acoustic transformations than neuronal populations in the primary auditory cortex. These findings are consistent with the hypothesis that invariant representations are created gradually through hierarchical transformation within the auditory pathway.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/CarruthersI/carruthers_2015_emergence_of_invariant_representation_of_vocalizations_in_the_auditory_cortex.pdf;/Users/jonny/Zotero/storage/SEMJ3Z4A/carruthers_2015_emergence_of_invariant_representation_of_vocalizations_in_the_auditory_cortex.pdf}
}

@article{Centanni2013,
  title = {Detection and Identification of Speech Sounds Using Cortical Activity Patterns},
  author = {Centanni, T. M. and Sloan, A. M. and Reed, A. C. and Engineer, C. T. and Rennaker, R. L. and Kilgard, M. P.},
  year = {2013},
  journal = {Neuroscience},
  volume = {258},
  pages = {292--306},
  publisher = {{IBRO}},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2013.11.030},
  abstract = {We have developed a classifier capable of locating and identifying speech sounds using activity from rat auditory cortex with an accuracy equivalent to behavioral performance and without the need to specify the onset time of the speech sounds. This classifier can identify speech sounds from a large speech set within 40. ms of stimulus presentation. To compare the temporal limits of the classifier to behavior, we developed a novel task that requires rats to identify individual consonant sounds from a stream of distracter consonants. The classifier successfully predicted the ability of rats to accurately identify speech sounds for syllable presentation rates up to 10 syllables per second (up to 17.9 {$\pm$} 1.5 bits/s), which is comparable to human performance. Our results demonstrate that the spatiotemporal patterns generated in primary auditory cortex can be used to quickly and accurately identify consonant sounds from a continuous speech stream without prior knowledge of the stimulus onset times. Improved understanding of the neural mechanisms that support robust speech processing in difficult listening conditions could improve the identification and treatment of a variety of speech-processing disorders. \textcopyright{} 2013 IBRO.},
  isbn = {0306-4522},
  pmid = {24286757},
  keywords = {Auditory cortex,Classifier,Coding,Rat,Temporal patterns},
  file = {/Users/jonny/Zotero/storage/KQFT5JAN/Centanni et al. - 2013 - Detection and identification of speech sounds using cortical activity patterns(3).pdf}
}

@article{changCategoricalSpeechRepresentation2010b,
  title = {Categorical Speech Representation in Human Superior Temporal Gyrus},
  author = {Chang, Edward F. and Rieger, Jochem W. and Johnson, Keith and Berger, Mitchel S. and Barbaro, Nicholas M. and Knight, Robert T.},
  year = {2010},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {13},
  number = {11},
  pages = {1428--1432},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2641},
  abstract = {A continuum of acoustically varying speech sounds is not perceived as a continuum, but as distinct phonetic categories. Chang et al. recorded directly from human posterior superior temporal gyrus and found that this area has a similarly discontinuous coding of objectively continuous sound, matching perception and indicating higher-level processing.},
  copyright = {2010 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/ChangE/false;/Users/jonny/Zotero/storage/WQVA8X3Q/chang_2010_categorical_speech_representation_in_human_superior_temporal_gyrus.pdf}
}

@article{chanSpeechSpecificTuningNeurons2014,
  title = {Speech-{{Specific Tuning}} of {{Neurons}} in {{Human Superior Temporal Gyrus}}},
  author = {Chan, Alexander M. and Dykstra, Andrew R. and Jayaram, Vinay and Leonard, Matthew K. and Travis, Katherine E. and Gygi, Brian and Baker, Janet M. and Eskandar, Emad and Hochberg, Leigh R. and Halgren, Eric and Cash, Sydney S.},
  year = {2014},
  month = oct,
  journal = {Cerebral Cortex},
  volume = {24},
  number = {10},
  pages = {2679--2693},
  issn = {1047-3211},
  doi = {10.1093/cercor/bht127},
  abstract = {How the brain extracts words from auditory signals is an unanswered question. We recorded approximately 150 single and multi-units from the left anterior superior temporal gyrus of a patient during multiple auditory experiments. Against low background activity, 45\% of units robustly fired to particular spoken words with little or no response to pure tones, noise-vocoded speech, or environmental sounds. Many units were tuned to complex but specific sets of phonemes, which were influenced by local context but invariant to speaker, and suppressed during self-produced speech. The firing of several units to specific visual letters was correlated with their response to the corresponding auditory phonemes, providing the first direct neural evidence for phonological recoding during reading. Maximal decoding of individual phonemes and words identities was attained using firing rates from approximately 5 neurons within 200 ms after word onset. Thus, neurons in human superior temporal gyrus use sparse spatially organized population encoding of complex acoustic\textendash phonetic features to help recognize auditory and visual words.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/ChanA/chan_2014_speech-specific_tuning_of_neurons_in_human_superior_temporal_gyrus.pdf;/Users/jonny/Zotero/storage/52JFVHNV/chan_2014_speech-specific_tuning_of_neurons_in_human_superior_temporal_gyrus.pdf}
}

@article{chaudhariAttentiveSurveyAttention2020,
  title = {An {{Attentive Survey}} of {{Attention Models}}},
  author = {Chaudhari, Sneha and Mithal, Varun and Polatkan, Gungor and Ramanath, Rohan},
  year = {2020},
  month = dec,
  journal = {arXiv:1904.02874 [cs, stat]},
  eprint = {1904.02874},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy which groups existing techniques into coherent categories. We review salient neural architectures in which attention has been incorporated, and discuss applications in which modeling attention has shown a significant impact. Finally, we also describe how attention has been used to improve the interpretability of neural networks. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jonny/Dropbox/papers/zotero/C/ChaudhariS/chaudhari_2020_an_attentive_survey_of_attention_models.pdf}
}

@article{clayardsDifferencesCueWeights2018,
  title = {Differences in Cue Weights for Speech Perception Are Correlated for Individuals within and across Contrasts},
  author = {Clayards, Meghan},
  year = {2018},
  month = sep,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {3},
  pages = {EL172-EL177},
  issn = {0001-4966},
  doi = {10.1121/1.5052025},
  abstract = {Speech perception requires multiple acoustic cues. Cue weighting may differ across individuals but be systematic within individuals. The current study compared individuals' cue weights within and across contrasts. Forty-two listeners performed a two-alternative forced choice task for four out of five sets of minimal pairs, each varying orthogonally in two dimensions. Individuals' cue weights within a contrast were positively correlated for bet-bat, Luce-lose, and sock-shock, but not for bog-dog and dear-tear. Importantly, individuals' cue weights were also positively correlated across contrasts. This indicates that some individuals are better able to extract and use phonetic information across different dimensions.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/ClayardsM/clayards_2018_differences_in_cue_weights_for_speech_perception_are_correlated_for_individuals.pdf;/Users/jonny/Zotero/storage/8FAQBUMB/Clayards - 2018 - Differences in cue weights for speech perception a.pdf;/Users/jonny/Zotero/storage/KTIAW3S9/1.html}
}

@incollection{clementsFeatureOrganization2006,
  title = {Feature {{Organization}}},
  booktitle = {Encyclopedia of {{Language}} \& {{Linguistics}}},
  author = {Clements, G.N.},
  year = {2006},
  pages = {433--440},
  publisher = {{Elsevier}},
  doi = {10.1016/B0-08-044854-2/00055-9},
  abstract = {Feature organization involves the study of the internal composition of` speech sounds. This article reviews linguistic approaches to this topic dating from the mid-20th century with a special emphasis on the theory known as Feature Geometry. This theory, in its simplest and most general form, characterizes segment-internal feature structure in terms of a feature tree whose terminal nodes are features, whose intermediate nodes are feature classes, and whose root node groups all features defining the segment. The principle objective of this approach is to provide a formal characterization of the class of possible phonological processes. More recent developments and extensions of this approach are examined, with special emphasis on applications to phonetics, such as Articulatory Phonology.},
  isbn = {978-0-08-044854-1},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Zotero/storage/TRPC25HS/clements_2006_feature_organization.pdf}
}

@article{clementsInternalOrganizationSpeech,
  title = {The Internal Organization of Speech Sounds},
  author = {Clements, George N. and Hume, Elizabeth V.},
  langid = {english}
}

@article{cohenSeparabilityGeometryObject2020,
  title = {Separability and Geometry of Object Manifolds in Deep Neural Networks},
  author = {Cohen, Uri and Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
  year = {2020},
  month = feb,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {746},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-14578-5},
  abstract = {Stimuli are represented in the brain by the collective population responses of sensory neurons, and an object presented under varying conditions gives rise to a collection of neural population responses called an `object manifold'. Changes in the object representation along a hierarchical sensory system are associated with changes in the geometry of those manifolds, and recent theoretical progress connects this geometry with `classification capacity', a quantitative measure of the ability to support object classification. Deep neural networks trained on object classification tasks are a natural testbed for the applicability of this relation. We show how classification capacity improves along the hierarchies of deep neural networks with different architectures. We demonstrate that changes in the geometry of the associated object manifolds underlie this improved capacity, and shed light on the functional roles different levels in the hierarchy play to achieve it, through orchestrated reduction of manifolds' radius, dimensionality and inter-manifold correlations.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/CohenU/cohen_2020_separability_and_geometry_of_object_manifolds_in_deep_neural_networks.pdf;/Users/jonny/Zotero/storage/SDJN6ZAU/cohen_2020_separability_and_geometry_of_object_manifolds_in_deep_neural_networks.pdf}
}

@article{cookStagesAbstractionExemplar2006,
  title = {Stages of Abstraction and Exemplar Memorization in Pigeon Category Learning},
  author = {Cook, Robert G. and Smith, J. David},
  year = {2006},
  month = dec,
  journal = {Psychological Science},
  volume = {17},
  number = {12},
  pages = {1059--1067},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.2006.01833.x},
  abstract = {It has been proposed that human category learning consists of an early abstraction-based stage followed by a later exemplar-memorization stage. To investigate whether similar processing stages extend to category learning in a nonverbal species, we applied a prototype-exception paradigm to investigating pigeon category learning. Five birds and 8 humans learned six-dimensional perceptual categories constructed to include prototypes, typical items, and exceptions. We evaluated the birds' and humans' categorization strategies at different points during learning. Early on in both species, prototype performance improved rapidly as exception performance remained below chance, indicating an initial mastery of the categories' general structure. Later on, exception performance improved selectively and dramatically, indicating exception-item resolution and exemplar memorization. Abstraction- and exemplar-based formal models reinforced these interpretations. The results suggest a psychological transition in pigeon category learning from abstraction- to exemplar-based processing similar to that found in humans.},
  langid = {english},
  pmid = {17201788},
  keywords = {_tablet,Animals,Association Learning,Cognition,Columbidae,Concept Formation,Humans,Learning,Male,Memory,Models; Psychological,Set; Psychology,Students,Task Performance and Analysis},
  file = {/Users/jonny/Dropbox/papers/zotero/C/CookR/cook_2006_stages_of_abstraction_and_exemplar_memorization_in_pigeon_category_learning.pdf}
}

@article{cooperInterconversionAudibleVisible1951,
  title = {The {{Interconversion}} of {{Audible}} and {{Visible Patterns}} as a {{Basis}} for {{Research}} in the {{Perception}} of {{Speech}}},
  author = {Cooper, Franklin S. and Liberman, Alvin M. and Borst, John M.},
  year = {1951},
  month = may,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {37},
  number = {5},
  pages = {318--325},
  issn = {0027-8424},
  pmcid = {PMC1063363},
  pmid = {14834156},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/CooperF/cooper_1951_the_interconversion_of_audible_and_visible_patterns_as_a_basis_for_research_in.pdf}
}

@article{couchmanRulesResemblanceTheir2010,
  title = {Rules and {{Resemblance}}: {{Their Changing Balance}} in the {{Category Learning}} of {{Humans}} ({{Homo}} Sapiens) and {{Monkeys}} ({{Macaca}} Mulatta)},
  shorttitle = {Rules and {{Resemblance}}},
  author = {Couchman, Justin J. and Coutinho, Mariana V. C. and Smith, J. David},
  year = {2010},
  month = apr,
  journal = {Journal of experimental psychology. Animal behavior processes},
  volume = {36},
  number = {2},
  pages = {172--183},
  issn = {0097-7403},
  doi = {10.1037/a0016748},
  abstract = {In an early dissociation between intentional and incidental category learning,  gave participants a categorization task that could be performed by responding either to a single-dimensional rule or to overall family resemblance. Humans learning intentionally deliberately adopted rule-based strategies; humans learning incidentally adopted family-resemblance strategies. The present authors replicated Kemler Nelson's human experiment and found a similar dissociation. They also extended her paradigm so as to evaluate the balance between rules and family-resemblance in determining the category decisions of rhesus monkeys. Monkeys heavily favored the family-resemblance strategy. Formal models showed that even after many sessions and thousands of trials, they spread attention across all stimulus dimensions rather than focus on a single, criterial dimension that could also produce perfect categorization.},
  pmcid = {PMC2890302},
  pmid = {20384398},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/CouchmanJ/couchman_2010_rules_and_resemblance.pdf}
}

@article{davidIntegrationMultipleTimescales2013,
  title = {Integration over Multiple Timescales in Primary Auditory Cortex},
  author = {David, Stephen V. and Shamma, Shihab A.},
  year = {2013},
  month = dec,
  journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  volume = {33},
  number = {49},
  pages = {19154--19166},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.2270-13.2013},
  abstract = {Speech and other natural vocalizations are characterized by large modulations in their sound envelope. The timing of these modulations contains critical information for discrimination of important features, such as phonemes. We studied how depression of synaptic inputs, a mechanism frequently reported in cortex, can contribute to the encoding of envelope dynamics. Using a nonlinear stimulus-response model that accounted for synaptic depression, we predicted responses of neurons in ferret primary auditory cortex (A1) to stimuli with natural temporal modulations. The depression model consistently performed better than linear and second-order models previously used to characterize A1 neurons, and it produced more biologically plausible fits. To test how synaptic depression can contribute to temporal stimulus integration, we used nonparametric maximum a posteriori decoding to compare the ability of neurons showing and not showing depression to reconstruct the stimulus envelope. Neurons showing evidence for depression reconstructed stimuli over a longer range of latencies. These findings suggest that variation in depression across the cortical population supports a rich code for representing the temporal dynamics of natural sounds.},
  langid = {english},
  pmcid = {PMC3850039},
  pmid = {24305812},
  keywords = {_tablet,Acoustic Stimulation,Algorithms,Animals,Auditory Cortex,Auditory Perception,Craniotomy,Data Interpretation; Statistical,Electrophysiological Phenomena,Female,Ferrets,Models; Neurological,Neurons,Nonlinear Dynamics,Synapses,Vocalization; Animal},
  file = {/Users/jonny/Dropbox/papers/zotero/D/DavidS/david_2013_integration_over_multiple_timescales_in_primary_auditory_cortex.pdf;/Users/jonny/Zotero/storage/DM64MB8W/david_2013_integration_over_multiple_timescales_in_primary_auditory_cortex.pdf}
}

@article{davidRapidSynapticDepression2009,
  title = {Rapid Synaptic Depression Explains Nonlinear Modulation of Spectro-Temporal Tuning in Primary Auditory Cortex by Natural Stimuli},
  author = {David, Stephen V. and Mesgarani, Nima and Fritz, Jonathan B. and Shamma, Shihab A.},
  year = {2009},
  month = mar,
  journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  volume = {29},
  number = {11},
  pages = {3374--3386},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.5249-08.2009},
  abstract = {In this study, we explored ways to account more accurately for responses of neurons in primary auditory cortex (A1) to natural sounds. The auditory cortex has evolved to extract behaviorally relevant information from complex natural sounds, but most of our understanding of its function is derived from experiments using simple synthetic stimuli. Previous neurophysiological studies have found that existing models, such as the linear spectro-temporal receptive field (STRF), fail to capture the entire functional relationship between natural stimuli and neural responses. To study this problem, we compared STRFs for A1 neurons estimated using a natural stimulus, continuous speech, with STRFs estimated using synthetic ripple noise. For about one-third of the neurons, we found significant differences between STRFs, usually in the temporal dynamics of inhibition and/or overall gain. This shift in tuning resulted primarily from differences in the coarse temporal structure of the speech and noise stimuli. Using simulations, we found that the stimulus dependence of spectro-temporal tuning can be explained by a model in which synaptic inputs to A1 neurons are susceptible to rapid nonlinear depression. This dynamic reshaping of spectro-temporal tuning suggests that synaptic depression may enable efficient encoding of natural auditory stimuli.},
  langid = {english},
  pmcid = {PMC2774136},
  pmid = {19295144},
  keywords = {_tablet,Acoustic Stimulation,Action Potentials,Animals,Auditory Cortex,Auditory Pathways,Auditory Perception,Female,Ferrets,Humans,Long-Term Synaptic Depression,Male,Neuronal Plasticity,Synapses,Time Factors},
  file = {/Users/jonny/Dropbox/papers/zotero/D/DavidS/david_2009_rapid_synaptic_depression_explains_nonlinear_modulation_of_spectro-temporal.pdf;/Users/jonny/Zotero/storage/UZHG7APL/david_2009_rapid_synaptic_depression_explains_nonlinear_modulation_of_spectro-temporal.pdf}
}

@article{davidTaskRewardStructure2012,
  title = {Task Reward Structure Shapes Rapid Receptive Field Plasticity in Auditory Cortex},
  author = {David, Stephen V. and Fritz, Jonathan B. and Shamma, Shihab A.},
  year = {2012},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {109},
  number = {6},
  pages = {2144--2149},
  issn = {1091-6490},
  doi = {10.1073/pnas.1117717109},
  abstract = {As sensory stimuli and behavioral demands change, the attentive brain quickly identifies task-relevant stimuli and associates them with appropriate motor responses. The effects of attention on sensory processing vary across task paradigms, suggesting that the brain may use multiple strategies and mechanisms to highlight attended stimuli and link them to motor action. To better understand factors that contribute to these variable effects, we studied sensory representations in primary auditory cortex (A1) during two instrumental tasks that shared the same auditory discrimination but required different behavioral responses, either approach or avoidance. In the approach task, ferrets were rewarded for licking a spout when they heard a target tone amid a sequence of reference noise sounds. In the avoidance task, they were punished unless they inhibited licking to the target. To explore how these changes in task reward structure influenced attention-driven rapid plasticity in A1, we measured changes in sensory neural responses during behavior. Responses to the target changed selectively during both tasks but did so with opposite sign. Despite the differences in sign, both effects were consistent with a general neural coding strategy that maximizes discriminability between sound classes. The dependence of the direction of plasticity on task suggests that representations in A1 change not only to sharpen representations of task-relevant stimuli but also to amplify responses to stimuli that signal aversive outcomes and lead to behavioral inhibition. Thus, top-down control of sensory processing can be shaped by task reward structure in addition to the required sensory discrimination.},
  langid = {english},
  pmcid = {PMC3277538},
  pmid = {22308415},
  keywords = {_tablet,Acoustic Stimulation,Animals,Auditory Cortex,Avoidance Learning,Behavior; Animal,Ferrets,Neuronal Plasticity,Reward,Task Performance and Analysis,Time Factors},
  file = {/Users/jonny/Dropbox/papers/zotero/D/DavidS/david_2012_task_reward_structure_shapes_rapid_receptive_field_plasticity_in_auditory_cortex.pdf;/Users/jonny/Zotero/storage/W8JVPSBR/david_2012_task_reward_structure_shapes_rapid_receptive_field_plasticity_in_auditory_cortex.pdf}
}

@article{davisLexicalInformationDrives2005,
  title = {Lexical Information Drives Perceptual Learning of Distorted Speech: Evidence from the Comprehension of Noise-Vocoded Sentences},
  shorttitle = {Lexical Information Drives Perceptual Learning of Distorted Speech},
  author = {Davis, Matthew H. and Johnsrude, Ingrid S. and {Hervais-Adelman}, Alexis and Taylor, Karen and McGettigan, Carolyn},
  year = {2005},
  month = may,
  journal = {Journal of Experimental Psychology. General},
  volume = {134},
  number = {2},
  pages = {222--241},
  issn = {0096-3445},
  doi = {10.1037/0096-3445.134.2.222},
  abstract = {Speech comprehension is resistant to acoustic distortion in the input, reflecting listeners' ability to adjust perceptual processes to match the speech input. For noise-vocoded sentences, a manipulation that removes spectral detail from speech, listeners' reporting improved from near 0\% to 70\% correct over 30 sentences (Experiment 1). Learning was enhanced if listeners heard distorted sentences while they knew the identity of the undistorted target (Experiments 2 and 3). Learning was absent when listeners were trained with nonword sentences (Experiments 4 and 5), although the meaning of the training sentences did not affect learning (Experiment 5). Perceptual learning of noise-vocoded speech depends on higher level information, consistent with top-down, lexically driven learning. Similar processes may facilitate comprehension of speech in an unfamiliar accent or following cochlear implantation.},
  langid = {english},
  pmid = {15869347},
  keywords = {_tablet,Adolescent,Adult,Female,Humans,Learning,Male,Noise,Perception,Sound Spectrography,Speech Perception,Vocabulary},
  file = {/Users/jonny/Dropbox/papers/zotero/D/DavisM/davis_2005_lexical_information_drives_perceptual_learning_of_distorted_speech.pdf;/Users/jonny/Zotero/storage/A7KNXLBX/davis_2005_lexical_information_drives_perceptual_learning_of_distorted_speech.pdf}
}

@article{deanRapidNeuralAdaptation2008b,
  title = {Rapid {{Neural Adaptation}} to {{Sound Level Statistics}}},
  author = {Dean, Isabel and Robinson, Ben L. and Harper, Nicol S. and McAlpine, David},
  year = {2008},
  month = jun,
  journal = {Journal of Neuroscience},
  volume = {28},
  number = {25},
  pages = {6430--6438},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0470-08.2008},
  abstract = {Auditory neurons must represent accurately a wide range of sound levels using firing rates that vary over a far narrower range of levels. Recently, we demonstrated that this ``dynamic range problem'' is lessened by neural adaptation, whereby neurons adjust their input\textendash output functions for sound level according to the prevailing distribution of levels. These adjustments in input\textendash output functions increase the accuracy with which levels around those occurring most commonly are coded by the neural population. Here, we examine how quickly this adaptation occurs. We recorded from single neurons in the auditory midbrain during a stimulus that switched repeatedly between two distributions of sound levels differing in mean level. The high-resolution analysis afforded by this stimulus showed that a prominent component of the adaptation occurs rapidly, with an average time constant across neurons of 160 ms after an increase in mean level, much faster than our previous experiments were able to assess. This time course appears to be independent of both the timescale over which sound levels varied and that over which sound level distributions varied, but is related to neural characteristic frequency. We find that adaptation to an increase in mean level occurs more rapidly than to a decrease. Finally, we observe an additional, slow adaptation in some neurons, which occurs over a timescale of tens of seconds. Our findings provide constraints in the search for mechanisms underlying adaptation to sound level. They also have functional implications for the role of adaptation in the representation of natural sounds.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2008 Society for Neuroscience 0270-6474/08/286430-09\$15.00/0},
  langid = {english},
  pmid = {18562614},
  keywords = {_tablet,adaptation,auditory neuron,inferior colliculus,rate level function,sound level,time course},
  file = {/Users/jonny/Dropbox/papers/zotero/D/DeanI/dean_2008_rapid_neural_adaptation_to_sound_level_statistics.pdf;/Users/jonny/Zotero/storage/QECNIGRM/dean_2008_rapid_neural_adaptation_to_sound_level_statistics.pdf}
}

@article{decharmsOptimizingSoundFeatures1998,
  title = {Optimizing Sound Features for Cortical Neurons},
  author = {{deCharms}, R. C. and Blake, D. T. and Merzenich, M. M.},
  year = {1998},
  month = may,
  journal = {Science (New York, N.Y.)},
  volume = {280},
  number = {5368},
  pages = {1439--1443},
  issn = {0036-8075},
  doi = {10.1126/science.280.5368.1439},
  abstract = {The brain's cerebral cortex decomposes visual images into information about oriented edges, direction and velocity information, and color. How does the cortex decompose perceived sounds? A reverse correlation technique demonstrates that neurons in the primary auditory cortex of the awake primate have complex patterns of sound-feature selectivity that indicate sensitivity to stimulus edges in frequency or in time, stimulus transitions in frequency or intensity, and feature conjunctions. This allows the creation of classes of stimuli matched to the processing characteristics of auditory cortical neurons. Stimuli designed for a particular neuron's preferred feature pattern can drive that neuron with higher sustained firing rates than have typically been recorded with simple stimuli. These data suggest that the cortex decomposes an auditory scene into component parts using a feature-processing system reminiscent of that used for the cortical decomposition of visual images.},
  langid = {english},
  pmid = {9603734},
  keywords = {_tablet,Acoustic Stimulation,Action Potentials,Animals,Aotidae,Auditory Cortex,Brain Mapping,Evoked Potentials; Auditory,Microelectrodes,Neurons,Sound},
  file = {/Users/jonny/Dropbox/papers/zotero/D/deCharmsR/decharms_1998_optimizing_sound_features_for_cortical_neurons.pdf;/Users/jonny/Zotero/storage/5JD7FRUR/decharms_1998_optimizing_sound_features_for_cortical_neurons.pdf}
}

@article{Dresher2008,
  title = {The Contrastive Hierarchy in Phonology},
  author = {Dresher, B Elan},
  year = {2008},
  journal = {Contrast in phonology: theory, perception, acquisition},
  volume = {13},
  pages = {11},
  issn = {1718-3510},
  doi = {10.1017/CBO9780511642005},
  abstract = {I will show that phonologists have vacillated between two different and incompatible approaches to determining whether a feature is contrastive in any particular phoneme. One approach involves extracting contrastive features from fully-specified minimal pairs. I will show that this approach is provably untenable. A second approach arrives at contrastive specifications by ordering features into a hierarchy, and splitting up the inventory by successive divisions until all phonemes have been distinguished. I will show that this hierarchical approach solves the problems encountered by the minimal-pairs method. Moreover, a hierarchical approach to contrastiveness is implicit in much descriptive phonological practice, and can be found even in the work of theorists who argue against it. Given the centrality of the issue, it is remarkable that it has received almost no attention in the literature. Recovering this missing chapter of phonological theory sheds new light on a number of controversies over contrast in phonology.},
  isbn = {9780521889735},
  keywords = {_tablet,\#nosource},
  file = {/Users/jonny/Dropbox/papers/zotero/D/DresherB/dresher_2008_the_contrastive_hierarchy_in_phonology.pdf}
}

@article{dubreuilComplementaryRolesDimensionality2020,
  title = {Complementary Roles of Dimensionality and Population Structure in Neural Computations},
  author = {Dubreuil, Alexis and Valente, Adrian and Beiran, Manuel and Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2020},
  month = jul,
  journal = {bioRxiv},
  pages = {2020.07.03.185942},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.07.03.185942},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Neural computations are currently investigated using two competing approaches: sorting neurons into functional classes, or examining the low-dimensional dynamics of collective activity. Whether and how these two aspects interact to shape computations is currently unclear. Using a novel approach to extract computational mechanisms from networks trained with machine-learning tools on neuroscience tasks, here we show that the dimensionality of the dynamics and cell-class structure play fundamentally complementary roles. While various tasks can be implemented by increasing the dimensionality in networks consisting of a single global population, flexible input-output mappings instead required networks to be organized into several sub-populations. Our analyses revealed that the subpopulation structure enabled flexible computations through a mechanism based on gain-controlled modulations that flexibly shape the dynamical landscape of collective dynamics. Our results lead to task-specific predictions for the structure of neural selectivity and inactivation experiments.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/D/DubreuilA/dubreuil_2020_complementary_roles_of_dimensionality_and_population_structure_in_neural.pdf;/Users/jonny/Zotero/storage/VKL6ZT95/2020.07.03.html}
}

@article{durstewitzAbruptTransitionsPrefrontal2010,
  title = {Abrupt {{Transitions}} between {{Prefrontal Neural Ensemble States Accompany Behavioral Transitions}} during {{Rule Learning}}},
  author = {Durstewitz, Daniel and Vittoz, Nicole M. and Floresco, Stan B. and Seamans, Jeremy K.},
  year = {2010},
  month = may,
  journal = {Neuron},
  volume = {66},
  number = {3},
  pages = {438--448},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2010.03.029},
  abstract = {One of the most intriguing aspects of adaptive behavior involves the inference of regularities and rules in ever-changing environments. Rules are often deduced through evidence-based learning which relies on the prefrontal cortex (PFC). This is a highly dynamic process, evolving trial by trial and therefore may not be adequately captured by averaging single-unit responses over numerous repetitions. Here, we employed advanced statistical techniques to visualize the trajectories of ensembles of simultaneously recorded medial PFC neurons on a trial-by-trial basis as rats deduced a novel rule in a set-shifting task. Neural populations formed clearly distinct and lasting representations of familiar and novel rules by entering unique network states. During rule acquisition, the recorded ensembles often exhibited abrupt transitions, rather than evolving continuously, in tight temporal relation to behavioral performance shifts. These results support the idea that rule learning is an evidence-based decision process, perhaps accompanied by moments of sudden insight.},
  langid = {english},
  keywords = {_tablet,SYSNEURO},
  file = {/Users/jonny/Dropbox/papers/zotero/D/DurstewitzD/durstewitz_2010_abrupt_transitions_between_prefrontal_neural_ensemble_states_accompany.pdf;/Users/jonny/Zotero/storage/4GDHC9PV/S0896627310002321.html}
}

@article{Edelman1995,
  title = {Representation of Similarity in Three-Dimensional Object Discrimination.},
  author = {Edelman, S},
  year = {1995},
  month = mar,
  journal = {Neural computation},
  volume = {7},
  number = {2},
  pages = {408--23},
  issn = {0899-7667},
  abstract = {How does the brain represent visual objects? In simple perceptual generalization tasks, the human visual system performs as if it represents the stimuli in a low-dimensional metric psychological space (Shepard 1987). In theories of three-dimensional (3D) shape recognition, the role of feature-space representations [as opposed to structural (Biederman 1987) or pictorial (Ullman 1989) descriptions] has long been a major point of contention. If shapes are indeed represented as points in a feature space, patterns of perceived similarity among different objects must reflect the structure of this space. The feature space hypothesis can then be tested by presenting subjects with complex parameterized 3D shapes, and by relating the similarities among subjective representations, as revealed in the response data by multidimensional scaling (Shepard 1980), to the objective parameterization of the stimuli. The results of four such tests, accompanied by computational simulations, support the notion that discrimination among 3D objects may rely on a low-dimensional feature space representation, and suggest that this space may be spanned by explicitly encoded class prototypes.},
  pmid = {8974734},
  keywords = {_tablet,\#nosource},
  file = {/Users/jonny/Dropbox/papers/zotero/E/EdelmanS/edelman_1995_representation_of_similarity_in_three-dimensional_object_discrimination.pdf}
}

@article{Edelman1998,
  title = {Representation Is Representation of Similarities.},
  author = {Edelman, S},
  year = {1998},
  month = aug,
  journal = {The Behavioral and brain sciences},
  volume = {21},
  number = {4},
  pages = {449-67; discussion 467-98},
  issn = {0140-525X},
  abstract = {Advanced perceptual systems are faced with the problem of securing a principled (ideally, veridical) relationship between the world and its internal representation. I propose a unified approach to visual representation, addressing the need for superordinate and basic-level categorization and for the identification of specific instances of familiar categories. According to the proposed theory, a shape is represented internally by the responses of a small number of tuned modules, each broadly selective for some reference shape, whose similarity to the stimulus it measures. This amounts to embedding the stimulus in a low-dimensional proximal shape space spanned by the outputs of the active modules. This shape space supports representations of distal shape similarities that are veridical as Shepard's (1968) second-order isomorphisms (i.e., correspondence between distal and proximal similarities among shapes, rather than between distal shapes and their proximal representations). Representation in terms of similarities to reference shapes supports processing (e.g., discrimination) of shapes that are radically different from the reference ones, without the need for the computationally problematic decomposition into parts required by other theories. Furthermore, a general expression for similarity between two stimuli, based on comparisons to reference shapes, can be used to derive models of perceived similarity ranging from continuous, symmetric, and hierarchical ones, as in multidimensional scaling (Shepard 1980), to discrete and nonhierarchical ones, as in the general contrast models (Shepard \& Arabie 1979; Tversky 1977).},
  pmid = {10097019},
  keywords = {\#nosource}
}

@article{elliottModulationTransferFunction2009a,
  title = {The {{Modulation Transfer Function}} for {{Speech Intelligibility}}},
  author = {Elliott, Taffeta M. and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {2009},
  month = mar,
  journal = {PLOS Computational Biology},
  volume = {5},
  number = {3},
  pages = {e1000302},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000302},
  abstract = {We systematically determined which spectrotemporal modulations in speech are necessary for comprehension by human listeners. Speech comprehension has been shown to be robust to spectral and temporal degradations, but the specific relevance of particular degradations is arguable due to the complexity of the joint spectral and temporal information in the speech signal. We applied a novel modulation filtering technique to recorded sentences to restrict acoustic information quantitatively and to obtain a joint spectrotemporal modulation transfer function for speech comprehension, the speech MTF. For American English, the speech MTF showed the criticality of low modulation frequencies in both time and frequency. Comprehension was significantly impaired when temporal modulations {$<$}12 Hz or spectral modulations {$<$}4 cycles/kHz were removed. More specifically, the MTF was bandpass in temporal modulations and low-pass in spectral modulations: temporal modulations from 1 to 7 Hz and spectral modulations {$<$}1 cycles/kHz were the most important. We evaluated the importance of spectrotemporal modulations for vocal gender identification and found a different region of interest: removing spectral modulations between 3 and 7 cycles/kHz significantly increases gender misidentifications of female speakers. The determination of the speech MTF furnishes an additional method for producing speech signals with reduced bandwidth but high intelligibility. Such compression could be used for audio applications such as file compression or noise removal and for clinical applications such as signal processing for cochlear implants.},
  langid = {english},
  keywords = {_tablet,Acoustic signals,Audio signal processing,Bioacoustics,Modulation,Signal bandwidth,Signal filtering,Speech,Speech signal processing},
  file = {/Users/jonny/Dropbox/papers/zotero/E/ElliottT/elliott_2009_the_modulation_transfer_function_for_speech_intelligibility.pdf;/Users/jonny/Zotero/storage/CIM5MGDV/elliott_2009_the_modulation_transfer_function_for_speech_intelligibility.pdf}
}

@article{engineerSpeechTrainingAlters2014a,
  title = {Speech Training Alters Tone Frequency Tuning in Rat Primary Auditory Cortex},
  author = {Engineer, Crystal T. and Perez, Claudia A. and Carraway, Ryan S. and Chang, Kevin Q. and Roland, Jarod L. and Kilgard, Michael P.},
  year = {2014},
  month = jan,
  journal = {Behavioural brain research},
  volume = {258},
  pages = {166--178},
  issn = {0166-4328},
  doi = {10.1016/j.bbr.2013.10.021},
  abstract = {Previous studies in both humans and animals have documented improved performance following discrimination training. This enhanced performance is often associated with cortical response changes. In this study, we tested the hypothesis that long-term speech training on multiple tasks can improve primary auditory cortex (A1) responses compared to rats trained on a single speech discrimination task or experimentally na\"ive rats. Specifically, we compared the percent of A1 responding to trained sounds, the responses to both trained and untrained sounds, receptive field properties of A1 neurons, and the neural discrimination of pairs of speech sounds in speech trained and na\"ive rats. Speech training led to accurate discrimination of consonant and vowel sounds, but did not enhance A1 response strength or the neural discrimination of these sounds. Speech training altered tone responses in rats trained on six speech discrimination tasks but not in rats trained on a single speech discrimination task. Extensive speech training resulted in broader frequency tuning, shorter onset latencies, a decreased driven response to tones, and caused a shift in the frequency map to favor tones in the range where speech sounds are the loudest. Both the number of trained tasks and the number of days of training strongly predict the percent of A1 responding to a low frequency tone. Rats trained on a single speech discrimination task performed less accurately than rats trained on multiple tasks and did not exhibit A1 response changes. Our results indicate that extensive speech training can reorganize the A1 frequency map, which may have downstream consequences on speech sound processing.},
  pmcid = {PMC3886187; https://web.archive.org/web/20211013212555/https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3886187/},
  pmid = {24344364},
  keywords = {archived},
  file = {/Users/jonny/Dropbox/papers/zotero/E/EngineerC/engineer_2014_speech_training_alters_tone_frequency_tuning_in_rat_primary_auditory_cortex.pdf}
}

@article{engineerSpeechTrainingAlters2015a,
  title = {Speech Training Alters Consonant and Vowel Responses in Multiple Auditory Cortex Fields},
  author = {Engineer, Crystal T. and Rahebi, Kimiya C. and Buell, Elizabeth P. and Fink, Melyssa K. and Kilgard, Michael P.},
  year = {2015},
  month = jul,
  journal = {Behavioural Brain Research},
  volume = {287},
  pages = {256--264},
  issn = {0166-4328},
  doi = {10.1016/j.bbr.2015.03.044},
  abstract = {Speech sounds evoke unique neural activity patterns in primary auditory cortex (A1). Extensive speech sound discrimination training alters A1 responses. While the neighboring auditory cortical fields each contain information about speech sound identity, each field processes speech sounds differently. We hypothesized that while all fields would exhibit training-induced plasticity following speech training, there would be unique differences in how each field changes. In this study, rats were trained to discriminate speech sounds by consonant or vowel in quiet and in varying levels of background speech-shaped noise. Local field potential and multiunit responses were recorded from four auditory cortex fields in rats that had received 10 weeks of speech discrimination training. Our results reveal that training alters speech evoked responses in each of the auditory fields tested. The neural response to consonants was significantly stronger in anterior auditory field (AAF) and A1 following speech training. The neural response to vowels following speech training was significantly weaker in ventral auditory field (VAF) and posterior auditory field (PAF). This differential plasticity of consonant and vowel sound responses may result from the greater paired pulse depression, expanded low frequency tuning, reduced frequency selectivity, and lower tone thresholds, which occurred across the four auditory fields. These findings suggest that alterations in the distributed processing of behaviorally relevant sounds may contribute to robust speech discrimination.},
  langid = {english},
  keywords = {_tablet,Auditory processing,Map reorganization,Receptive field plasticity,Speech therapy},
  file = {/Users/jonny/Dropbox/papers/zotero/E/EngineerC/false;/Users/jonny/Zotero/storage/B4QI36P9/engineer_2015_speech_training_alters_consonant_and_vowel_responses_in_multiple_auditory.pdf}
}

@article{evansMythLanguageUniversals2009,
  title = {The Myth of Language Universals: {{Language}} Diversity and Its Importance for Cognitive Science},
  shorttitle = {The Myth of Language Universals},
  author = {Evans, Nicholas and Levinson, Stephen C.},
  year = {2009},
  month = oct,
  journal = {Behavioral and Brain Sciences},
  volume = {32},
  number = {5},
  pages = {429--448},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X0999094X},
  abstract = {Talk of linguistic universals has given cognitive scientists the impression that languages are all built to a common pattern. In fact, there are vanishingly few universals of language in the direct sense that all languages exhibit them. Instead, diversity can be found at almost every level of linguistic organization. This fundamentally changes the object of enquiry from a cognitive science perspective. This target article summarizes decades of cross-linguistic work by typologists and descriptive linguists, showing just how few and unprofound the universal characteristics of language are, once we honestly confront the diversity offered to us by the world's 6,000 to 8,000 languages. After surveying the various uses of ``universal,'' we illustrate the ways languages vary radically in sound, meaning, and syntactic organization, and then we examine in more detail the core grammatical machinery of recursion, constituency, and grammatical relations. Although there are significant recurrent patterns in organization, these are better explained as stable engineering solutions satisfying multiple design constraints, reflecting both cultural-historical factors and the constraints of human cognition.Linguistic diversity then becomes the crucial datum for cognitive science: we are the only species with a communication system that is fundamentally variable at all levels. Recognizing the true extent of structural diversity in human language opens up exciting new research directions for cognitive scientists, offering thousands of different natural experiments given by different languages, with new opportunities for dialogue with biological paradigms concerned with change and diversity, and confronting us with the extraordinary plasticity of the highest human skills.},
  langid = {english},
  keywords = {_tablet,Chomsky,coevolution,constituency,culture,dependency,evolutionary theory,Greenberg,linguistic diversity,linguistic typology,recursion,universal grammar},
  file = {/Users/jonny/Dropbox/papers/zotero/E/EvansN/evans_2009_the_myth_of_language_universals.pdf;/Users/jonny/Zotero/storage/FLQX2WWN/evans_2009_the_myth_of_language_universals.pdf}
}

@book{FeatureGeometryInternal2018,
  title = {5 {{Feature Geometry}} \textemdash{} {{Internal Organization}} of {{Speech Sounds}} : 8.5.1 {{Introduction The}} Discussion in the Preceding Section Illustrates That the Basic Units of Phonological Representation Are Not Phonemes but Features. {{Features}} Play the Same Linguistically Meaningful Distinctive Role as Phonemes, but the Use of Features Offers Straightforward},
  shorttitle = {5 {{Feature Geometry}} \textemdash{} {{Internal Organization}} of {{Speech Sounds}}},
  year = {2018},
  month = oct,
  journal = {Speech Processing},
  pages = {300--305},
  publisher = {{CRC Press}},
  doi = {10.1201/9781482276237-42},
  isbn = {978-1-315-21470-2},
  langid = {english}
}

@article{feldmanStructurePerceptualCategories1997,
  title = {The {{Structure}} of {{Perceptual Categories}}},
  author = {Feldman, Jacob},
  year = {1997},
  month = jun,
  journal = {Journal of Mathematical Psychology},
  volume = {41},
  number = {2},
  pages = {145--170},
  issn = {0022-2496},
  doi = {10.1006/jmps.1997.1154},
  abstract = {When presented with a small set of sample objects, human observers have the striking capacity to induce a more general class. Generalization can even proceed from a single object (``one-shot categorization''). The inference is apparently guided by the principle that a good categorical hypothesis is one in which the observed object would be a typcal, ``non-accidental,'' orgenericexample; this idea is formalized here as the Genericity Constraint. In the theory proposed here, each categorical hypothesis is a ``generative model,'' a sequence of transformations by which the object is interpreted as having been created; objects are considered to be in the same category if they were created by the same set of operations. The set of all available category models can be explicitly enumerated in a lattice, an explicit structure that partially orders the models by their degree of regularity or genericity\textemdash more abstract models are higher in the lattice, and more regular or constrained models are lower. The Genericity Constraint dictates that among all the models on the lattice that apply, the observer should choose the one in which the observed object is generic, which is simply the lowest in the partial order. A series of experiments are reported in which subjects are asked to generalize from simple figures. The results corroborate the role of the lattice and the Genericity Constraint in subjects' interpretations},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/F/FeldmanJ/feldman_1997_the_structure_of_perceptual_categories.pdf;/Users/jonny/Zotero/storage/FFE4IDLI/feldman_1997_the_structure_of_perceptual_categories.pdf}
}

@article{fengRoleHumanAuditory2018,
  title = {The {{Role}} of the {{Human Auditory Corticostriatal Network}} in {{Speech Learning}}},
  author = {Feng, Gangyi and Yi, Han Gyol and Chandrasekaran, Bharath},
  year = {2018},
  month = dec,
  journal = {Cerebral Cortex (New York, N.Y.: 1991)},
  issn = {1460-2199},
  doi = {10.1093/cercor/bhy289},
  abstract = {We establish a mechanistic account of how the mature human brain functionally reorganizes to acquire and represent new speech sounds. Native speakers of English learned to categorize Mandarin lexical tone categories produced by multiple talkers using trial-by-trial feedback. We hypothesized that the corticostriatal system is a key intermediary in mediating temporal lobe plasticity and the acquisition of new speech categories in adulthood. We conducted a functional magnetic resonance imaging experiment in which participants underwent a sound-to-category mapping task. Diffusion tensor imaging data were collected, and probabilistic fiber tracking analysis was employed to assay the auditory corticostriatal pathways. Multivariate pattern analysis showed that talker-invariant novel tone category representations emerged in the left superior temporal gyrus (LSTG) within a few hundred training trials. Univariate analysis showed that the putamen, a subregion of the striatum, was sensitive to positive feedback in correctly categorized trials. With learning, functional coupling between the putamen and LSTG increased during error processing. Furthermore, fiber tractography demonstrated robust structural connectivity between the feedback-sensitive striatal regions and the LSTG regions that represent the newly learned tone categories. Our convergent findings highlight a critical role for the auditory corticostriatal circuitry in mediating the acquisition of new speech categories.},
  langid = {english},
  pmid = {30535138},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/F/FengG/feng_2018_the_role_of_the_human_auditory_corticostriatal_network_in_speech_learning.pdf}
}

@article{fengTaskGeneralAcousticInvariantNeural2018,
  title = {Task-{{General}} and {{Acoustic-Invariant Neural Representation}} of {{Speech Categories}} in the {{Human Brain}}},
  author = {Feng, Gangyi and Gan, Zhenzhong and Wang, Suiping and Wong, Patrick C. M. and Chandrasekaran, Bharath},
  year = {2018},
  month = sep,
  journal = {Cerebral Cortex (New York, N.Y.: 1991)},
  volume = {28},
  number = {9},
  pages = {3241--3254},
  issn = {1460-2199},
  doi = {10.1093/cercor/bhx195},
  abstract = {A significant neural challenge in speech perception includes extracting discrete phonetic categories from continuous and multidimensional signals despite varying task demands and surface-acoustic variability. While neural representations of speech categories have been previously identified in frontal and posterior temporal-parietal regions, the task dependency and dimensional specificity of these neural representations are still unclear. Here, we asked native Mandarin participants to listen to speech syllables carrying 4 distinct lexical tone categories across passive listening, repetition, and categorization tasks while they underwent functional magnetic resonance imaging (fMRI). We used searchlight classification and representational similarity analysis (RSA) to identify the dimensional structure underlying neural representation across tasks and surface-acoustic properties. Searchlight classification analyses revealed significant "cross-task" lexical tone decoding within the bilateral superior temporal gyrus (STG) and left inferior parietal lobule (LIPL). RSA revealed that the LIPL and LSTG, in contrast to the RSTG, relate to 2 critical dimensions (pitch height, pitch direction) underlying tone perception. Outside this core representational network, we found greater activation in the inferior frontal and parietal regions for stimuli that are more perceptually similar during tone categorization. Our findings reveal the specific characteristics of fronto-tempo-parietal regions that support speech representation and categorization processing.},
  langid = {english},
  pmcid = {PMC6454529},
  pmid = {28968658},
  keywords = {_tablet,Brain,Female,Humans,Male,Speech Perception,Young Adult},
  file = {/Users/jonny/Dropbox/papers/zotero/F/FengG/feng_2018_task-general_and_acoustic-invariant_neural_representation_of_speech_categories.pdf;/Users/jonny/Zotero/storage/CT69ERNC/feng_2018_task-general_and_acoustic-invariant_neural_representation_of_speech_categories.pdf}
}

@article{fiebelkornSpikeTimingAttention2021,
  title = {Spike {{Timing}} in the {{Attention Network Predicts Behavioral Outcome Prior}} to {{Target Selection}}},
  author = {Fiebelkorn, Ian C. and Kastner, Sabine},
  year = {2021},
  month = jan,
  journal = {Neuron},
  volume = {109},
  number = {1},
  pages = {177-188.e4},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2020.09.039},
  langid = {english},
  pmid = {33098762},
  keywords = {_tablet,attention,FEF,LIP,neurophysiology,oscillations,spike timing,vision},
  file = {/Users/jonny/Dropbox/papers/zotero/F/FiebelkornI/fiebelkorn_2021_spike_timing_in_the_attention_network_predicts_behavioral_outcome_prior_to.pdf;/Users/jonny/Zotero/storage/43FDT7TZ/fiebelkorn_2021_spike_timing_in_the_attention_network_predicts_behavioral_outcome_prior_to.pdf}
}

@misc{ForeignlanguageExperienceInfancy,
  title = {Foreign-Language Experience in Infancy: {{Effects}} of Short-Term Exposure and Social Interaction on Phonetic Learning | {{PNAS}}},
  howpublished = {https://www.pnas.org/content/100/15/9096}
}

@article{fritzActiveListeningTaskdependent2005b,
  title = {Active Listening: Task-Dependent Plasticity of Spectrotemporal Receptive Fields in Primary Auditory Cortex},
  shorttitle = {Active Listening},
  author = {Fritz, Jonathan and Elhilali, Mounya and Shamma, Shihab},
  year = {2005},
  month = aug,
  journal = {Hearing Research},
  volume = {206},
  number = {1-2},
  pages = {159--176},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2005.01.015},
  abstract = {Listening is an active process in which attentive focus on salient acoustic features in auditory tasks can influence receptive field properties of cortical neurons. Recent studies showing rapid task-related changes in neuronal spectrotemporal receptive fields (STRFs) in primary auditory cortex of the behaving ferret are reviewed in the context of current research on cortical plasticity. Ferrets were trained on spectral tasks, including tone detection and two-tone discrimination, and on temporal tasks, including gap detection and click-rate discrimination. STRF changes could be measured on-line during task performance and occurred within minutes of task onset. During spectral tasks, there were specific spectral changes (enhanced response to tonal target frequency in tone detection and discrimination, suppressed response to tonal reference frequency in tone discrimination). However, only in the temporal tasks, the STRF was changed along the temporal dimension by sharpening temporal dynamics. In ferrets trained on multiple tasks, distinctive and task-specific STRF changes could be observed in the same cortical neurons in successive behavioral sessions. These results suggest that rapid task-related plasticity is an ongoing process that occurs at a network and single unit level as the animal switches between different tasks and dynamically adapts cortical STRFs in response to changing acoustic demands.},
  langid = {english},
  pmid = {16081006},
  keywords = {_tablet,Animals,Auditory Cortex,Auditory Perception,Conditioning; Psychological,Discrimination; Psychological,Evoked Potentials; Auditory,Ferrets,Haplorhini,Humans,Learning,Neuronal Plasticity,Reaction Time,Task Performance and Analysis},
  file = {/Users/jonny/Dropbox/papers/zotero/F/FritzJ/fritz_2005_active_listening.pdf}
}

@article{fritzRapidTaskrelatedPlasticity2003a,
  title = {Rapid Task-Related Plasticity of Spectrotemporal Receptive Fields in Primary Auditory Cortex},
  author = {Fritz, Jonathan and Shamma, Shihab and Elhilali, Mounya and Klein, David},
  year = {2003},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {6},
  number = {11},
  pages = {1216--1223},
  issn = {1097-6256},
  doi = {10.1038/nn1141},
  abstract = {We investigated the hypothesis that task performance can rapidly and adaptively reshape cortical receptive field properties in accord with specific task demands and salient sensory cues. We recorded neuronal responses in the primary auditory cortex of behaving ferrets that were trained to detect a target tone of any frequency. Cortical plasticity was quantified by measuring focal changes in each cell's spectrotemporal response field (STRF) in a series of passive and active behavioral conditions. STRF measurements were made simultaneously with task performance, providing multiple snapshots of the dynamic STRF during ongoing behavior. Attending to a specific target frequency during the detection task consistently induced localized facilitative changes in STRF shape, which were swift in onset. Such modulatory changes may enhance overall cortical responsiveness to the target tone and increase the likelihood of 'capturing' the attended target during the detection task. Some receptive field changes persisted for hours after the task was over and hence may contribute to long-term sensory memory.},
  langid = {english},
  pmid = {14583754},
  keywords = {_tablet,Acoustic Stimulation,Action Potentials,Animals,Auditory Cortex,Avoidance Learning,Behavior; Animal,Conditioning; Psychological,Discrimination; Psychological,Electrophysiology,Evoked Potentials; Auditory,Ferrets,Neuronal Plasticity,Probability,Random Allocation,Reaction Time,Task Performance and Analysis,Time Factors},
  file = {/Users/jonny/Dropbox/papers/zotero/F/FritzJ/fritz_2003_rapid_task-related_plasticity_of_spectrotemporal_receptive_fields_in_primary.pdf}
}

@misc{FrontiersDifferentialActivation,
  title = {Frontiers | {{Differential}} Activation of Human Core, Non-Core and Auditory-Related Cortex during Speech Categorization Tasks as Revealed by Intracranial Recordings | {{Neuroscience}}},
  howpublished = {https://www.frontiersin.org/articles/10.3389/fnins.2014.00240/full}
}

@misc{FrontiersReconsideringTonotopic,
  title = {Frontiers | {{Reconsidering Tonotopic Maps}} in the {{Auditory Cortex}} and {{Lemniscal Auditory Thalamus}} in {{Mice}} | {{Frontiers}} in {{Neural Circuits}}},
  howpublished = {https://www.frontiersin.org/articles/10.3389/fncir.2017.00014/full}
}

@article{gallegoCorticalPopulationActivity2018,
  title = {Cortical Population Activity within a Preserved Neural Manifold Underlies Multiple Motor Behaviors},
  author = {Gallego, Juan A. and Perich, Matthew G. and Naufel, Stephanie N. and Ethier, Christian and Solla, Sara A. and Miller, Lee E.},
  year = {2018},
  month = oct,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {4233},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-06560-z},
  abstract = {Populations of cortical neurons flexibly perform different functions; for the primary motor cortex (M1) this means a rich repertoire of motor behaviors. We investigate the flexibility of M1 movement control by analyzing neural population activity during a variety of skilled wrist and reach-to-grasp tasks. We compare across tasks the neural modes that capture dominant neural covariance patterns during each task. While each task requires different patterns of muscle and single unit activity, we find unexpected similarities at the neural population level: the structure and activity of the neural modes is largely preserved across tasks. Furthermore, we find two sets of neural modes with task-independent activity that capture, respectively, generic temporal features of the set of tasks and a task-independent mapping onto muscle activity. This system of flexibly combined, well-preserved neural modes may underlie the ability of M1 to learn and generate a wide-ranging behavioral repertoire.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/G/GallegoJ/gallego_2018_cortical_population_activity_within_a_preserved_neural_manifold_underlies.pdf;/Users/jonny/Zotero/storage/DP836M3M/s41467-018-06560-z.html}
}

@article{gallegoNeuralManifoldsControl2017,
  title = {Neural {{Manifolds}} for the {{Control}} of {{Movement}}},
  author = {Gallego, Juan A. and Perich, Matthew G. and Miller, Lee E. and Solla, Sara A.},
  year = {2017},
  month = jun,
  journal = {Neuron},
  volume = {94},
  number = {5},
  pages = {978--984},
  issn = {1097-4199},
  doi = {10.1016/j.neuron.2017.05.025},
  abstract = {The analysis of neural dynamics in several brain cortices has consistently uncovered low-dimensional manifolds that capture a significant fraction of neural variability. These neural manifolds are spanned by specific patterns of correlated neural activity, the "neural modes." We discuss a model for neural control of movement in which the time-dependent activation of these neural modes is the generator of motor behavior. This manifold-based view of motor cortex may lead to a better understanding of how the brain controls movement.},
  langid = {english},
  pmcid = {PMC6122849},
  pmid = {28595054},
  keywords = {_tablet,Animals,Brain,Humans,Models; Neurological,Motor Cortex,Movement,Neurons},
  file = {/Users/jonny/Dropbox/papers/zotero/G/GallegoJ/gallego_2017_neural_manifolds_for_the_control_of_movement.pdf}
}

@article{gatiRepresentationsQualitativeQuantitative1982,
  title = {Representations of Qualitative and Quantitative Dimensions},
  author = {Gati, Itamar and Tversky, Amos},
  year = {1982},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {8},
  number = {2},
  pages = {325--340},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.8.2.325},
  abstract = {Geometric representations of psychological dimensions were analyzed and compared to an alternative set-theoretical approach. Judgments of similarity between forms and figures by 244 high school and undergraduate students revealed the following effects: (a) Qualitative attributes were curved relative to quantitative attributes, contrary to intradimensional subtractivity; (b) quantitative attributes augmented differences in qualitative attributes, contrary to interdimensional additivity; and (c) adding a new dimension with a fixed value increased similarity, contrary to translation invariance. The implications of these results for multidimensional representations of proximity data are discussed. (24 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {_tablet,Models,Multidimensional Scaling,Stimulus Similarity},
  file = {/Users/jonny/Dropbox/papers/zotero/G/GatiI/gati_1982_representations_of_qualitative_and_quantitative_dimensions.pdf;/Users/jonny/Zotero/storage/3IM8T7QL/1982-27032-001.html}
}

@article{Geffen2011,
  title = {Auditory Perception of Self-Similarity in Water Sounds.},
  author = {Geffen, Maria N and Gervain, Judit and Werker, Janet F and Magnasco, Marcelo O},
  year = {2011},
  journal = {Front Integr Neurosci},
  volume = {5},
  number = {May},
  pages = {15},
  issn = {1662-5145},
  doi = {10.3389/fnint.2011.00015},
  abstract = {Many natural signals, including environmental sounds, exhibit scale-invariant statistics: their structure is repeated at multiple scales. Such scale-invariance has been identified separately across spectral and temporal correlations of natural sounds (Clarke and Voss, 1975; Attias and Schreiner, 1997; Escabi et al., 2003; Singh and Theunissen, 2003). Yet the role of scale-invariance across overall spectro-temporal structure of the sound has not been explored directly in auditory perception. Here, we identify that the acoustic waveform from the recording of running water is a self-similar fractal, exhibiting scale-invariance not only within spectral channels, but also across the full spectral bandwidth. The auditory perception of the water sound did not change with its scale. We tested the role of scale-invariance in perception by using an artificial sound, which could be rendered scale-invariant. We generated a random chirp stimulus: an auditory signal controlled by two parameters, Q, controlling the relative, and r, controlling the absolute, temporal structure of the sound. Imposing scale-invariant statistics on the artificial sound was required for its perception as natural and water-like. Further, Q had to be restricted to a specific range for the sound to be perceived as natural. To detect self-similarity in the water sound, and identify Q, the auditory system needs to process the temporal dynamics of the waveform across spectral bands in terms of the number of cycles, rather than absolute timing. We propose a two-stage neural model implementing this computation. This computation may be carried out by circuits of neurons in the auditory cortex. The set of auditory stimuli developed in this study are particularly suitable for measurements of response properties of neurons in the auditory pathway, allowing for quantification of the effects of varying the statistics of the spectro-temporal statistical structure of the stimulus.},
  pmid = {21617734},
  keywords = {_tablet,\#nosource,auditory,coherence,perception,psychophys,psychophysics,receptive field,scale-invariance,temporal adaptation},
  file = {/Users/jonny/Dropbox/papers/zotero/G/GeffenM/geffen_2011_auditory_perception_of_self-similarity_in_water_sounds.pdf}
}

@misc{giacagliaTransformers2020,
  title = {Transformers},
  author = {Giacaglia, Giuliano},
  year = {2020},
  month = oct,
  journal = {Medium},
  abstract = {Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in\ldots},
  howpublished = {https://towardsdatascience.com/transformers-141e32e69591},
  langid = {english}
}

@article{gillPreciseHolographicManipulation2020,
  title = {Precise {{Holographic Manipulation}} of {{Olfactory Circuits Reveals Coding Features Determining Perceptual Detection}}},
  author = {Gill, Jonathan V. and Lerman, Gilad M. and Zhao, Hetince and Stetler, Benjamin J. and Rinberg, Dmitry and Shoham, Shy},
  year = {2020},
  month = oct,
  journal = {Neuron},
  volume = {108},
  number = {2},
  pages = {382-393.e5},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2020.07.034},
  langid = {english},
  pmid = {32841590},
  keywords = {holography,imaging,olfaction,olfactory bulb,optogenetics,perceptual threshold,photostimulation,synchrony,two-photon},
  file = {/Users/jonny/Zotero/storage/BGQJ9UD8/S0896-6273(20)30578-X.html}
}

@article{goddardInterpretingDimensionsNeural2018,
  title = {Interpreting the Dimensions of Neural Feature Representations Revealed by Dimensionality Reduction},
  author = {Goddard, Erin and Klein, Colin and Solomon, Samuel G. and Hogendoorn, Hinze and Carlson, Thomas A.},
  year = {2018},
  month = oct,
  journal = {NeuroImage},
  series = {New Advances in Encoding and Decoding of Brain Signals},
  volume = {180},
  pages = {41--67},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2017.06.068},
  abstract = {Recent progress in understanding the structure of neural representations in the cerebral cortex has centred around the application of multivariate classification analyses to measurements of brain activity. These analyses have proved a sensitive test of whether given brain regions provide information about specific perceptual or cognitive processes. An exciting extension of this approach is to infer the structure of this information, thereby drawing conclusions about the underlying neural representational space. These approaches rely on exploratory data-driven dimensionality reduction to extract the natural dimensions of neural spaces, including natural visual object and scene representations, semantic and conceptual knowledge, and working memory. However, the efficacy of these exploratory methods is unknown, because they have only been applied to representations in brain areas for which we have little or no secondary knowledge. One of the best-understood areas of the cerebral cortex is area MT of primate visual cortex, which is known to be important in motion analysis. To assess the effectiveness of dimensionality reduction for recovering neural representational space we applied several dimensionality reduction methods to multielectrode measurements of spiking activity obtained from area MT of marmoset monkeys, made while systematically varying the motion direction and speed of moving stimuli. Despite robust tuning at individual electrodes, and high classifier performance, dimensionality reduction rarely revealed dimensions for direction and speed. We use this example to illustrate important limitations of these analyses, and suggest a framework for how to best apply such methods to data where the structure of the neural representation is unknown.},
  langid = {english},
  keywords = {_tablet,Exploratory analysis,Multi-dimensional scaling (MDS),Multivariate pattern analysis,Principal component analysis (PCA)},
  file = {/Users/jonny/Dropbox/papers/zotero/G/GoddardE/goddard_2018_interpreting_the_dimensions_of_neural_feature_representations_revealed_by.pdf}
}

@article{guestHowComputationalModeling2021,
  title = {How {{Computational Modeling Can Force Theory Building}} in {{Psychological Science}}},
  author = {Guest, Olivia and Martin, Andrea E.},
  year = {2021},
  month = jan,
  journal = {Perspectives on Psychological Science},
  pages = {1745691620970585},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620970585},
  abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined\textemdash what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
  langid = {english},
  keywords = {_tablet,computational model,open science,scientific inference,theoretical psychology},
  file = {/Users/jonny/Dropbox/papers/zotero/G/GuestO/guest_2021_how_computational_modeling_can_force_theory_building_in_psychological_science.pdf}
}

@article{halleFeatureSpreadingRepresentation2000,
  title = {On {{Feature Spreading}} and the {{Representation}} of {{Place}} of {{Articulation}}},
  author = {Halle, Morris and Vaux, Bert and Wolfe, Andrew},
  year = {2000},
  month = jul,
  journal = {Linguistic Inquiry},
  volume = {31},
  number = {3},
  pages = {387--444},
  publisher = {{MIT Press}},
  issn = {0024-3892},
  doi = {10.1162/002438900554398},
  abstract = {Since Clements (1985) introduced feature geometry, four major innovations have been proposed: Unified Feature Theory, Vowel-Place Theory, Strict Locality, and Partial Spreading. We set out the problems that each innovation encounters and propose a new model of feature geometry and feature spreading that is not subject to these problems. Of the four innovations, the new model-Revised Articulator Theory (RAT)-keeps Partial Spreading, but rejects the rest. RAT also introduces a new type of unary feature-one for each articulator-to indicate that the articulator is the designated articulator of the segment.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/H/HalleM/halle_2000_on_feature_spreading_and_the_representation_of_place_of_articulation.pdf}
}

@article{hamiltonSpatialMapOnset2018a,
  title = {A {{Spatial Map}} of {{Onset}} and {{Sustained Responses}} to {{Speech}} in the {{Human Superior Temporal Gyrus}}},
  author = {Hamilton, Liberty S. and Edwards, Erik and Chang, Edward F.},
  year = {2018},
  month = jun,
  journal = {Current Biology},
  volume = {28},
  number = {12},
  pages = {1860-1871.e4},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2018.04.033},
  abstract = {Summary To derive meaning from speech, we must extract multiple dimensions of concurrent information from incoming speech signals. That is, equally important to processing phonetic features is the detection of acoustic cues that give structure and context to the information we hear. How the brain organizes this information is unknown. Using data-driven computational methods on high-density intracranial recordings from 27 human participants, we reveal the functional distinction of neural responses to speech in the posterior superior temporal gyrus according to either onset or sustained response profiles. Though similar response types have been observed throughout the auditory system, we found novel evidence for a major spatial parcellation in which a distinct caudal zone detects acoustic onsets and a rostral-surround zone shows sustained, relatively delayed responses to ongoing speech stimuli. While posterior onset and anterior sustained responses are used substantially during natural speech perception, they are not limited to speech stimuli and are seen even for reversed or spectrally rotated speech. Single-electrode encoding of phonetic features in each zone depended upon whether the sound occurred at sentence onset, suggesting joint encoding of phonetic features and their temporal context. Onset responses in the caudal zone could accurately decode sentence and phrase onset boundaries, providing a potentially important internal mechanism for detecting temporal landmarks in speech and other natural sounds. These findings suggest that onset and sustained responses not only define the basic spatial organization of high-order auditory cortex but also have direct implications for how speech information is parsed in the cortex. Video Abstract},
  keywords = {_tablet,auditory,ECoG,electrocorticography,intracranial recordings,natural speech,neurolinguistics,spectrotemporal receptive field,unsupervised learning},
  file = {/Users/jonny/Dropbox/papers/zotero/H/HamiltonL/hamilton_2018_a_spatial_map_of_onset_and_sustained_responses_to_speech_in_the_human_superior.pdf;/Users/jonny/Papers/HamiltonL/2018/Hamilton_2018_A Spatial Map of Onset and Sustained Responses to Speech in the Human Superior.pdf;/Users/jonny/Zotero/storage/NDWBZISC/S0960982218304615.html}
}

@misc{HaskinsLaboratories2020,
  title = {Haskins {{Laboratories}}},
  year = {2020},
  month = aug,
  howpublished = {https://web.archive.org/web/20200809223413/http://www.haskins.yale.edu/featured/sws/sws.html}
}

@article{hebartRevealingMultidimensionalMental2020,
  title = {Revealing the Multidimensional Mental Representations of Natural Objects Underlying Human Similarity Judgements},
  author = {Hebart, Martin N. and Zheng, Charles Y. and Pereira, Francisco and Baker, Chris I.},
  year = {2020},
  month = nov,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {11},
  pages = {1173--1185},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00951-3},
  abstract = {Objects can be characterized according to a vast number of possible criteria (such as animacy, shape, colour and function), but some dimensions are more useful than others for making sense of the objects around us. To identify these core dimensions of object representations, we developed a data-driven computational model of similarity judgements for real-world images of 1,854 objects. The model captured most explainable variance in similarity judgements and produced 49 highly reproducible and meaningful object dimensions that reflect various conceptual and perceptual properties of those objects. These dimensions predicted external categorization behaviour and reflected typicality judgements of those categories. Furthermore, humans can accurately rate objects along these dimensions, highlighting their interpretability and opening up a way to generate similarity estimates from object dimensions alone. Collectively, these results demonstrate that human similarity judgements can be captured by a fairly low-dimensional, interpretable embedding that generalizes to external behaviour.},
  copyright = {2020 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply},
  langid = {english},
  file = {/Users/jonny/Zotero/storage/649HMKDT/s41562-020-00951-3.html}
}

@article{heegerComputationalModelsCortical1996,
  title = {Computational Models of Cortical Visual Processing},
  author = {Heeger, D. J. and Simoncelli, E. P. and Movshon, J. A.},
  year = {1996},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {93},
  number = {2},
  pages = {623--627},
  issn = {0027-8424},
  doi = {10.1073/pnas.93.2.623},
  abstract = {The visual responses of neurons in the cerebral cortex were first adequately characterized in the 1960s by D. H. Hubel and T. N. Wiesel [(1962) J. Physiol. (London) 160, 106-154; (1968) J. Physiol. (London) 195, 215-243] using qualitative analyses based on simple geometric visual targets. Over the past 30 years, it has become common to consider the properties of these neurons by attempting to make formal descriptions of these transformations they execute on the visual image. Most such models have their roots in linear-systems approaches pioneered in the retina by C. Enroth-Cugell and J. R. Robson [(1966) J. Physiol. (London) 187, 517-552], but it is clear that purely linear models of cortical neurons are inadequate. We present two related models: one designed to account for the responses of simple cells in primary visual cortex (V1) and one designed to account for the responses of pattern direction selective cells in MT (or V5), an extrastriate visual area thought to be involved in the analysis of visual motion. These models share a common structure that operates in the same way on different kinds of input, and instantiate the widely held view that computational strategies are similar throughout the cerebral cortex. Implementations of these models for Macintosh microcomputers are available and can be used to explore the models' properties.},
  langid = {english},
  pmcid = {PMC40101},
  pmid = {8570605},
  keywords = {_tablet,Computer Simulation,Models; Neurological,Neurons,Perceptual Masking,Software,Visual Cortex,Visual Perception},
  file = {/Users/jonny/Dropbox/papers/zotero/H/HeegerD/heeger_1996_computational_models_of_cortical_visual_processing.pdf}
}

@article{hipolitoMarkovBlanketsBrain2020,
  title = {Markov {{Blankets}} in the {{Brain}}},
  author = {Hipolito, Ines and Ramstead, Maxwell and Convertino, Laura and Bhat, Anjali and Friston, Karl and Parr, Thomas},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.02741 [physics, q-bio]},
  eprint = {2006.02741},
  eprinttype = {arxiv},
  primaryclass = {physics, q-bio},
  abstract = {Recent characterisations of self-organising systems depend upon the presence of a Markov blanket: a statistical boundary that mediates the interactions between what is inside of and outside of a system. We leverage this idea to provide an analysis of partitions in neuronal systems. This is applicable to brain architectures at multiple scales, enabling partitions into single neurons, brain regions, and brain-wide networks. This treatment is based upon the canonical micro-circuitry used in empirical studies of effective connectivity, so as to speak directly to practical applications. This depends upon the dynamic coupling between functional units, whose form recapitulates that of a Markov blanket at each level. The nuance afforded by partitioning neural systems in this way highlights certain limitations of modular perspectives of brain function that only consider a single level of description.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Physics - Biological Physics,Quantitative Biology - Neurons and Cognition},
  file = {/Users/jonny/Dropbox/papers/zotero/H/HipolitoI/hipolito_2020_markov_blankets_in_the_brain.pdf;/Users/jonny/Zotero/storage/YDG37RJB/2006.html}
}

@article{holtMeanMattersEffects2006,
  title = {The Mean Matters: Effects of Statistically Defined Nonspeech Spectral Distributions on Speech Categorization},
  shorttitle = {The Mean Matters},
  author = {Holt, Lori L.},
  year = {2006},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {120},
  number = {5 Pt 1},
  pages = {2801--2817},
  issn = {0001-4966},
  doi = {10.1121/1.2354071},
  abstract = {Adjacent speech, and even nonspeech, contexts influence phonetic categorization. Four experiments investigated how preceding sequences of sine-wave tones influence phonetic categorization. This experimental paradigm provides a means of investigating the statistical regularities of acoustic events that influence online speech categorization and, reciprocally, reveals regularities of the sound environment tracked by auditory processing. The tones comprising the sequences were drawn from distributions sampling different acoustic frequencies. Results indicate that whereas the mean of the distributions predicts contrastive shifts in speech categorization, variability of the distributions has little effect. Moreover, speech categorization is influenced by the global mean of the tone sequence, without significant influence of local statistical regularities within the tone sequence. Further arguing that the effect is strongly related to the average spectrum of the sequence, notched noise spectral complements of the tone sequences produce a complementary effect on speech categorization. Lastly, these effects are modulated by the number of tones in the acoustic history and the overall duration of the sequence, but not by the density with which the distribution defining the sequence is sampled. Results are discussed in light of stimulus-specific adaptation to statistical regularity in the acoustic input and a speculative link to talker normalization is postulated.},
  langid = {english},
  pmcid = {PMC1635014},
  pmid = {17091133},
  keywords = {_tablet,Acoustic Stimulation,Analysis of Variance,Female,Humans,Male,Phonetics,Pitch Perception,Sound Spectrography,Speech Acoustics,Speech Perception},
  file = {/Users/jonny/Dropbox/papers/zotero/H/HoltL/holt_2006_the_mean_matters.pdf;/Users/jonny/Zotero/storage/2ZKB9E3E/holt_2006_the_mean_matters.pdf}
}

@article{holtNeighboringSpectralContent2000,
  title = {Neighboring Spectral Content Influences Vowel Identification},
  author = {Holt, L. L. and Lotto, A. J. and Kluender, K. R.},
  year = {2000},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {108},
  number = {2},
  pages = {710--722},
  issn = {0001-4966},
  doi = {10.1121/1.429604},
  abstract = {Four experiments explored the relative contributions of spectral content and phonetic labeling in effects of context on vowel perception. Two 10-step series of CVC syllables ([bVb] and [dVd]) varying acoustically in F2 midpoint frequency and varying perceptually in vowel height from [delta] to [epsilon] were synthesized. In a forced-choice identification task, listeners more often labeled vowels as [delta] in [dVd] context than in [bVb] context. To examine whether spectral content predicts this effect, nonspeech-speech hybrid series were created by appending 70-ms sine-wave glides following the trajectory of CVC F2's to 60-ms members of a steady-state vowel series varying in F2 frequency. In addition, a second hybrid series was created by appending constant-frequency sine-wave tones equivalent in frequency to CVC F2 onset/offset frequencies. Vowels flanked by frequency-modulated glides or steady-state tones modeling [dVd] were more often labeled as [delta] than were the same vowels surrounded by nonspeech modeling [bVb]. These results suggest that spectral content is important in understanding vowel context effects. A final experiment tested whether spectral content can modulate vowel perception when phonetic labeling remains intact. Voiceless consonants, with lower-amplitude more-diffuse spectra, were found to exert less of an influence on vowel perception than do their voiced counterparts. The data are discussed in terms of a general perceptual account of context effects in speech perception.},
  langid = {english},
  pmid = {10955638},
  keywords = {_tablet,Humans,Phonetics,Speech Perception},
  file = {/Users/jonny/Zotero/storage/5HZYH7VC/holt_2000_neighboring_spectral_content_influences_vowel_identification.pdf;/Users/jonny/Zotero/storage/PKW9GW7Z/holt_2000_neighboring_spectral_content_influences_vowel_identification.pdf}
}

@article{holtSpeechPerceptionCategorization2010,
  title = {Speech Perception as Categorization},
  author = {Holt, Lori L. and Lotto, Andrew J.},
  year = {2010},
  month = jul,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {72},
  number = {5},
  pages = {1218--1227},
  issn = {1943-393X},
  doi = {10.3758/APP.72.5.1218},
  abstract = {Speech perception (SP) most commonly refers to the perceptual mapping from the highly variable acoustic speech signal to a linguistic representation, whether it be phonemes, diphones, syllables, or words. This is an example of categorization, in that potentially discriminable speech sounds are assigned to functionally equivalent classes. In this tutorial, we present some of the main challenges to our understanding of the categorization of speech sounds and the conceptualization of SP that has resulted from these challenges. We focus here on issues and experiments that define open research questions relevant to phoneme categorization, arguing that SP is best understood as perceptual categorization, a position that places SP in direct contact with research from other areas of perception and cognition.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Zotero/storage/6YTIA36Q/holt_2010_speech_perception_as_categorization.pdf;/Users/jonny/Zotero/storage/FMR9972V/holt_2010_speech_perception_as_categorization.pdf}
}

@article{holtTemporallyNonadjacentNonlinguistic2005,
  title = {Temporally Nonadjacent Nonlinguistic Sounds Affect Speech Categorization},
  author = {Holt, Lori L.},
  year = {2005},
  month = apr,
  journal = {Psychological Science},
  volume = {16},
  number = {4},
  pages = {305--312},
  issn = {0956-7976},
  doi = {10.1111/j.0956-7976.2005.01532.x},
  abstract = {Speech perception is an ecologically important example of the highly context-dependent nature of perception; adjacent speech, and even nonspeech, sounds influence how listeners categorize speech. Some theories emphasize linguistic or articulation-based processes in speech-elicited context effects and peripheral (cochlear) auditory perceptual interactions in non-speech-elicited context effects. The present studies challenge this division. Results of three experiments indicate that acoustic histories composed of sine-wave tones drawn from spectral distributions with different mean frequencies robustly affect speech categorization. These context effects were observed even when the acoustic context temporally adjacent to the speech stimulus was held constant and when more than a second of silence or multiple intervening sounds separated the nonlinguistic acoustic context and speech targets. These experiments indicate that speech categorization is sensitive to statistical distributions of spectral information, even if the distributions are composed of nonlinguistic elements. Acoustic context need be neither linguistic nor local to influence speech perception.},
  langid = {english},
  pmid = {15828978},
  keywords = {_tablet,Adult,Attention,Female,Humans,Male,Phonetics,Pitch Perception,Psychoacoustics,Sound Spectrography,Speech Acoustics,Speech Perception},
  file = {/Users/jonny/Dropbox/papers/zotero/H/HoltL/holt_2005_temporally_nonadjacent_nonlinguistic_sounds_affect_speech_categorization.pdf}
}

@article{huberTestLinearFeature1993,
  title = {A Test of the Linear Feature Model of Polymorphous Concept Discrimination with Pigeons},
  author = {Huber, Ludwig and Lenz, Renate},
  year = {1993},
  month = feb,
  journal = {The Quarterly Journal of Experimental Psychology Section B},
  volume = {46},
  number = {1},
  pages = {1--18},
  publisher = {{Routledge}},
  issn = {0272-4995},
  doi = {10.1080/14640749308401092},
  abstract = {One of the most important and usually implicitly assumed models of animal concept discrimination\textendash -the feature learning theory\textendash -was explicitly tested with pigeons. In an experiment that employed a novel method of housing and testing, pigeons were trained to discriminate between two sets of Brunswik faces that differed along four dimensions, each represented by three values. The category rule was defined as an additive combination (sum) of the four feature values and thus resembles the polymorphous m-out-of-n rule. In the course of training, the birds tended to classify members of the categories according to their feature sum. This result was confirmed by the fact that the weightings assigned to each feature were similar in the final stage of training. A multiple regression analysis of the response rates enabled the classification decisions of the pigeons to be predicted from the linear feature model in accordance with feature learning theory.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/14640749308401092},
  file = {/Users/jonny/Zotero/storage/K2WXPKGM/14640749308401092.html}
}

@article{hwangDisengagementMotorCortex2019,
  title = {Disengagement of Motor Cortex from Movement Control during Long-Term Learning},
  author = {Hwang, Eun Jung and Dahlen, Jeffrey E. and Hu, Yvonne Yuling and Aguilar, Karina and Yu, Bin and Mukundan, Madan and Mitani, Akinori and Komiyama, Takaki},
  year = {2019},
  month = oct,
  journal = {Science Advances},
  volume = {5},
  number = {10},
  pages = {eaay0001},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aay0001},
  abstract = {Motor learning involves reorganization of the primary motor cortex (M1). However, it remains unclear how the involvement of M1 in movement control changes during long-term learning. To address this, we trained mice in a forelimb-based motor task over months and performed optogenetic inactivation and two-photon calcium imaging in M1 during the long-term training. We found that M1 inactivation impaired the forelimb movements in the early and middle stages, but not in the late stage, indicating that the movements that initially required M1 became independent of M1. As previously shown, M1 population activity became more consistent across trials from the early to middle stage while task performance rapidly improved. However, from the middle to late stage, M1 population activity became again variable despite consistent expert behaviors. This later decline in activity consistency suggests dissociation between M1 and movements. These findings suggest that long-term motor learning can disengage M1 from movement control. Long-term training of motor skills reorganizes motor circuits to bypass primary motor cortex. Long-term training of motor skills reorganizes motor circuits to bypass primary motor cortex.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/H/HwangE/hwang_2019_disengagement_of_motor_cortex_from_movement_control_during_long-term_learning.pdf;/Users/jonny/Zotero/storage/PNBNYEYN/eaay0001.html}
}

@article{iosadVowelReductionRussian2012,
  title = {Vowel Reduction in {{Russian}}: {{No}} Phonetics in Phonology},
  shorttitle = {Vowel Reduction in {{Russian}}},
  author = {Iosad, Pavel},
  year = {2012},
  doi = {10.1017/S0022226712000102},
  abstract = {Much recent work in phonology concentrates on the role of sonority in the phenomenon of vowel reduction, capitalizing on two facts: that reduction involves raising and/or shortening and that higher vowels and schwa are normally interpreted as having low sonority. This paper presents a different approach to vowel reduction in Standard Russian. It is proposed that the apparent sonority-driven effects in Russian are epiphenomenal. In particular, reduction to schwa is outside the domain of phonological computation in Russian, being an artifact of reduced duration. Other types of neutralization arising in vowel reduction are potentially amenable to a sonority-based analysis, but I argue that current approaches to sonority-driven reduction suffer from representational shortcomings. When these shortcomings are rectified, however, sonority is unnecessary as an explicit factor in vowel reduction: standard markedness mechanisms suffice to explain the data.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/I/IosadP/iosad_2012_vowel_reduction_in_russian.pdf}
}

@article{irvinePlasticityAuditorySystem2018,
  title = {Plasticity in the Auditory System},
  author = {Irvine, Dexter R. F.},
  year = {2018},
  month = may,
  journal = {Hearing Research},
  volume = {362},
  pages = {61--73},
  issn = {1878-5891},
  doi = {10.1016/j.heares.2017.10.011},
  abstract = {Over the last 30 years a wide range of manipulations of auditory input and experience have been shown to result in plasticity in auditory cortical and subcortical structures. The time course of plasticity ranges from very rapid stimulus-specific adaptation to longer-term changes associated with, for example, partial hearing loss or perceptual learning. Evidence for plasticity as a consequence of these and a range of other manipulations of auditory input and/or its significance is reviewed, with an emphasis on plasticity in adults and in the auditory cortex. The nature of the changes in auditory cortex associated with attention, memory and perceptual learning depend critically on task structure, reward contingencies, and learning strategy. Most forms of auditory system plasticity are adaptive, in that they serve to optimize auditory performance, prompting attempts to harness this plasticity for therapeutic purposes. However, plasticity associated with cochlear trauma and partial hearing loss appears to be maladaptive, and has been linked to tinnitus. Three important forms of human learning-related auditory system plasticity are those associated with language development, musical training, and improvement in performance with a cochlear implant. Almost all forms of plasticity involve changes in synaptic excitatory - inhibitory balance within existing patterns of connectivity. An attractive model applicable to a number of forms of learning-related plasticity is dynamic multiplexing by individual neurons, such that learning involving a particular stimulus attribute reflects a particular subset of the diverse inputs to a given neuron being gated by top-down influences. The plasticity evidence indicates that auditory cortex is a component of complex distributed networks that integrate the representation of auditory stimuli with attention, decision and reward processes.},
  langid = {english},
  pmid = {29126650},
  keywords = {_tablet,Acoustic Stimulation,Animals,Attention,Auditory Cortex,Auditory Pathways,Auditory Perception,Hearing,Hearing loss,Humans,Memory,Models; Neurological,Nerve Net,Neuromodulators,Neuronal Plasticity,Perceptual learning,Stimulus-specific adaptation,Synaptic Transmission,Synaptic weights},
  file = {/Users/jonny/Dropbox/papers/zotero/I/IrvineD/irvine_2018_plasticity_in_the_auditory_system.pdf}
}

@article{iversonInfluencesPhoneticIdentification1996,
  title = {Influences of Phonetic Identification and Category Goodness on {{American}} Listeners' Perception of /r/ and /l/},
  author = {Iverson, Paul and Kuhl, Patricia K.},
  year = {1996},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {99},
  number = {2},
  pages = {1130--1140},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.415234},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/I/IversonP/iverson_1996_influences_of_phonetic_identification_and_category_goodness_on_american.pdf;/Users/jonny/Zotero/storage/L8C37HH9/iverson_1996_influences_of_phonetic_identification_and_category_goodness_on_american.pdf}
}

@article{kanwisherFunctionalSpecificityHuman2010,
  title = {Functional Specificity in the Human Brain: {{A}} Window into the Functional Architecture of the Mind},
  shorttitle = {Functional Specificity in the Human Brain},
  author = {Kanwisher, Nancy},
  year = {2010},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {107},
  number = {25},
  pages = {11163--11170},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1005062107},
  abstract = {Is the human mind/brain composed of a set of highly specialized components, each carrying out a specific aspect of human cognition, or is it more of a general-purpose device, in which each component participates in a wide variety of cognitive processes? For nearly two centuries, proponents of specialized organs or modules of the mind and brain\textemdash from the phrenologists to Broca to Chomsky and Fodor\textemdash have jousted with the proponents of distributed cognitive and neural processing\textemdash from Flourens to Lashley to McClelland and Rumelhart. I argue here that research using functional MRI is beginning to answer this long-standing question with new clarity and precision by indicating that at least a few specific aspects of cognition are implemented in brain regions that are highly specialized for that process alone. Cortical regions have been identified that are specialized not only for basic sensory and motor processes but also for the high-level perceptual analysis of faces, places, bodies, visually presented words, and even for the very abstract cognitive function of thinking about another person's thoughts. I further consider the as-yet unanswered questions of how much of the mind and brain are made up of these functionally specialized components and how they arise developmentally.},
  chapter = {Biological Sciences},
  langid = {english},
  pmid = {20484679},
  keywords = {_tablet,brain imaging,functional MRI,fusiform face area,modularity},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KanwisherN/kanwisher_2010_functional_specificity_in_the_human_brain.pdf}
}

@article{katoDynamicSensoryRepresentations2012,
  title = {Dynamic {{Sensory Representations}} in the {{Olfactory Bulb}}: {{Modulation}} by {{Wakefulness}} and {{Experience}}},
  shorttitle = {Dynamic {{Sensory Representations}} in the {{Olfactory Bulb}}},
  author = {Kato, Hiroyuki K. and Chu, Monica W. and Isaacson, Jeffry S. and Komiyama, Takaki},
  year = {2012},
  month = dec,
  journal = {Neuron},
  volume = {76},
  number = {5},
  pages = {962--975},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2012.09.037},
  abstract = {How are sensory representations in the brain influenced by the state of an animal? Here we use chronic two-photon calcium imaging to explore how wakefulness and experience shape odor representations in the mouse olfactory bulb. Comparing the awake and anesthetized state, we show that wakefulness greatly enhances the activity of inhibitory granule cells and makes principal mitral cell odor responses more sparse and temporally dynamic. In awake mice, brief repeated odor experience leads to a gradual and long-lasting (months) weakening of mitral cell odor representations. This mitral cell plasticity is odor-specific, recovers gradually over months and can be repeated with different odors. Furthermore, the expression of this experience-dependent plasticity is prevented by anesthesia. Together, our results demonstrate the dynamic nature of mitral cell odor representations in awake animals, which is constantly shaped by recent odor experience.},
  pmcid = {PMC3523713},
  pmid = {23217744},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KatoH/kato_2012_dynamic_sensory_representations_in_the_olfactory_bulb.pdf}
}

@article{kaufeldLinguisticStructureMeaning2020,
  title = {Linguistic {{Structure}} and {{Meaning Organize Neural Oscillations}} into a {{Content-Specific Hierarchy}}},
  author = {Kaufeld, Greta and Bosker, Hans Rutger and ten Oever, Sanne and Alday, Phillip M. and Meyer, Antje S. and Martin, Andrea E.},
  year = {2020},
  month = dec,
  journal = {Journal of Neuroscience},
  volume = {40},
  number = {49},
  pages = {9467--9475},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0302-20.2020},
  abstract = {Neural oscillations track linguistic information during speech comprehension (Ding et al., 2016; Keitel et al., 2018), and are known to be modulated by acoustic landmarks and speech intelligibility (Doelling et al., 2014; Zoefel and VanRullen, 2015). However, studies investigating linguistic tracking have either relied on non-naturalistic isochronous stimuli or failed to fully control for prosody. Therefore, it is still unclear whether low-frequency activity tracks linguistic structure during natural speech, where linguistic structure does not follow such a palpable temporal pattern. Here, we measured electroencephalography (EEG) and manipulated the presence of semantic and syntactic information apart from the timescale of their occurrence, while carefully controlling for the acoustic-prosodic and lexical-semantic information in the signal. EEG was recorded while 29 adult native speakers (22 women, 7 men) listened to naturally spoken Dutch sentences, jabberwocky controls with morphemes and sentential prosody, word lists with lexical content but no phrase structure, and backward acoustically matched controls. Mutual information (MI) analysis revealed sensitivity to linguistic content: MI was highest for sentences at the phrasal (0.8\textendash 1.1 Hz) and lexical (1.9\textendash 2.8 Hz) timescales, suggesting that the delta-band is modulated by lexically driven combinatorial processing beyond prosody, and that linguistic content (i.e., structure and meaning) organizes neural oscillations beyond the timescale and rhythmicity of the stimulus. This pattern is consistent with neurophysiologically inspired models of language comprehension (Martin, 2016, 2020; Martin and Doumas, 2017) where oscillations encode endogenously generated linguistic content over and above exogenous or stimulus-driven timing and rhythm information. SIGNIFICANCE STATEMENT Biological systems like the brain encode their environment not only by reacting in a series of stimulus-driven responses, but by combining stimulus-driven information with endogenous, internally generated, inferential knowledge and meaning. Understanding language from speech is the human benchmark for this. Much research focuses on the purely stimulus-driven response, but here, we focus on the goal of language behavior: conveying structure and meaning. To that end, we use naturalistic stimuli that contrast acoustic-prosodic and lexical-semantic information to show that, during spoken language comprehension, oscillatory modulations reflect computations related to inferring structure and meaning from the acoustic signal. Our experiment provides the first evidence to date that compositional structure and meaning organize the oscillatory response, above and beyond prosodic and lexical controls.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2020 the authors. SfN exclusive license.},
  langid = {english},
  pmid = {33097640},
  keywords = {_tablet,combinatorial processing,lexical semantics,mutual information,neural oscillations,prosody,sentence comprehension},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KaufeldG/kaufeld_2020_linguistic_structure_and_meaning_organize_neural_oscillations_into_a.pdf;/Users/jonny/Zotero/storage/KYBEV7L5/9467.html;/Users/jonny/Zotero/storage/UF96VJJF/9467.html}
}

@article{kiefteAbsorptionReliableSpectral2008,
  title = {Absorption of Reliable Spectral Characteristics in Auditory Perception},
  author = {Kiefte, Michael and Kluender, Keith R.},
  year = {2008},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {123},
  number = {1},
  pages = {366--376},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.2804951},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KiefteM/kiefte_2008_absorption_of_reliable_spectral_characteristics_in_auditory_perception.pdf}
}

@article{kimLongTermOpticalAccess2016,
  title = {Long-{{Term Optical Access}} to an {{Estimated One Million Neurons}} in the {{Live Mouse Cortex}}},
  author = {Kim, Tony Hyun and Zhang, Yanping and Lecoq, J{\'e}r{\^o}me and Jung, Juergen C. and Li, Jane and Zeng, Hongkui and Niell, Cristopher M. and Schnitzer, Mark J.},
  year = {2016},
  month = dec,
  journal = {Cell Reports},
  volume = {17},
  number = {12},
  pages = {3385--3394},
  publisher = {{Elsevier}},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2016.12.004},
  langid = {english},
  pmid = {28009304},
  keywords = {_tablet,brain imaging,calcium imaging,dendrites,dendritic spines,fluorescence imaging,mouse behavior,neocortex,neural ensembles,two-photon microscopy},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KimT/kim_2016_long-term_optical_access_to_an_estimated_one_million_neurons_in_the_live_mouse.pdf;/Users/jonny/Zotero/storage/PRQFE8AT/S2211-1247(16)31676-X.html}
}

@article{kingRecentAdvancesUnderstanding2018a,
  title = {Recent Advances in Understanding the Auditory Cortex},
  author = {King, Andrew J. and Teki, Sundeep and Willmore, Ben D. B.},
  year = {2018},
  journal = {F1000Research},
  volume = {7},
  issn = {2046-1402},
  doi = {10.12688/f1000research.15580.1},
  abstract = {Our ability to make sense of the auditory world results from neural processing that begins in the ear, goes through multiple subcortical areas, and continues in the cortex. The specific contribution of the auditory cortex to this chain of processing is far from understood. Although many of the properties of neurons in the auditory cortex resemble those of subcortical neurons, they show somewhat more complex selectivity for sound features, which is likely to be important for the analysis of natural sounds, such as speech, in real-life listening conditions. Furthermore, recent work has shown that auditory cortical processing is highly context-dependent, integrates auditory inputs with other sensory and motor signals, depends on experience, and is shaped by cognitive demands, such as attention. Thus, in addition to being the locus for more complex sound selectivity, the auditory cortex is increasingly understood to be an integral part of the network of brain regions responsible for prediction, auditory perceptual decision-making, and learning. In this review, we focus on three key areas that are contributing to this understanding: the sound features that are preferentially represented by cortical neurons, the spatial organization of those preferences, and the cognitive roles of the auditory cortex.},
  langid = {english},
  pmcid = {PMC6173113},
  pmid = {30345008},
  keywords = {_tablet,auditory cortex,Auditory Cortex,Auditory Pathways,Auditory Perception,cognition,Humans,map,model,Neurons,plasticity,receptive field},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KingA/king_2018_recent_advances_in_understanding_the_auditory_cortex.pdf;/Users/jonny/Zotero/storage/GUZH62YW/king_2018_recent_advances_in_understanding_the_auditory_cortex.pdf}
}

@article{kleinschmidtRobustSpeechPerception2015a,
  title = {Robust Speech Perception: {{Recognize}} the Familiar, Generalize to the Similar, and Adapt to the Novel.},
  shorttitle = {Robust Speech Perception},
  author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
  year = {2015},
  month = apr,
  journal = {Psychological Review},
  volume = {122},
  number = {2},
  pages = {148--203},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0038695},
  abstract = {Successful speech perception requires that listeners map the acoustic signal to linguistic categories. These mappings are not only probabilistic, but change depending on the situation. For example, one talker's /p/ might be physically indistinguishable from another talker's /b/ (cf. lack of invariance). We characterize the computational problem posed by such a subjectively nonstationary world and propose that the speech perception system overcomes this challenge by (a) recognizing previously encountered situations, (b) generalizing to other situations based on previous similar experience, and (c) adapting to novel situations. We formalize this proposal in the ideal adapter framework: (a) to (c) can be understood as inference under uncertainty about the appropriate generative model for the current talker, thereby facilitating robust speech perception despite the lack of invariance. We focus on 2 critical aspects of the ideal adapter. First, in situations that clearly deviate from previous experience, listeners need to adapt. We develop a distributional (belief-updating) learning model of incremental adaptation. The model provides a good fit against known and novel phonetic adaptation data, including perceptual recalibration and selective adaptation. Second, robust speech recognition requires that listeners learn to represent the structured component of cross-situation variability in the speech signal. We discuss how these 2 aspects of the ideal adapter provide a unifying explanation for adaptation, talker-specificity, and generalization across talkers and groups of talkers (e.g., accents and dialects). The ideal adapter provides a guiding framework for future investigations into speech perception and adaptation, and more broadly language comprehension.},
  langid = {english},
  file = {/Users/jonny/Zotero/storage/ZD77KIYK/Kleinschmidt and Jaeger - 2015 - Robust speech perception Recognize the familiar, .pdf}
}

@article{Kluender2000,
  title = {Contributions of Nonhuman Animal Models to Understanding Human Speech Perception},
  author = {Kluender, Keith R.},
  year = {2000},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {107},
  number = {5},
  pages = {2835--2835},
  publisher = {{Acoustical Society of AmericaASA}},
  issn = {0001-4966},
  doi = {10.1121/1.429153},
  abstract = {Broadly speaking, nonhuman animal models contribute to understanding speech perception by humans in two ways\textemdash by analogy and by homology. The former is generally easier and examples are more abundant. Because demonstrating homology requires deeper explication of underlying mechanisms, claims can be more precarious but carry potentially greater explanatory payoff. When studying nonhuman organisms as an analogy, the emphasis is typically upon how animal physiological or behavioral processes have adapted to fulfill requirements of particular ecological niches. By contrast, study of animals as homology often violates ecology in search of common underlying processes, and the animal becomes a method more than an object of study. Examples of findings for animal analogies and homologies will be reviewed. Data will be presented from experiments in which nonhuman subjects play the role of homology in revealing both foundational sensory processes and more plastic processes of perceptual development. Animal models pro...},
  keywords = {\#nosource}
}

@article{kluenderLongstandingProblemsSpeech2019,
  ids = {kluenderLongstandingProblemsSpeech2019a},
  title = {Long-Standing Problems in Speech Perception Dissolve within an Information-Theoretic Perspective},
  author = {Kluender, Keith R. and Stilp, Christian E. and Lucas, Fernando Llanos},
  year = {2019},
  month = may,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {81},
  number = {4},
  pages = {861--883},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01702-x},
  abstract = {An information theoretic framework is proposed to have the potential to dissolve (rather than attempt to solve) multiple long-standing problems concerning speech perception. By this view, speech perception can be reframed as a series of processes through which sensitivity to information\textemdash that which changes and/or is unpredictable\textemdash becomes increasingly sophisticated and shaped by experience. Problems concerning appropriate objects of perception (gestures vs. sounds), rate normalization, variance consequent to articulation, and talker normalization are reframed, or even dissolved, within this information-theoretic framework. Application of discriminative models founded on information theory provides a productive approach to answer questions concerning perception of speech, and perception most broadly.},
  langid = {english},
  keywords = {_tablet,Perceptual learning,Psychoacoustics,Speech perception},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KluenderK/kluender_2019_long-standing_problems_in_speech_perception_dissolve_within_an.pdf;/Users/jonny/Papers/KluenderK/2019/Kluender_2019_Long-standing problems in speech perception dissolve within an.pdf;/Users/jonny/Papers/KluenderK/2019/Kluender_2019_Long-standing problems in speech perception dissolve within an2.pdf}
}

@incollection{kluenderPerceptionVowelSounds2013,
  title = {Perception of {{Vowel Sounds Within}} a {{Biologically Realistic Model}} of {{Efficient Coding}}},
  booktitle = {Vowel {{Inherent Spectral Change}}},
  author = {Kluender, Keith R. and Stilp, Christian E. and Kiefte, Michael},
  editor = {Morrison, Geoffrey Stewart and Assmann, Peter F.},
  year = {2013},
  pages = {117--151},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-14209-3_6},
  isbn = {978-3-642-14208-6 978-3-642-14209-3},
  langid = {english},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KluenderK/false}
}

@article{krantzSimilarityRectanglesAnalysis1975a,
  title = {Similarity of Rectangles: {{An}} Analysis of Subjective Dimensions},
  shorttitle = {Similarity of Rectangles},
  author = {Krantz, David H and Tversky, Amos},
  year = {1975},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  volume = {12},
  number = {1},
  pages = {4--34},
  issn = {00222496},
  doi = {10.1016/0022-2496(75)90047-4},
  langid = {english},
  file = {/Users/jonny/Zotero/storage/F7AEPRBV/Krantz and Tversky - 1975 - Similarity of rectangles An analysis of subjectiv.pdf}
}

@article{krantzSimilarityRectanglesAnalysis1975b,
  title = {Similarity of Rectangles: {{An}} Analysis of Subjective Dimensions},
  shorttitle = {Similarity of Rectangles},
  author = {Krantz, David H and Tversky, Amos},
  year = {1975},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  volume = {12},
  number = {1},
  pages = {4--34},
  issn = {0022-2496},
  doi = {10.1016/0022-2496(75)90047-4},
  abstract = {Two defining properties of psychological dimensions (intradimensional subtractivity and interdimensional additivity) are introduced and their consequences, formulated in terms of an ordinal dissimilarity scale, are derived. These consequences are investigated using dissimilarity judgments between rectangles to determine which of two alternative dimensional structures area (A) and shape (S), or width (W) and height (H), satisfies additivity and/or subtractivity. The results show that neither dimensional structure is acceptable, although A \texttimes{} S provides a better account for the data of most Ss than does W \texttimes{} H. Tests of relative straightness show that A is the least ``curved'' of the four attributes. Methodological and substantive implications of the study are discussed.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KrantzD/false;/Users/jonny/Zotero/storage/DUTC7NB5/krantz_1975_similarity_of_rectangles.pdf;/Users/jonny/Zotero/storage/LS8Z5QD8/krantz_1975_similarity_of_rectangles.pdf}
}

@article{Kronrod2016a,
  title = {A Unified Account of Categorical Effects in Phonetic Perception},
  author = {Kronrod, Yakov and Coppess, Emily and Feldman, Naomi H.},
  year = {2016},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {6},
  pages = {1681--1712},
  publisher = {{Springer US}},
  issn = {1069-9384},
  doi = {10.3758/s13423-016-1049-y},
  keywords = {_tablet,\#nosource},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KronrodY/kronrod_2016_a_unified_account_of_categorical_effects_in_phonetic_perception.pdf}
}

@article{kuhlBrainMechanismsEarly2010,
  title = {Brain {{Mechanisms}} in {{Early Language Acquisition}}},
  author = {Kuhl, Patricia K.},
  year = {2010},
  month = sep,
  journal = {Neuron},
  volume = {67},
  number = {5},
  pages = {713--727},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2010.08.038},
  abstract = {The last decade has produced an explosion in neuroscience research examining young children's early processing of language. Noninvasive, safe functional brain measurements have now been proven feasible for use with children starting at birth. The phonetic level of language is especially accessible to experimental studies that document the innate state and the effect of learning on the brain. The neural signatures of learning at the phonetic level can be documented at a remarkably early point in development. Continuity in linguistic development from infants' earliest brain responses to phonetic stimuli is reflected in their language and prereading abilities in the second, third, and fifth year of life, a finding with theoretical and clinical impact. There is evidence that early mastery of the phonetic units of language requires learning in a social context. Neuroscience on early language learning is beginning to reveal the multiple brain systems that underlie the human language faculty.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KuhlP/kuhl_2010_brain_mechanisms_in_early_language_acquisition.pdf;/Users/jonny/Zotero/storage/74ZYTQ2C/kuhl_2010_brain_mechanisms_in_early_language_acquisition.pdf}
}

@article{kuhlEarlyLanguageAcquisition2004,
  title = {Early Language Acquisition: Cracking the Speech Code},
  shorttitle = {Early Language Acquisition},
  author = {Kuhl, Patricia K.},
  year = {2004},
  month = nov,
  journal = {Nature Reviews Neuroscience},
  volume = {5},
  number = {11},
  pages = {831--843},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn1533},
  abstract = {Infants learn their native language quickly and effortlessly, and follow the same developmental path regardless of culture. However, it has proved difficult to understand how they do this, or to build computers that can reproduce this feat.An early and essential task for infants is to make sense of the speech that they hear. Each language uses a unique set of about 40 phonemes, and infants must learn to partition varied speech sounds into these phonemic categories. Young infants are sensitive to subtle differences between all phonetic units, whereas older children lose their sensitivity to distinctions that are not used in their native language. The loss of discrimination for foreign-language distinctions is paralleled by an increase in sensitivity to native-language phonetic units.There is evidence that infants analyse the statistical distributions of sounds that they hear in ambient language, and use this information to form phonemic categories. They also learn phonotactic rules \textemdash{} language-specific rules that govern the sequences of phonemes that can be used to compose words.To identify word boundaries, infants can use both transitional probabilities between syllables, and prosodic cues, which relate to linguistic stress. Most languages are dominated by either trochaic words (with the stress on the first syllable) or iambic ones (with the stress on later syllables). Infants seem to use a combination of statistical and prosodic cues to segment words in speech.Social influences are important in speech learning. Infants learn more easily from interactions with human beings speaking another language than they do from audiovisual exposure to the same language material, and their speech is strongly influenced by the response of others around them, such as their mothers. The importance of social input in language learning has some similarities to social influences on song learning in birds.Language experience causes neural changes. One hypothesis, native language neural commitment (NLNC), proposes that language learning produces dedicated neural networks that code the patterns of native-language speech. As these networks develop, they make it easier for new speech elements and patterns to be learned if they are consistent with the existing patterns, but place constraints on the learning of foreign-language patterns. NLNC might explain the closing of the 'sensitive period' for language learning; once a certain amount of learning has occurred, neural commitment interferes with the learning of new languages so they cannot be learned as easily.},
  copyright = {2004 Nature Publishing Group},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KuhlP/kuhl_2004_early_language_acquisition.pdf}
}

@article{kuhlNewViewLanguage2000,
  title = {A New View of Language Acquisition},
  author = {Kuhl, Patricia K.},
  year = {2000},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {97},
  number = {22},
  pages = {11850--11857},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.97.22.11850},
  abstract = {At the forefront of debates on language are new data demonstrating infants' early acquisition of information about their native language. The data show that infants perceptually ``map'' critical aspects of ambient language in the first year of life before they can speak. Statistical properties of speech are picked up through exposure to ambient language. Moreover, linguistic experience alters infants' perception of speech, warping perception in the service of language. Infants' strategies are unexpected and unpredicted by historical views. A new theoretical position has emerged, and six postulates of this position are described.},
  copyright = {Copyright \textcopyright{} 2000, The National Academy of Sciences},
  langid = {english},
  pmid = {11050219},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KuhlP/kuhl_2000_a_new_view_of_language_acquisition.pdf;/Users/jonny/Papers/KuhlP/2000/Kuhl_2000_A new view of language acquisition.pdf;/Users/jonny/Zotero/storage/S9RMDR2I/11850.html}
}

@article{kuhlPhoneticLearningPathway2008,
  title = {Phonetic Learning as a Pathway to Language: New Data and Native Language Magnet Theory Expanded ({{NLM-e}})},
  shorttitle = {Phonetic Learning as a Pathway to Language},
  author = {Kuhl, Patricia K and Conboy, Barbara T and {Coffey-Corina}, Sharon and Padden, Denise and {Rivera-Gaxiola}, Maritza and Nelson, Tobey},
  year = {2008},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {363},
  number = {1493},
  pages = {979--1000},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2007.2154},
  abstract = {Infants' speech perception skills show a dual change towards the end of the first year of life. Not only does non-native speech perception decline, as often shown, but native language speech perception skills show improvement, reflecting a facilitative effect of experience with native language. The mechanism underlying change at this point in development, and the relationship between the change in native and non-native speech perception, is of theoretical interest. As shown in new data presented here, at the cusp of this developmental change, infants' native and non-native phonetic perception skills predict later language ability, but in opposite directions. Better native language skill at 7.5 months of age predicts faster language advancement, whereas better non-native language skill predicts slower advancement. We suggest that native language phonetic performance is indicative of neural commitment to the native language, while non-native phonetic performance reveals uncommitted neural circuitry. This paper has three goals: (i) to review existing models of phonetic perception development, (ii) to present new event-related potential data showing that native and non-native phonetic perception at 7.5 months of age predicts language growth over the next 2 years, and (iii) to describe a revised version of our previous model, the native language magnet model, expanded (NLM-e). NLM-e incorporates five new principles. Specific testable predictions for future research programmes are described.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KuhlP/kuhl_2008_phonetic_learning_as_a_pathway_to_language.pdf;/Users/jonny/Zotero/storage/BRFFX43H/kuhl_2008_phonetic_learning_as_a_pathway_to_language.pdf}
}

@article{kuhlTheoreticalContributionsTests1986,
  title = {Theoretical Contributions of Tests on Animals to the Special-Mechanisms Debate in Speech},
  author = {Kuhl, P. K.},
  year = {1986},
  journal = {Experimental Biology},
  volume = {45},
  number = {3},
  pages = {233--265},
  issn = {0176-8638},
  abstract = {Many animal species demonstrate a keen sensitivity very early in life to stimuli that play a role in their survival. Theorists have taken this to mean that species-specific mechanisms evolved in animals that aid in the detection and recognition of important stimuli. Similar arguments have been made about the mechanisms that underlie the perception of speech in humans. Theories of speech perception present convincing arguments that even the phonetic level of language requires specially evolved mechanisms, because of the extreme complexity involved in the mapping between sound and percept. Phenomena such as categorical perception have been attributed to the workings of these mechanisms. This paper lays out an argument concerning the contribution of animal studies on categorical perception to the special-mechanisms debate. Animals provide a model of auditory-level processing in the absence of phonetic-level processing, and test whether the existence of a phenomenon such as categorical perception necessitates specialized mechanisms. A review of the studies shows that animal demonstrate categorical perception of the voicing and place features. These data, as well as some recent findings on young human infants, are considered with regard to their impact on theories of infant speech perception and on the evolution of speech.},
  langid = {english},
  pmid = {3525222},
  keywords = {Adult,Animals,Auditory Perception,Chinchilla,Computers,Discrimination Learning,Humans,Infant,Language Development,Macaca,Models; Biological,Phonetics,Speech,Speech Perception,Visual Perception,Voice}
}

@article{leaUseMultipleDimensions2008,
  title = {Use of Multiple Dimensions in Learned Discriminations},
  author = {Lea, Stephen E. G. and Wills, A. J.},
  year = {2008},
  journal = {Comparative Cognition \& Behavior Reviews},
  volume = {3},
  issn = {19114745},
  doi = {10.3819/ccbr.2008.30007},
  abstract = {Many naturally occurring categories vary across multiple stimulus dimensions (e.g. size, color, texture). When humans categorize multidimensional stimuli on the basis of a single dimension this has been taken to indicate use of a rule that could be verbalized. Sorting on the basis of all the stimulus dimensions (`overall similarity' or `family resemblance') has been taken to indicate a more basic, implicit, automatic, perhaps associative process. However, a review of the literature on animal discrimination learning shows that animals often discriminate on the basis of one dominant dimension. In recent experiments, situations conducive to more complex cognitive processes have increased family resemblance sorting in humans. In an effort to resolve this apparent paradox, experiments were conducted in which humans and pigeons were exposed to multidimensional category discrimination tasks under closely similar conditions. Preliminary results show no evidence that even a non-verbal rule can be said to be involved in pigeons' choices in these conditions, despite the fact that under some conditions a single dimension may dominate their behavior.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LeaS/lea_2008_use_of_multiple_dimensions_in_learned_discriminations.pdf}
}

@article{leeExtendingALCOVEModel2002,
  title = {Extending the {{ALCOVE}} Model of Category Learning to Featural Stimulus Domains},
  author = {Lee, Michael D. and Navarro, Daniel J.},
  year = {2002},
  month = mar,
  journal = {Psychonomic Bulletin \& Review},
  volume = {9},
  number = {1},
  pages = {43--58},
  issn = {1531-5320},
  doi = {10.3758/BF03196256},
  abstract = {The ALCOVE model of category learning, despite its considerable success in accounting for human performance across a wide range of empirical tasks, is limited by its reliance on spatial stimulus representations. Some stimulus domains are better suited to featural representation, characterizing stimuli in terms of the presence or absence of discrete features, rather than as points in a multidimensional space. We report on empirical data measuring human categorization performance across a featural stimulus domain and show that ALCOVE is unable to capture fundamental qualitative aspects of this performance. In response, a featural version of the ALCOVE model is developed, replacing the spatial stimulus representations that are usually generated by multidimensional scaling with featural representations generated by additive clustering. We demonstrate that this featural version of ALCOVE is able to capture human performance where the spatial model failed, explaining the difference in terms of the contrasting representational assumptions made by the two approaches. Finally, we discuss ways in which the ALCOVE categorization model might be extended further to use ``hybrid'' representational structures combining spatial and featural components.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LeeM/lee_2002_extending_the_alcove_model_of_category_learning_to_featural_stimulus_domains.pdf;/Users/jonny/Zotero/storage/UIRF5I5B/lee_2002_extending_the_alcove_model_of_category_learning_to_featural_stimulus_domains.pdf}
}

@article{levyCircuitAsymmetriesUnderlie2019a,
  title = {Circuit Asymmetries Underlie Functional Lateralization in the Mouse Auditory Cortex},
  author = {Levy, Robert B. and Marquarding, Tiemo and Reid, Ashlan P. and Pun, Christopher M. and Renier, Nicolas and Oviedo, Hysell V.},
  year = {2019},
  month = jun,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {2783},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10690-3},
  abstract = {The left hemisphere's dominance in processing social communication has been known for over a century, but the mechanisms underlying this lateralized cortical function are poorly understood. Here, we compare the structure, function, and development of each auditory cortex (ACx) in the mouse to look for specializations that may underlie lateralization. Using Fos brain volume imaging, we found greater activation in the left ACx in response to vocalizations, while the right ACx responded more to frequency sweeps. In vivo recordings identified hemispheric differences in spectrotemporal selectivity, reinforcing their functional differences. We then compared the synaptic connectivity within each hemisphere and discovered lateralized circuit-motifs that are hearing experience-dependent. Our results suggest a specialist role for the left ACx, focused on facilitating the detection of specific vocalization features, while the right ACx is a generalist with the ability to integrate spectrotemporal features more broadly.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LevyR/levy_2019_circuit_asymmetries_underlie_functional_lateralization_in_the_mouse_auditory.pdf;/Users/jonny/Zotero/storage/6BGBRVP3/s41467-019-10690-3.html}
}

@article{Liberman1985a,
  title = {The Motor Theory of Speech Perception Revised},
  author = {Liberman, Alvin M. and Mattingly, Ignatius G.},
  year = {1985},
  journal = {Cognition},
  volume = {21},
  number = {1},
  pages = {1--36},
  issn = {00100277},
  doi = {10.1016/0010-0277(85)90021-6},
  abstract = {A motor theory of speech perception, initially proposed to account for results of early experiments with synthetic speech, is now extensively revised to accommodate recent findings, and to relate the assumptions of the theory to those that might be made about other perceptual modes. According to the revised theory, phonetic information is perceived in a biologically distinct system, a `module' specialized to detect the intended gestures of the speaker that are the basis for phonetic categories. Built into the structure of this module is the unique but lawful relationship between the gestures and the acoustic patterns in which they are variously overlapped. In consequence, the module causes perception of phonetic structure without translation from preliminary auditory impressions. Thus, it is comparable to such other modules as the one that enables an animal to localize sound. Peculiar to the phonetic module are the relation between perception and production it incorporates and the fact that it must compete with other modules for the same stimulus variations. Une th\'eorie motrice de la perception propos\'ee initialement pour rendre compte des r\'esultats des premi\`eres exp\'eriences avec de la parole synth\'etique a \'et\'e largement r\'evis\'ee afin d'interpr\'eter les donn\'ees r\'ecentes et de relier les propositions de cette th\'eorie \`a celles que l'on peut faire pour d'autres modalit\'es de perception. La r\'evision de cette th\'eorie stipule que l'information phon\'etique est fournie par un syst\`eme biologique distinct, un `module' sp\'ecialis\'e pour d\'etecter les gestes que le locuteur a eu l'intention de faire: ces gestes fondent les cat\'egories phon\'etiques. La relation entre les gestes et les patterns acoustiques dans lesquels ceux-ci sont imbriqu\'es de facon vari\'ee est unique mais r\'egul\'ee. Cette relation est construite dans la structure du module. En cons\'equence le module provoque la perception de la structure phon\'etique sans traduction \`a partir d'impressions auditives pr\'eliminaires. Ce module est ainsi comparable \`a d'autres modules tels que celui qui permet \`a l'animal de localiser les sons. La particularit\'e de ce module tient \`a la relation entre perception et production qu'il incorpore et an fait qu'il doit rivaliser avec d'autres modules pour de m\^emes variations de stimulus.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LibermanA/liberman_1985_the_motor_theory_of_speech_perception_revised.pdf;/Users/jonny/Zotero/storage/GHZFWMHT/Liberman, Mattingly - 1985 - The motor theory of speech perception revised(3).pdf}
}

@article{libermanCharacteristicsPerceptionSpeech1970,
  title = {Some Characteristics of Perception in the Speech Mode},
  author = {Liberman, A. M.},
  year = {1970},
  journal = {Research Publications - Association for Research in Nervous and Mental Disease},
  volume = {48},
  pages = {238--254},
  issn = {0091-7443},
  langid = {english},
  pmid = {5458835},
  keywords = {Auditory Perception,Cues,Discrimination Learning,Humans,Language,Phonetics,Semantics,Sound,Speech}
}

@article{limHowMayBasal2014,
  title = {How May the Basal Ganglia Contribute to Auditory Categorization and Speech Perception?},
  author = {Lim, Sung-Joo and Fiez, Julie A. and Holt, Lori L.},
  year = {2014},
  journal = {Frontiers in Neuroscience},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2014.00230},
  abstract = {Listeners must accomplish two complementary perceptual feats in extracting a message from speech. They must discriminate linguistically-relevant acoustic variability and generalize across irrelevant variability. Said another way, they must categorize speech. Since the mapping of acoustic variability is language-specific, these categories must be learned from experience. Thus, understanding how, in general, the auditory system acquires and represents categories can inform us about the toolbox of mechanisms available to speech perception. This perspective invites consideration of findings from cognitive neuroscience literatures outside of the speech domain as a means of constraining models of speech perception. Although neurobiological models of speech perception have mainly focused on cerebral cortex, research outside the speech domain is consistent with the possibility of significant subcortical contributions in category learning. Here, we review the functional role of one such structure, the basal ganglia. We examine research from animal electrophysiology, human neuroimaging, and behavior to consider characteristics of basal ganglia processing that may be advantageous for speech category learning. We also present emerging evidence for a direct role for basal ganglia in learning auditory categories in a complex, naturalistic task intended to model the incidental manner in which speech categories are acquired. To conclude, we highlight new research questions that arise in incorporating the broader neuroscience research literature in modeling speech perception, and suggest how understanding contributions of the basal ganglia can inform attempts to optimize training protocols for learning non-native speech categories in adulthood.},
  langid = {english},
  keywords = {_tablet,Basal Ganglia,Categorization,Perceptual Learning,plasticity,speech category learning,Speech Perception},
  file = {/Users/jonny/Zotero/storage/778B42T6/lim_2014_how_may_the_basal_ganglia_contribute_to_auditory_categorization_and_speech.pdf;/Users/jonny/Zotero/storage/9Z7IAKA2/lim_2014_how_may_the_basal_ganglia_contribute_to_auditory_categorization_and_speech.pdf}
}

@article{limHowMayBasal2014a,
  title = {How May the Basal Ganglia Contribute to Auditory Categorization and Speech Perception?},
  author = {Lim, Sung-Joo and Fiez, Julie A. and Holt, Lori L.},
  year = {2014},
  journal = {Frontiers in Neuroscience},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2014.00230},
  abstract = {Listeners must accomplish two complementary perceptual feats in extracting a message from speech. They must discriminate linguistically-relevant acoustic variability and generalize across irrelevant variability. Said another way, they must categorize speech. Since the mapping of acoustic variability is language-specific, these categories must be learned from experience. Thus, understanding how, in general, the auditory system acquires and represents categories can inform us about the toolbox of mechanisms available to speech perception. This perspective invites consideration of findings from cognitive neuroscience literatures outside of the speech domain as a means of constraining models of speech perception. Although neurobiological models of speech perception have mainly focused on cerebral cortex, research outside the speech domain is consistent with the possibility of significant subcortical contributions in category learning. Here, we review the functional role of one such structure, the basal ganglia. We examine research from animal electrophysiology, human neuroimaging, and behavior to consider characteristics of basal ganglia processing that may be advantageous for speech category learning. We also present emerging evidence for a direct role for basal ganglia in learning auditory categories in a complex, naturalistic task intended to model the incidental manner in which speech categories are acquired. To conclude, we highlight new research questions that arise in incorporating the broader neuroscience research literature in modeling speech perception, and suggest how understanding contributions of the basal ganglia can inform attempts to optimize training protocols for learning non-native speech categories in adulthood.},
  langid = {english},
  keywords = {_tablet,Basal Ganglia,Categorization,Perceptual Learning,plasticity,speech category learning,Speech Perception},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LimS/lim_2014_how_may_the_basal_ganglia_contribute_to_auditory_categorization_and_speech.pdf;/Users/jonny/Zotero/storage/MVIJM2DP/lim_2014_how_may_the_basal_ganglia_contribute_to_auditory_categorization_and_speech.pdf}
}

@article{limInferringLearningRules2015,
  title = {Inferring Learning Rules from Distributions of Firing Rates in Cortical Neurons},
  author = {Lim, Sukbin and McKee, Jillian L. and Woloszyn, Luke and Amit, Yali and Freedman, David J. and Sheinberg, David L. and Brunel, Nicolas},
  year = {2015},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {18},
  number = {12},
  pages = {1804--1810},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4158},
  abstract = {Experience-dependent synaptic modifications are one of the fundamental mechanisms of learning and memory, yet they are difficult to measure in vivo. Here the authors introduce a network model\textendash based method that infers synaptic plasticity rules from the analysis of the statistics of neuronal responses to novel versus familiar stimuli.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LimS/lim_2015_inferring_learning_rules_from_distributions_of_firing_rates_in_cortical_neurons.pdf;/Users/jonny/Zotero/storage/XHSDR78C/nn.html}
}

@article{lindauStory1980,
  title = {The Story of /r/},
  author = {Lindau, Mona},
  year = {1980},
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {67},
  number = {S1},
  pages = {S27-S27},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.2018134},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LindauM/lindau_1980_the_story_of_-r.pdf;/Users/jonny/Zotero/storage/73D429D2/lindau_1980_the_story_of_-r.pdf}
}

@article{lindenSpectrotemporalStructureReceptive2003b,
  title = {Spectrotemporal {{Structure}} of {{Receptive Fields}} in {{Areas AI}} and {{AAF}} of {{Mouse Auditory Cortex}}},
  author = {Linden, Jennifer F. and Liu, Robert C. and Sahani, Maneesh and Schreiner, Christoph E. and Merzenich, Michael M.},
  year = {2003},
  month = oct,
  journal = {Journal of Neurophysiology},
  volume = {90},
  number = {4},
  pages = {2660--2675},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.00751.2002},
  abstract = {The mouse is a promising model system for auditory cortex research because of the powerful genetic tools available for manipulating its neural circuitry. Previous studies have identified two tonotopic auditory areas in the mouse\textemdash primary auditory cortex (AI) and anterior auditory field (AAF)\textemdash{} but auditory receptive fields in these areas have not yet been described. To establish a foundation for investigating auditory cortical circuitry and plasticity in the mouse, we characterized receptive-field structure in AI and AAF of anesthetized mice using spectrally complex and temporally dynamic stimuli as well as simple tonal stimuli. Spectrotemporal receptive fields (STRFs) were derived from extracellularly recorded responses to complex stimuli, and frequency-intensity tuning curves were constructed from responses to simple tonal stimuli. Both analyses revealed temporal differences between AI and AAF responses: peak latencies and receptive-field durations for STRFs and first-spike latencies for responses to tone bursts were significantly longer in AI than in AAF. Spectral properties of AI and AAF receptive fields were more similar, although STRF bandwidths were slightly broader in AI than in AAF. Finally, in both AI and AAF, a substantial minority of STRFs were spectrotemporally inseparable. The spectrotemporal interaction typically appeared in the form of clearly disjoint excitatory and inhibitory subfields or an obvious spectrotemporal slant in the STRF. These data provide the first detailed description of auditory receptive fields in the mouse and suggest that although neurons in areas AI and AAF share many response characteristics, area AAF may be specialized for faster temporal processing.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LindenJ/linden_2003_spectrotemporal_structure_of_receptive_fields_in_areas_ai_and_aaf_of_mouse.pdf;/Users/jonny/Zotero/storage/52QT2WBF/linden_2003_spectrotemporal_structure_of_receptive_fields_in_areas_ai_and_aaf_of_mouse.pdf}
}

@article{Lisker1977,
  title = {Rapid versus Rabid: {{A}} Catalogue of Acoustic Features That May Cue the Distinction},
  author = {Lisker, Leigh},
  year = {1977},
  journal = {The Journal of the Acoustical Society of America},
  volume = {62},
  number = {S1},
  pages = {S77},
  issn = {00014966},
  doi = {10.1121/1.2016377},
  abstract = {InAmerican English, initial /bdg/ often lack the acoustic feature takenas the defining feature of voiced stops; intervocalically before unstressedvowel /ptk/ lack aspiration, without which initial stops are notlabeled /ptk/. Initially, the two categories differ in the timingof vocal fold adduction and onset of fold vibration, andseveral acoustic cues, all tied to the VOT difference, havebeen studied. Medially there is also a difference in themanagement of the larynx, though it results in a phoneticallysimpler contrast, one of voicing with no accompanying aspiration difference.Acoustically, however, the list of features that play, or mightplausibly play a role is quite large. The word pairrapid-rabid, for example, might be affected by the following: (1)presence/absence of low-frequency buzz during the closure interval, (2) durationof closure, (3) F1 offset frequency before closure, (4) F1offset transition duration, (5) F1 onset frequency following closure, (6)F1 onset transition duration, (7) \ae{} duration, (8) F1 cut-backbefore closure, (9) F1 cutback following closure, (10) VOT cutbackbefore closure, (11) VOT delay after closure, (12) F0 contourbefore closure, (13) F0 contour after closure, (14) amplitude ofi relative to \ae, (15) decay time of glottal signalpreceding closure, (16) intensity of burst following closure. Even ifsome of these should turn out to be perceptually negligible,enough of them surely have cue value to make ita formidable task to justify preferring an acoustic to anarticulatory account of the distinction between the two English words.The support of the National Institute of Child Health andHuman Development is gratefully acknowledged. 1977 Acoustical Society of America},
  isbn = {0001-4966},
  keywords = {\#nosource}
}

@article{liuIsotopyEnergyPhysical2020,
  title = {Isotopy and Energy of Physical Networks},
  author = {Liu, Yanchen and Dehmamy, Nima and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2020},
  month = oct,
  journal = {Nature Physics},
  issn = {1745-2473, 1745-2481},
  doi = {10.1038/s41567-020-1029-z},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LiuY/liu_2020_isotopy_and_energy_of_physical_networks.pdf}
}

@article{liuOptimalFeaturesAuditory2019,
  title = {Optimal Features for Auditory Categorization},
  author = {Liu, Shi Tong and {Montes-Lourido}, Pilar and Wang, Xiaoqin and Sadagopan, Srivatsun},
  year = {2019},
  month = mar,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1302},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09115-y},
  abstract = {Humans and vocal animals use vocalizations to communicate with members of their species. A necessary function of auditory perception is to generalize across the high variability inherent in vocalization production and classify them into behaviorally distinct categories (`words' or `call types'). Here, we demonstrate that detecting mid-level features in calls achieves production-invariant classification. Starting from randomly chosen marmoset call features, we use a greedy search algorithm to determine the most informative and least redundant features necessary for call classification. High classification performance is achieved using only 10\textendash 20 features per call type. Predictions of tuning properties of putative feature-selective neurons accurately match some observed auditory cortical responses. This feature-based approach also succeeds for call categorization in other species, and for other complex classification tasks such as caller identification. Our results suggest that high-level neural representations of sounds are based on task-dependent features optimized for specific computational goals.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LiuS/liu_2019_optimal_features_for_auditory_categorization.pdf;/Users/jonny/Zotero/storage/Z8JR9GJK/liu_2019_optimal_features_for_auditory_categorization.pdf}
}

@article{liuParallelProcessingSound2019,
  title = {Parallel {{Processing}} of {{Sound Dynamics}} across {{Mouse Auditory Cortex}} via {{Spatially Patterned Thalamic Inputs}} and {{Distinct Areal Intracortical Circuits}}},
  author = {Liu, Ji and Whiteway, Matthew R. and Sheikhattar, Alireza and Butts, Daniel A. and Babadi, Behtash and Kanold, Patrick O.},
  year = {2019},
  month = apr,
  journal = {Cell Reports},
  volume = {27},
  number = {3},
  pages = {872-885.e7},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2019.03.069},
  abstract = {Natural sounds have rich spectrotemporal dynamics. Spectral information is spatially represented in the auditory cortex (ACX) via large-scale maps. However, the representation of temporal information, e.g., sound offset, is unclear. We perform multiscale imaging of neuronal and thalamic activity evoked by sound onset and offset in awake mouse ACX. ACX areas differed in onset responses (On-Rs) and offset responses (Off-Rs). Most excitatory L2/3 neurons show either On-Rs or Off-Rs, and ACX areas are characterized by differing fractions of On and Off-R neurons. Somatostatin and parvalbumin interneurons show distinct temporal dynamics, potentially amplifying Off-Rs. Functional network analysis shows that ACX areas contain distinct parallel onset and offset networks. Thalamic (MGB) terminals show either On-Rs or Off-Rs, indicating a thalamic origin of On and Off-R pathways. Thus, ACX areas spatially represent temporal features, and this representation is created by spatial~convergence and co-activation of distinct MGB inputs and is refined by specific intracortical connectivity.},
  langid = {english},
  keywords = {_tablet,auditory cortex,MGB,mouse,pattern,temporal,two-photon imaging},
  file = {/Users/jonny/Zotero/storage/6FHJXNLN/liu_2019_parallel_processing_of_sound_dynamics_across_mouse_auditory_cortex_via.pdf}
}

@article{Lotto1997; https://web.archive.org/web/20211013212325/https://www.researchgate.net/publication/237280984_Animal_Models_of_Speech_Perception_Phenomena,
  title = {Animal Models of Speech Perception Phenomena},
  author = {Lotto, AJ and Kluender, KR and Holt, LL},
  year = {1997},
  journal = {Chicago Linguistic Society},
  keywords = {\#nosource,archived},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LottoA/lotto_1997_animal_models_of_speech_perception_phenomena.pdf}
}

@article{macellaioWhySensoryNeurons2020,
  title = {Why Sensory Neurons Are Tuned to Multiple Stimulus Features},
  author = {Macellaio, Matthew V. and Liu, Bing and Beck, Jeffrey M. and Osborne, Leslie C.},
  year = {2020},
  month = dec,
  journal = {bioRxiv},
  pages = {2020.12.29.424235},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.29.424235},
  abstract = {{$<$}p{$>$}Many sensory neurons encode information about more than one stimulus feature. Multidimensional tuning increases ambiguity in stimulus-response relationships, but we find that it also offers an unexpected computational advantage, allowing the brain to better reconstruct sensory stimuli. From the responses of sensory neurons, populations, and sensory-driven movement behavior, more information can be recovered about a stimulus vector than about its individual components. We term this coding advantage 9stimulus synergy9 and show that it is distinct from other coding synergies, arising from inseparability of the response- conditioned stimulus distribution along individual stimulus dimensions. From extracellular recordings in motion sensitive cortex and measurements of pursuit eye movements, we demonstrate that stimulus synergy in cortical populations is preserved downstream in the precision of pursuit, and that a common decoding model predicts the level of synergy in pursuit behavior. This suggests that the brain exploits the information advantage afforded by multidimensional sensory tuning.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/M/MacellaioM/macellaio_2020_why_sensory_neurons_are_tuned_to_multiple_stimulus_features.pdf;/Users/jonny/Zotero/storage/6GYX9NLQ/2020.12.29.html}
}

@article{mastrogiuseppeIntrinsicallygeneratedFluctuatingActivity2017,
  title = {Intrinsically-Generated Fluctuating Activity in Excitatory-Inhibitory Networks},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2017},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {4},
  pages = {e1005498},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005498},
  abstract = {Recurrent networks of non-linear units display a variety of dynamical regimes depending on the structure of their synaptic connectivity. A particularly remarkable phenomenon is the appearance of strongly fluctuating, chaotic activity in networks of deterministic, but randomly connected rate units. How this type of intrinsically generated fluctuations appears in more realistic networks of spiking neurons has been a long standing question. To ease the comparison between rate and spiking networks, recent works investigated the dynamical regimes of randomly-connected rate networks with segregated excitatory and inhibitory populations, and firing rates constrained to be positive. These works derived general dynamical mean field (DMF) equations describing the fluctuating dynamics, but solved these equations only in the case of purely inhibitory networks. Using a simplified excitatory-inhibitory architecture in which DMF equations are more easily tractable, here we show that the presence of excitation qualitatively modifies the fluctuating activity compared to purely inhibitory networks. In presence of excitation, intrinsically generated fluctuations induce a strong increase in mean firing rates, a phenomenon that is much weaker in purely inhibitory networks. Excitation moreover induces two different fluctuating regimes: for moderate overall coupling, recurrent inhibition is sufficient to stabilize fluctuations; for strong coupling, firing rates are stabilized solely by the upper bound imposed on activity, even if inhibition is stronger than excitation. These results extend to more general network architectures, and to rate networks receiving noisy inputs mimicking spiking activity. Finally, we show that signatures of the second dynamical regime appear in networks of integrate-and-fire neurons.},
  langid = {english},
  keywords = {_tablet,Action potentials,Autocorrelation,Dynamical systems,Eigenvalues,Neural networks,Neurons,Transfer functions,White noise},
  file = {/Users/jonny/Zotero/storage/BMMTU3RJ/mastrogiuseppe_2017_intrinsically-generated_fluctuating_activity_in_excitatory-inhibitory_networks.pdf;/Users/jonny/Zotero/storage/UQLLHABJ/mastrogiuseppe_2017_intrinsically-generated_fluctuating_activity_in_excitatory-inhibitory_networks.pdf}
}

@article{mastrogiuseppeLinkingConnectivityDynamics2018,
  title = {Linking {{Connectivity}}, {{Dynamics}}, and {{Computations}} in {{Low-Rank Recurrent Neural Networks}}},
  author = {Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  year = {2018},
  month = aug,
  journal = {Neuron},
  volume = {99},
  number = {3},
  pages = {609-623.e29},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.07.003},
  abstract = {Large-scale neural recordings have established that the transformation of sensory stimuli into motor outputs relies on low-dimensional dynamics at the population level, while individual neurons exhibit complex selectivity. Understanding how low-dimensional computations on mixed, distributed representations emerge from the structure of the recurrent connectivity and inputs to cortical networks is a major challenge. Here, we study a class of recurrent network models in which the connectivity is a sum of a random part and a minimal, low-dimensional structure. We show that, in such networks, the dynamics are low dimensional and can be directly inferred from connectivity using a geometrical approach. We exploit this understanding to determine minimal connectivity required to implement specific computations and find that the dynamical range and computational capacity quickly increase with the dimensionality of the connectivity structure. This framework produces testable experimental predictions for the relationship between connectivity, low-dimensional dynamics, and computational features of recorded neurons.},
  langid = {english},
  keywords = {_tablet,low dimensional dynamics,mixed selectivity,neural computations,recurrent neural networks},
  file = {/Users/jonny/Dropbox/papers/zotero/M/MastrogiuseppeF/mastrogiuseppe_2018_linking_connectivity,_dynamics,_and_computations_in_low-rank_recurrent_neural.pdf;/Users/jonny/Zotero/storage/UMSEGU9E/S0896627318305439.html}
}

@article{mesgaraniMechanismsNoiseRobust2014,
  title = {Mechanisms of Noise Robust Representation of Speech in Primary Auditory Cortex},
  author = {Mesgarani, Nima and David, Stephen V. and Fritz, Jonathan B. and Shamma, Shihab A.},
  year = {2014},
  month = may,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {111},
  number = {18},
  pages = {6792--6797},
  issn = {1091-6490},
  doi = {10.1073/pnas.1318017111},
  abstract = {Humans and animals can reliably perceive behaviorally relevant sounds in noisy and reverberant environments, yet the neural mechanisms behind this phenomenon are largely unknown. To understand how neural circuits represent degraded auditory stimuli with additive and reverberant distortions, we compared single-neuron responses in ferret primary auditory cortex to speech and vocalizations in four conditions: clean, additive white and pink (1/f) noise, and reverberation. Despite substantial distortion, responses of neurons to the vocalization signal remained stable, maintaining the same statistical distribution in all conditions. Stimulus spectrograms reconstructed from population responses to the distorted stimuli resembled more the original clean than the distorted signals. To explore mechanisms contributing to this robustness, we simulated neural responses using several spectrotemporal receptive field models that incorporated either a static nonlinearity or subtractive synaptic depression and multiplicative gain normalization. The static model failed to suppress the distortions. A dynamic model incorporating feed-forward synaptic depression could account for the reduction of additive noise, but only the combined model with feedback gain normalization was able to predict the effects across both additive and reverberant conditions. Thus, both mechanisms can contribute to the abilities of humans and animals to extract relevant sounds in diverse noisy environments.},
  langid = {english},
  pmcid = {PMC4020083},
  pmid = {24753585},
  keywords = {_tablet,Acoustic Stimulation,Animals,Auditory Cortex,cortical,Female,Ferrets,hearing,Humans,Models; Neurological,Neurons,Noise,Nonlinear Dynamics,phonemes,population code,Speech Perception,Vocalization; Animal},
  file = {/Users/jonny/Dropbox/papers/zotero/M/MesgaraniN/mesgarani_2014_mechanisms_of_noise_robust_representation_of_speech_in_primary_auditory_cortex.pdf;/Users/jonny/Zotero/storage/5SBJHPN5/mesgarani_2014_mechanisms_of_noise_robust_representation_of_speech_in_primary_auditory_cortex.pdf}
}

@article{mesgaraniPhoneticFeatureEncoding2014,
  title = {Phonetic {{Feature Encoding}} in {{Human Superior Temporal Gyrus}}},
  author = {Mesgarani, N. and Cheung, C. and Johnson, K. and Chang, E. F.},
  year = {2014},
  month = feb,
  journal = {Science},
  volume = {343},
  number = {6174},
  pages = {1006--1010},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1245994},
  langid = {english},
  file = {/Users/jonny/Dropbox/papers/zotero/M/MesgaraniN/false}
}

@article{metherateSpectralIntegrationAuditory2005,
  title = {Spectral Integration in Auditory Cortex: Mechanisms and Modulation},
  shorttitle = {Spectral Integration in Auditory Cortex},
  author = {Metherate, Raju and Kaur, Simranjit and Kawai, Hideki and Lazar, Ronit and Liang, Kevin and Rose, Heather J.},
  year = {2005},
  month = aug,
  journal = {Hearing Research},
  volume = {206},
  number = {1-2},
  pages = {146--158},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2005.01.014},
  abstract = {Auditory cortex contributes to the processing and perception of spectrotemporally complex stimuli. However, the mechanisms by which this is accomplished are not well understood. In this review, we examine evidence that single cortical neurons receive input covering much of the audible spectrum. We then propose an anatomical framework by which spectral information converges on single neurons in primary auditory cortex, via a combination of thalamocortical and intracortical "horizontal" pathways. By its nature, the framework confers sensitivity to specific, spectrotemporally complex stimuli. Finally, to address how spectral integration can be regulated, we show how one neuromodulator, acetylcholine, could act within the hypothesized framework to alter integration in single neurons. The results of these studies promote a cellular understanding of information processing in auditory cortex.},
  langid = {english},
  pmid = {16081005},
  keywords = {_tablet,Acetylcholine,Acoustic Stimulation,Animals,Auditory Cortex,Auditory Perception,Humans,Neurons,Reaction Time,Spectrum Analysis,Synapses},
  file = {/Users/jonny/Dropbox/papers/zotero/M/MetherateR/metherate_2005_spectral_integration_in_auditory_cortex.pdf}
}

@article{millerRetrosplenialCorticalRepresentations2019,
  title = {Retrosplenial {{Cortical Representations}} of {{Space}} and {{Future Goal Locations Develop}} with {{Learning}}},
  author = {Miller, Adam M. P. and Mau, William and Smith, David M.},
  year = {2019},
  month = jun,
  journal = {Current Biology},
  volume = {29},
  number = {12},
  pages = {2083-2090.e4},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2019.05.034},
  abstract = {Recent findings suggest that long-term spatial and contextual memories depend on the retrosplenial cortex (RSC) [1, 2, 3, 4, 5]. RSC damage impairs navigation in humans and rodents [6, 7, 8], and the RSC is closely interconnected with brain regions known to play a role in navigation, including the hippocampus and anterior thalamus [9, 10]. Navigation-related neural activity is seen in humans [11] and rodents, including spatially localized firing [12, 13], directional firing [12, 14, 15], and responses to navigational cues [16]. RSC neuronal activity is modulated by allocentric, egocentric, and route-centered spatial reference frames [17, 18], consistent with an RSC role in integrating different kinds of navigational information [19]. However, the relationship between RSC firing patterns and spatial memory remains largely unexplored, as previous physiology studies have not employed behavioral tasks with a clear memory demand. To address this, we trained rats on a continuous T-maze alternation task and examined RSC firing patterns throughout learning. We found that the RSC developed a distributed population-level representation of the rat's spatial location and current trajectory to the goal as the rats learned. After the rats reached peak performance, RSC firing patterns began to represent the upcoming goal location as the rats approached the choice point. These neural simulations of the goal emerged at the same time that lesions impaired alternation performance, suggesting that the RSC gradually acquired task representations that contribute to navigational decision-making.},
  langid = {english},
  keywords = {_tablet,attention,cingulate,consolidation,decision making,long term,memory,navigation,prediction,simulation,space},
  file = {/Users/jonny/Dropbox/papers/zotero/M/MillerA/miller_2019_retrosplenial_cortical_representations_of_space_and_future_goal_locations.pdf}
}

@article{moerelEvaluatingColumnarStability2018,
  title = {Evaluating the {{Columnar Stability}} of {{Acoustic Processing}} in the {{Human Auditory Cortex}}},
  author = {Moerel, Michelle and Martino, Federico De and U{\u g}urbil, K{\^a}mil and Formisano, Elia and Yacoub, Essa},
  year = {2018},
  month = sep,
  journal = {Journal of Neuroscience},
  volume = {38},
  number = {36},
  pages = {7822--7832},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3576-17.2018},
  abstract = {Using ultra-high field fMRI, we explored the cortical depth-dependent stability of acoustic feature preference in human auditory cortex. We collected responses from human auditory cortex (subjects from either sex) to a large number of natural sounds at submillimeter spatial resolution, and observed that these responses were well explained by a model that assumes neuronal population tuning to frequency-specific spectrotemporal modulations. We observed a relatively stable (columnar) tuning to frequency and temporal modulations. However, spectral modulation tuning was variable throughout the cortical depth. This difference in columnar stability between feature maps could not be explained by a difference in map smoothness, as the preference along the cortical sheet varied in a similar manner for the different feature maps. Furthermore, tuning to all three features was more columnar in primary than nonprimary auditory cortex. The observed overall lack of overlapping columnar regions across acoustic feature maps suggests, especially for primary auditory cortex, a coding strategy in which across cortical depths tuning to some features is kept stable, whereas tuning to other features systematically varies. SIGNIFICANCE STATEMENT In the human auditory cortex, sound aspects are processed in large-scale maps. Invasive animal studies show that an additional processing organization may be implemented orthogonal to the cortical sheet (i.e., in the columnar direction), but it is unknown whether observed organizational principles apply to the human auditory cortex. Combining ultra-high field fMRI with natural sounds, we explore the columnar organization of various sound aspects. Our results suggest that the human auditory cortex contains a modular coding strategy, where, for each module, several sound aspects act as an anchor along which computations are performed while the processing of another sound aspect undergoes a transformation. This strategy may serve to optimally represent the content of our complex acoustic natural environment.},
  chapter = {Research Articles},
  copyright = {Copyright \textcopyright{} 2018 the authors 0270-6474/18/387822-11\$15.00/0},
  langid = {english},
  pmid = {30185539},
  keywords = {_tablet,auditory cortex,columnar processing,spectrotemporal modulations,tonotopy,ultra-high field fMRI},
  file = {/Users/jonny/Dropbox/papers/zotero/M/MoerelM/moerel_2018_evaluating_the_columnar_stability_of_acoustic_processing_in_the_human_auditory.pdf;/Users/jonny/Zotero/storage/N7KRS7G6/moerel_2018_evaluating_the_columnar_stability_of_acoustic_processing_in_the_human_auditory.pdf}
}

@article{natanComplementaryControlSensory2015b,
  title = {Complementary Control of Sensory Adaptation by Two Types of Cortical Interneurons},
  author = {Natan, Ryan G and Briguglio, John J and {Mwilambwe-Tshilobo}, Laetitia and Jones, Sara I and Aizenberg, Mark and Goldberg, Ethan M and Geffen, Maria Neimark},
  editor = {King, Andrew J},
  year = {2015},
  month = oct,
  journal = {eLife},
  volume = {4},
  pages = {e09868},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.09868},
  abstract = {Reliably detecting unexpected sounds is important for environmental awareness and survival. By selectively reducing responses to frequently, but not rarely, occurring sounds, auditory cortical neurons are thought to enhance the brain's ability to detect unexpected events through stimulus-specific adaptation (SSA). The majority of neurons in the primary auditory cortex exhibit SSA, yet little is known about the underlying cortical circuits. We found that two types of cortical interneurons differentially amplify SSA in putative excitatory neurons. Parvalbumin-positive interneurons (PVs) amplify SSA by providing non-specific inhibition: optogenetic suppression of PVs led to an equal increase in responses to frequent and rare tones. In contrast, somatostatin-positive interneurons (SOMs) selectively reduce excitatory responses to frequent tones: suppression of SOMs led to an increase in responses to frequent, but not to rare tones. A mutually coupled excitatory-inhibitory network model accounts for distinct mechanisms by which cortical inhibitory neurons enhance the brain's sensitivity to unexpected sounds.},
  keywords = {_tablet,auditory cortex,auditory processing,circuit,computational neuroscience,interneuron,optogenetics},
  file = {/Users/jonny/Zotero/storage/S4KAJKGX/natan_2015_complementary_control_of_sensory_adaptation_by_two_types_of_cortical.pdf}
}

@article{natanCorticalInterneuronsDifferentially2017,
  title = {Cortical {{Interneurons Differentially Shape Frequency Tuning}} Following {{Adaptation}}},
  author = {Natan, Ryan G. and Rao, Winnie and Geffen, Maria N.},
  year = {2017},
  month = oct,
  journal = {Cell Reports},
  volume = {21},
  number = {4},
  pages = {878--890},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2017.10.012},
  abstract = {Neuronal stimulus selectivity is shaped by feedforward and recurrent excitatory-inhibitory interactions. In the auditory cortex (AC), parvalbumin- (PV) and~somatostatin-positive (SOM) inhibitory interneurons differentially modulate frequency-dependent responses of excitatory neurons. Responsiveness of neurons in the AC to sound is also dependent on stimulus history. We found that the inhibitory effects of SOMs and PVs diverged as a function of adaptation to temporal repetition of tones. Prior to adaptation, suppressing either SOM or PV inhibition drove both increases and decreases in excitatory spiking activity. After adaptation, suppressing SOM activity caused predominantly disinhibitory effects, whereas suppressing PV activity still evoked bi-directional changes. SOM, but not PV-driven inhibition, dynamically modulated frequency tuning with adaptation. Unlike PV-driven inhibition, SOM-driven inhibition elicited gain-like increases in frequency tuning reflective of adaptation. Our findings suggest that distinct cortical interneurons differentially shape tuning to sensory stimuli across the neuronal receptive field, altering frequency selectivity of excitatory neurons during adaptation.},
  langid = {english},
  keywords = {_tablet,adaptation,archaerhodopsin,auditory cortex,auditory processing,cortical processing,frequency tuning,inhibition,interneurons,optogenetics},
  file = {/Users/jonny/Dropbox/papers/zotero/N/NatanR/natan_2017_cortical_interneurons_differentially_shape_frequency_tuning_following_adaptation.pdf;/Users/jonny/Zotero/storage/QJ8PXRBB/natan_2017_cortical_interneurons_differentially_shape_frequency_tuning_following_adaptation.pdf}
}

@article{navarroDevilDeepBlue2019,
  title = {Between the {{Devil}} and the {{Deep Blue Sea}}: {{Tensions Between Scientific Judgement}} and {{Statistical Model Selection}}},
  shorttitle = {Between the {{Devil}} and the {{Deep Blue Sea}}},
  author = {Navarro, Danielle J.},
  year = {2019},
  month = mar,
  journal = {Computational Brain \& Behavior},
  volume = {2},
  number = {1},
  pages = {28--34},
  issn = {2522-087X},
  doi = {10.1007/s42113-018-0019-z},
  abstract = {Discussions of model selection in the psychological literature typically frame the issues as a question of statistical inference, with the goal being to determine which model makes the best predictions about data. Within this setting, advocates of leave-one-out cross-validation and Bayes factors disagree on precisely which prediction problem model selection questions should aim to answer. In this comment, I discuss some of these issues from a scientific perspective. What goal does model selection serve when all models are known to be systematically wrong? How might ``toy problems'' tell a misleading story? How does the scientific goal of explanation align with (or differ from) traditional statistical concerns? I do not offer answers to these questions, but hope to highlight the reasons why psychological researchers cannot avoid asking them.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/N/NavarroD/navarro_2019_between_the_devil_and_the_deep_blue_sea.pdf;/Users/jonny/Zotero/storage/BZHSPPI5/navarro_2019_between_the_devil_and_the_deep_blue_sea.pdf}
}

@misc{navarroNoteAppliedUse2019,
  title = {A Note on the Applied Use of {{MDL}} Approximations},
  author = {Navarro, Danielle},
  year = {2019},
  month = aug,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/upf42},
  abstract = {An applied problem is discussed in which two nested psychological models of retention are compared using minimum description length (MDL). The standard Fisher information approximation to the normalized maximum likelihood is calculated for these two models, with the result that the full model is assigned a smaller complexity, even for moderately large samples. A geometric interpretation for this behavior is considered, along with its practical implications.},
  keywords = {_tablet,Mathematical Psychology,minimum description length,Quantitative Methods,Social and Behavioral Sciences},
  file = {/Users/jonny/Dropbox/papers/zotero/N/NavarroD/navarro_2019_a_note_on_the_applied_use_of_mdl_approximations.pdf}
}

@article{neophytouUsingNeuralCircuit2020,
  title = {Using {{Neural Circuit Interrogation}} in {{Rodents}} to {{Unravel Human Speech Decoding}}},
  author = {Neophytou, Demetrios and Oviedo, Hysell V.},
  year = {2020},
  journal = {Frontiers in Neural Circuits},
  volume = {14},
  publisher = {{Frontiers}},
  issn = {1662-5110},
  doi = {10.3389/fncir.2020.00002},
  abstract = {The circuits responsible for social communication are some of the most important and least understood in the brain. Human studies have made great progress in advancing our understanding of the global computations required for processing speech, and animal models offer the opportunity to discover evolutionarily conserved mechanisms for decoding these signals. In this review we describe some of the most well-established speech decoding computations from human studies and describe animal studies designed to reveal potential circuit mechanisms underlying these processes. Human and animal brains must perform the challenging tasks of rapidly recognizing, categorizing, and assigning communicative importance to sounds in a noisy environment. The instructions to these functions are found in the precise connections neurons make with one another. Therefore, identifying circuit-motifs in the auditory cortices and linking them to communicative functions is pivotal. We review recent advances in human recordings that have revealed the most basic unit of speech encoded by neurons, and relate these findings to circuit-mapping studies in rodents that have shown potential connectivity schemes to achieve this. Finally, we discuss other putative important processing features in humans like lateralization, sensitivity to fine temporal features, and hierarchical processing. The goal is for animal studies to investigate neurophysiological and anatomical pathways responsible for establishing accessible behavioral phenotypes that are shared between humans and animals. This can be accomplished by establishing cell types, connectivity patterns, genetic pathways and critical periods that are relevant in the development and function of social communication.},
  langid = {english},
  keywords = {_tablet,animal models - rodent,auditory cortex (AC),cortical circuit,Speech - Brain,Temporal processing and spectral processing},
  file = {/Users/jonny/Dropbox/papers/zotero/N/NeophytouD/neophytou_2020_using_neural_circuit_interrogation_in_rodents_to_unravel_human_speech_decoding.pdf;/Users/jonny/Zotero/storage/3Y2UNKV3/neophytou_2020_using_neural_circuit_interrogation_in_rodents_to_unravel_human_speech_decoding.pdf}
}

@article{norman-haignereDivergenceFunctionalOrganization2019a,
  title = {Divergence in the Functional Organization of Human and Macaque Auditory Cortex Revealed by {{fMRI}} Responses to Harmonic Tones},
  author = {{Norman-Haignere}, Sam V. and Kanwisher, Nancy and McDermott, Josh H. and Conway, Bevil R.},
  year = {2019},
  month = jun,
  journal = {Nature Neuroscience},
  pages = {1},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0410-7},
  abstract = {Norman-Haignere et al. report that humans but not macaque monkeys possess cortical regions with a strong preference for harmonic tones compared to noise. This species difference may be driven by the demands of speech and music perception in humans.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/N/Norman-HaignereS/norman-haignere_2019_divergence_in_the_functional_organization_of_human_and_macaque_auditory_cortex.pdf;/Users/jonny/Zotero/storage/X6XLNTHQ/Norman-Haignere et al. - 2019 - Divergence in the functional organization of human.pdf;/Users/jonny/Zotero/storage/IYKYCJTX/s41593-019-0410-7.html}
}

@article{norman-haignereHierarchicalIntegrationMultiple2020,
  title = {Hierarchical Integration across Multiple Timescales in Human Auditory Cortex},
  author = {{Norman-Haignere}, Sam V. and Long, Laura K. and Devinsky, Orrin and Doyle, Werner and Irobunda, Ifeoma and Merricks, Edward M. and Feldstein, Neil A. and McKhann, Guy M. and Schevon, Catherine A. and Flinker, Adeen and Mesgarani, Nima},
  year = {2020},
  month = oct,
  journal = {bioRxiv},
  pages = {2020.09.30.321687},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.09.30.321687},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}To derive meaning from natural sounds, the brain must integrate information across tens to hundreds of milliseconds. But it is unknown how different regions and hemispheres of human auditory cortex collectively integrate across multiple timescales. To answer this question, we developed a novel method to estimate integration periods and applied this method to intracranial recordings. We show that human auditory cortex integrates across time hierarchically, with three-fold longer integration periods in non-primary vs. primary regions, but no difference between hemispheres. Moreover, we show that selectivity for categories, such as speech and music, is restricted to electrodes with long integration periods. These findings suggest that short-term structure in natural sounds is analyzed by general-purpose acoustic mechanisms in primary auditory cortex, and then integrated over long timescales to form category-specific representations in non-primary regions. Our study thus reveals how the human brain constructs abstract representations of sound by integrating across multiple timescales.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/N/Norman-HaignereS/norman-haignere_2020_hierarchical_integration_across_multiple_timescales_in_human_auditory_cortex.pdf;/Users/jonny/Zotero/storage/R9UJQV4X/2020.09.30.html}
}

@book{ohalaGuideHistoryPhonetic1999,
  title = {A Guide to the History of the Phonetic Sciences in the {{United States}}},
  editor = {Ohala, John J and Bronstein, J, Arthur and Bus{\`a}, M. Grazia and Lewis, Julie A and Weigel, William F},
  year = {1999},
  abstract = {Editor(s): Ohala, John J.; Bronstein, Arthur J.; Bus\`a, M. Grazia; Lewis, Julie A.; Weigel, William F.},
  langid = {english},
  file = {/Users/jonny/Dropbox/papers/zotero/undefined/undefined/1999_a_guide_to_the_history_of_the_phonetic_sciences_in_the_united_states.pdf;/Users/jonny/Dropbox/papers/zotero/undefined/undefined/1999_a_guide_to_the_history_of_the_phonetic_sciences_in_the_united_states2.pdf}
}

@article{oviedoConnectivityMotifsInhibitory2017,
  title = {Connectivity Motifs of Inhibitory Neurons in the Mouse {{Auditory Cortex}}},
  author = {Oviedo, Hysell V.},
  year = {2017},
  month = dec,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {16987},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-16904-2},
  abstract = {Connectivity determines the function of neural circuits and it is the gateway to behavioral output. The emergent properties of the Auditory Cortex (ACx) have been difficult to unravel partly due to our assumption that it is organized similarly to other sensory areas. But detailed investigations of its functional connectivity have begun to reveal significant differences from other cortical areas that perform different functions. Using Laser Scanning Photostimulation we previously discovered unique circuit features in the ACx. Specifically, we found that the functional asymmetry of the ACx (tonotopy and isofrequency axes) is reflected in the local circuitry of excitatory inputs to Layer 3 pyramidal neurons. In the present study we extend the functional wiring diagram of the ACx with an investigation of the connectivity patterns of inhibitory subclasses. We compared excitatory input to parvalbumin (PV) and somatostatin (SOM)-expressing interneurons and found distinct circuit-motifs between and within these subpopulations. Moreover, these connectivity motifs emerged as intrinsic differences between the left and right ACx. Our results support a functional circuit based approach to understand the role of inhibitory neurons in auditory processing.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/O/OviedoH/oviedo_2017_connectivity_motifs_of_inhibitory_neurons_in_the_mouse_auditory_cortex.pdf;/Users/jonny/Zotero/storage/T4FM5VCI/oviedo_2017_connectivity_motifs_of_inhibitory_neurons_in_the_mouse_auditory_cortex.pdf}
}

@article{pandarinathInferringSingletrialNeural2018,
  title = {Inferring Single-Trial Neural Population Dynamics Using Sequential Auto-Encoders},
  author = {Pandarinath, Chethan and O'Shea, Daniel J. and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D. and Kao, Jonathan C. and Trautmann, Eric M. and Kaufman, Matthew T. and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V. and Abbott, L. F. and Sussillo, David},
  year = {2018},
  month = oct,
  journal = {Nature Methods},
  volume = {15},
  number = {10},
  pages = {805--815},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-018-0109-9},
  abstract = {Neuroscience is experiencing a revolution in which simultaneous recording of thousands of neurons is revealing population dynamics that are not apparent from single-neuron responses. This structure is typically extracted from data averaged across many trials, but deeper understanding requires studying phenomena detected in single trials, which is challenging due to incomplete sampling of the neural population, trial-to-trial variability, and fluctuations in action potential timing. We introduce latent factor analysis via dynamical systems, a deep learning method to infer latent dynamics from single-trial neural spiking data. When applied to a variety of macaque and human motor cortical datasets, latent factor analysis via dynamical systems accurately predicts observed behavioral variables, extracts precise firing rate estimates of neural dynamics on single trials, infers perturbations to those dynamics that correlate with behavioral choices, and combines data from non-overlapping recording sessions spanning months to improve inference of underlying dynamics.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PandarinathC/pandarinath_2018_inferring_single-trial_neural_population_dynamics_using_sequential_auto-encoders.pdf;/Users/jonny/Zotero/storage/NCTWPMBC/pandarinath_2018_inferring_single-trial_neural_population_dynamics_using_sequential_auto-encoders.pdf}
}

@article{pandyaSpectralTemporalProcessing2008,
  title = {Spectral and {{Temporal Processing}} in {{Rat Posterior Auditory Cortex}}},
  author = {Pandya, Pritesh K. and Rathbun, Daniel L. and Moucha, Raluca and Engineer, Navzer D. and Kilgard, Michael P.},
  year = {2008},
  month = feb,
  journal = {Cerebral Cortex},
  volume = {18},
  number = {2},
  pages = {301--314},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhm055},
  abstract = {The rat auditory cortex is divided anatomically into several areas, but little is known about the functional differences in information processing between these areas. To determine the filter properties of rat posterior auditory field (PAF) neurons, we compared neurophysiological responses to simple tones, frequency modulated (FM) sweeps, and amplitude modulated noise and tones with responses of primary auditory cortex (A1) neurons. PAF neurons have excitatory receptive fields that are on average 65\% broader than A1 neurons. The broader receptive fields of PAF neurons result in responses to narrow and broadband inputs that are stronger than A1. In contrast to A1, we found little evidence for an orderly topographic gradient in PAF based on frequency. These neurons exhibit latencies that are twice as long as A1. In response to modulated tones and noise, PAF neurons adapt to repeated stimuli at significantly slower rates. Unlike A1, neurons in PAF rarely exhibit facilitation to rapidly repeated sounds. Neurons in PAF do not exhibit strong selectivity for rate or direction of narrowband one octave FM sweeps. These results indicate that PAF, like nonprimary visual fields, processes sensory information on larger spectral and longer temporal scales than primary cortex.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PandyaP/pandya_2008_spectral_and_temporal_processing_in_rat_posterior_auditory_cortex.pdf;/Users/jonny/Zotero/storage/X3CBK4UZ/pandya_2008_spectral_and_temporal_processing_in_rat_posterior_auditory_cortex.pdf}
}

@article{parkerMovementRelatedSignalsSensory2020,
  title = {Movement-{{Related Signals}} in {{Sensory Areas}}: {{Roles}} in {{Natural Behavior}}},
  shorttitle = {Movement-{{Related Signals}} in {{Sensory Areas}}},
  author = {Parker, Philip R. L. and Brown, Morgan A. and Smear, Matthew C. and Niell, Cristopher M.},
  year = {2020},
  month = aug,
  journal = {Trends in Neurosciences},
  volume = {43},
  number = {8},
  pages = {581--595},
  publisher = {{Elsevier}},
  issn = {0166-2236, 1878-108X},
  doi = {10.1016/j.tins.2020.05.005},
  langid = {english},
  pmid = {32580899},
  keywords = {active sensation,cortical processing,ethology,sensory physiology},
  file = {/Users/jonny/Zotero/storage/WCHBBHDJ/S0166-2236(20)30123-5.html}
}

@article{pashkovskiStructureFlexibilityCortical2020,
  title = {Structure and Flexibility in Cortical Representations of Odour Space},
  author = {Pashkovski, Stan L. and Iurilli, Giuliano and Brann, David and Chicharro, Daniel and Drummey, Kristen and Franks, Kevin M. and Panzeri, Stefano and Datta, Sandeep Robert},
  year = {2020},
  month = jul,
  journal = {Nature},
  volume = {583},
  number = {7815},
  pages = {253--258},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2451-1},
  abstract = {The cortex organizes sensory information to enable discrimination and generalization1\textendash 4. As systematic representations of chemical odour space have not~yet been described in the olfactory cortex, it remains unclear how odour relationships are encoded to place chemically distinct but similar odours, such as lemon and orange, into perceptual categories, such as citrus5\textendash 7. Here, by combining chemoinformatics and multiphoton imaging in the mouse, we show that both the piriform cortex and its sensory inputs from the olfactory bulb represent chemical odour relationships through correlated patterns of activity. However, cortical odour codes differ from those in the bulb: cortex more strongly clusters together representations for related odours, selectively rewrites pairwise odour relationships, and better matches odour perception. The bulb-to-cortex transformation depends on the associative network originating within the piriform cortex, and can be reshaped by passive odour experience. Thus, cortex actively builds a structured representation of chemical odour space that highlights odour relationships; this representation is similar across individuals but remains plastic, suggesting a means through which the olfactory system can assign related odour cues to common and yet personalized percepts.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PashkovskiS/pashkovski_2020_structure_and_flexibility_in_cortical_representations_of_odour_space.pdf;/Users/jonny/Zotero/storage/7JE6KCVR/s41586-020-2451-1.html;/Users/jonny/Zotero/storage/DX9YGZD8/s41586-020-2451-1.html}
}

@article{Pasley2012,
  title = {Reconstructing Speech from Human Auditory Cortex},
  author = {Pasley, Brian N. and David, Stephen V. and Mesgarani, Nima and Flinker, Adeen and Shamma, Shihab A. and Crone, Nathan E. and Knight, Robert T. and Chang, Edward F.},
  year = {2012},
  journal = {PLoS Biology},
  volume = {10},
  number = {1},
  issn = {15449173},
  doi = {10.1371/journal.pbio.1001251},
  abstract = {How the human auditory system extracts perceptually relevant acoustic features of speech is unknown. To address this question, we used intracranial recordings from nonprimary auditory cortex in the human superior temporal gyrus to determine what acoustic information in speech sounds can be reconstructed from population neural activity. We found that slow and intermediate temporal fluctuations, such as those corresponding to syllable rate, were accurately reconstructed using a linear model based on the auditory spectrogram. However, reconstruction of fast temporal fluctuations, such as syllable onsets and offsets, required a nonlinear sound representation based on temporal modulation energy. Reconstruction accuracy was highest within the range of spectro-temporal fluctuations that have been found to be critical for speech intelligibility. The decoded speech representations allowed readout and identification of individual words directly from brain activity during single trial sound presentations. These findings reveal neural encoding mechanisms of speech acoustic parameters in higher order human auditory cortex.},
  isbn = {1545-7885},
  pmid = {22303281},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PasleyB/pasley_2012_reconstructing_speech_from_human_auditory_cortex.pdf;/Users/jonny/Zotero/storage/N5PE7ILM/Pasley et al. - 2012 - Reconstructing speech from human auditory cortex(3).pdf}
}

@article{perichNeuralPopulationMechanism2018,
  title = {A {{Neural Population Mechanism}} for {{Rapid Learning}}},
  author = {Perich, Matthew G. and Gallego, Juan A. and Miller, Lee E.},
  year = {2018},
  month = nov,
  journal = {Neuron},
  volume = {100},
  number = {4},
  pages = {964-976.e7},
  publisher = {{Elsevier}},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.09.030},
  langid = {english},
  pmid = {30344047},
  keywords = {_tablet,functional connectivity,monkeys,motor cortex,motor learning,neural manifold,nonhuman primates,premotor cortex,single neurons},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PerichM/perich_2018_a_neural_population_mechanism_for_rapid_learning.pdf;/Users/jonny/Zotero/storage/4KNBX5YS/S0896-6273(18)30832-8.html}
}

@article{Polley2006,
  title = {Perceptual {{Learning Directs Auditory Cortical Map Reorganization}} through {{Top-Down Influences}}},
  author = {Polley, D. B.},
  year = {2006},
  journal = {Journal of Neuroscience},
  volume = {26},
  number = {18},
  pages = {4970--4982},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.3771-05.2006},
  abstract = {The primary sensory cortex is positioned at a confluence of bottom-up dedicated sensory inputs and top-down inputs related to higher-order sensory features, attentional state, and behavioral reinforcement. We tested whether topographic map plasticity in the adult primary auditory cortex and a secondary auditory area, the suprarhinal auditory field, was controlled by the statistics of bottom-up sensory inputs or by top-down task-dependent influences. Rats were trained to attend to independent parameters, either frequency or intensity, within an identical set of auditory stimuli, allowing us to vary task demands while holding the bottom-up sensory inputs constant. We observed a clear double-dissociation in map plasticity in both cortical fields. Rats trained to attend to frequency cues exhibited an expanded representation of the target frequency range within the tonotopic map but no change in sound intensity encoding compared with controls. Rats trained to attend to intensity cues expressed an increased proportion of nonmonotonic intensity response profiles preferentially tuned to the target intensity range but no change in tonotopic map organization relative to controls. The degree of topographic map plasticity within the task-relevant stimulus dimension was correlated with the degree of perceptual learning for rats in both tasks. These data suggest that enduring receptive field plasticity in the adult auditory cortex may be shaped by task-specific top-down inputs that interact with bottom-up sensory inputs and reinforcement-based neuromodulator release. Top-down inputs might confer the selectivity necessary to modify a single feature representation without affecting other spatially organized feature representations embedded within the same neural circuitry.},
  isbn = {1529-2401 (Electronic)},
  pmid = {16672673},
  keywords = {_tablet,attention,conditioning,cortex,plasticity,reward,topographic map},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PolleyD/polley_2006_perceptual_learning_directs_auditory_cortical_map_reorganization_through.pdf;/Users/jonny/Zotero/storage/MDHMPHT4/Polley, Steinberg, Merzenich - 2006 - Perceptual learning directs auditory cortical map reorganization through top-down influences(3).pdf}
}

@article{pritchettHighlevelLanguageProcessing2018,
  title = {High-Level Language Processing Regions Are Not Engaged in Action Observation or Imitation},
  author = {Pritchett, Brianna L. and Hoeflin, Caitlyn and Koldewyn, Kami and Dechter, Eyal and Fedorenko, Evelina},
  year = {2018},
  month = aug,
  journal = {Journal of Neurophysiology},
  volume = {120},
  number = {5},
  pages = {2555--2570},
  issn = {0022-3077},
  doi = {10.1152/jn.00222.2018},
  abstract = {A set of left frontal, temporal, and parietal brain regions respond robustly during language comprehension and production (e.g., Fedorenko E, Hsieh PJ, Nieto-Casta\~n\'on A, Whitfield-Gabrieli S, Kanwisher N. J Neurophysiol 104: 1177\textendash 1194, 2010; Menenti L, Gierhan SM, Segaert K, Hagoort P. Psychol Sci 22: 1173\textendash 1182, 2011). These regions have been further shown to be selective for language relative to other cognitive processes, including arithmetic, aspects of executive function, and music perception (e.g., Fedorenko E, Behr MK, Kanwisher N. Proc Natl Acad Sci USA 108: 16428\textendash 16433, 2011; Monti MM, Osherson DN. Brain Res 1428: 33\textendash 42, 2012). However, one claim about overlap between language and nonlinguistic cognition remains prominent. In particular, some have argued that language processing shares computational demands with action observation and/or execution (e.g., Rizzolatti G, Arbib MA. Trends Neurosci 21: 188\textendash 194, 1998; Koechlin E, Jubault T. Neuron 50: 963\textendash 974, 2006; Tettamanti M, Weniger D. Cortex 42: 491\textendash 494, 2006). However, the evidence for these claims is indirect, based on observing activation for language and action tasks within the same broad anatomical areas (e.g., on the lateral surface of the left frontal lobe). To test whether language indeed shares machinery with action observation/execution, we examined the responses of language brain regions, defined functionally in each individual participant (Fedorenko E, Hsieh PJ, Nieto-Casta\~n\'on A, Whitfield-Gabrieli S, Kanwisher N. J Neurophysiol 104: 1177\textendash 1194, 2010) to action observation (experiments 1, 2, and 3a) and action imitation (experiment 3b). With the exception of the language region in the angular gyrus, all language regions, including those in the inferior frontal gyrus (within ``Broca's area''), showed little or no response during action observation/imitation. These results add to the growing body of literature suggesting that high-level language regions are highly selective for language processing (see Fedorenko E, Varley R. Ann NY Acad Sci 1369: 132\textendash 153, 2016 for a review).NEW \& NOTEWORTHY Many have argued for overlap in the machinery used to interpret language and others' actions, either because action observation was a precursor to linguistic communication or because both require interpreting hierarchically-structured stimuli. However, existing evidence is indirect, relying on group analyses or reverse inference. We examined responses to action observation in language regions defined functionally in individual participants and found no response. Thus language comprehension and action observation recruit distinct circuits in the modern brain.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PritchettB/pritchett_2018_high-level_language_processing_regions_are_not_engaged_in_action_observation_or.pdf;/Users/jonny/Zotero/storage/LWU2T3B2/Pritchett et al. - 2018 - High-level language processing regions are not eng.pdf;/Users/jonny/Zotero/storage/TT2WDCAB/jn.00222.html}
}

@article{rabinowitzConstructingNoiseinvariantRepresentations2013,
  title = {Constructing Noise-Invariant Representations of Sound in the Auditory Pathway},
  author = {Rabinowitz, Neil C. and Willmore, Ben D. B. and King, Andrew J. and Schnupp, Jan W. H.},
  year = {2013},
  month = nov,
  journal = {PLoS biology},
  volume = {11},
  number = {11},
  pages = {e1001710},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1001710},
  abstract = {Identifying behaviorally relevant sounds in the presence of background noise is one of the most important and poorly understood challenges faced by the auditory system. An elegant solution to this problem would be for the auditory system to represent sounds in a noise-invariant fashion. Since a major effect of background noise is to alter the statistics of the sounds reaching the ear, noise-invariant representations could be promoted by neurons adapting to stimulus statistics. Here we investigated the extent of neuronal adaptation to the mean and contrast of auditory stimulation as one ascends the auditory pathway. We measured these forms of adaptation by presenting complex synthetic and natural sounds, recording neuronal responses in the inferior colliculus and primary fields of the auditory cortex of anaesthetized ferrets, and comparing these responses with a sophisticated model of the auditory nerve. We find that the strength of both forms of adaptation increases as one ascends the auditory pathway. To investigate whether this adaptation to stimulus statistics contributes to the construction of noise-invariant sound representations, we also presented complex, natural sounds embedded in stationary noise, and used a decoding approach to assess the noise tolerance of the neuronal population code. We find that the code for complex sounds in the periphery is affected more by the addition of noise than the cortical code. We also find that noise tolerance is correlated with adaptation to stimulus statistics, so that populations that show the strongest adaptation to stimulus statistics are also the most noise-tolerant. This suggests that the increase in adaptation to sound statistics from auditory nerve to midbrain to cortex is an important stage in the construction of noise-invariant sound representations in the higher auditory brain.},
  langid = {english},
  pmcid = {PMC3825667},
  pmid = {24265596},
  keywords = {_tablet,Acoustic Stimulation,Adaptation; Physiological,Animals,Auditory Cortex,Auditory Perception,Cochlear Nerve,Computer Simulation,Female,Ferrets,Hearing,Humans,Male,Models; Neurological,Neural Conduction,Noise,Signal-To-Noise Ratio},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RabinowitzN/rabinowitz_2013_constructing_noise-invariant_representations_of_sound_in_the_auditory_pathway.pdf;/Users/jonny/Zotero/storage/R9NDWXJ4/rabinowitz_2013_constructing_noise-invariant_representations_of_sound_in_the_auditory_pathway.pdf}
}

@article{rabinowitzContrastGainControl2011c,
  title = {Contrast Gain Control in Auditory Cortex},
  author = {Rabinowitz, Neil C. and Willmore, Ben D. B. and Schnupp, Jan W. H. and King, Andrew J.},
  year = {2011},
  month = jun,
  journal = {Neuron},
  volume = {70},
  number = {6},
  pages = {1178--1191},
  issn = {1097-4199},
  doi = {10.1016/j.neuron.2011.04.030},
  abstract = {The auditory system must represent sounds with a wide range of statistical properties. One important property is the spectrotemporal contrast in the acoustic environment: the variation in sound pressure in each frequency band, relative to the mean pressure. We show that neurons in ferret auditory cortex rescale their gain to partially compensate for the spectrotemporal contrast of recent stimulation. When contrast is low, neurons increase their gain, becoming more sensitive to small changes in the stimulus, although the effectiveness of contrast gain control is reduced at low mean levels. Gain is primarily determined by contrast near each neuron's preferred frequency, but there is also a contribution from contrast in more distant frequency bands. Neural responses are modulated by contrast over timescales of {$\sim$}100~ms. By using contrast gain control to expand or compress the representation of its inputs, the auditory system may be seeking an efficient coding of natural sounds.},
  langid = {english},
  pmcid = {PMC3133688},
  pmid = {21689603},
  keywords = {_tablet,Acoustic Stimulation,Adaptation; Physiological,Animals,Auditory Cortex,Auditory Threshold,Discrimination; Psychological,Electrophysiology,Female,Ferrets,Male,Models; Neurological,Neurons,Pitch Perception,Sound Spectrography},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RabinowitzN/rabinowitz_2011_contrast_gain_control_in_auditory_cortex.pdf}
}

@article{ramaswamyAlgorithmicBarrierNeural2019a,
  title = {An {{Algorithmic Barrier}} to {{Neural Circuit Understanding}}},
  author = {Ramaswamy, Venkatakrishnan},
  year = {2019},
  month = may,
  journal = {bioRxiv},
  pages = {639724},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/639724},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Neuroscience is witnessing extraordinary progress in experimental techniques, especially at the neural circuit level. These advances are largely aimed at enabling us to understand how neural circuit computations mechanistically \emph{cause} behavior. Here, using techniques from Theoretical Computer Science, we examine how many experiments are needed to obtain such an empirical understanding. It is proved, mathematically, that establishing the most extensive notions of understanding \emph{need} exponentially-many experiments in the number of neurons, in general, unless a widely-posited hypothesis about computation is false. Worse still, the feasible experimental regime is one where the number of experiments scales sub-linearly in the number of neurons, suggesting a fundamental impediment to such an understanding. Determining which notions of understanding are algorithmically tractable, thus, becomes an important new endeavor in Neuroscience.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RamaswamyV/false}
}

@article{rauscheckerMapsStreamsAuditory2009b,
  title = {Maps and Streams in the Auditory Cortex: Nonhuman Primates Illuminate Human Speech Processing},
  shorttitle = {Maps and Streams in the Auditory Cortex},
  author = {Rauschecker, Josef P and Scott, Sophie K},
  year = {2009},
  month = jun,
  journal = {Nature neuroscience},
  volume = {12},
  number = {6},
  pages = {718--724},
  issn = {1097-6256},
  doi = {10.1038/nn.2331},
  abstract = {Speech and language are considered uniquely human abilities: animals have communication systems, but they do not match human linguistic skills in terms of recursive structure and combinatorial power. Yet, in evolution, spoken language must have emerged from neural mechanisms at least partially available in animals. In this paper, we will demonstrate how our understanding of speech perception, one important facet of language, has profited from findings and theory in nonhuman primate studies. Chief among these are physiological and anatomical studies showing that primate auditory cortex, across species, shows patterns of hierarchical structure, topographic mapping and streams of functional processing. We will identify roles for different cortical areas in the perceptual processing of speech and review functional imaging work in humans that bears on our understanding of how the brain decodes and monitors speech. A new model connects structures in the temporal, frontal and parietal lobes linking speech perception and production.},
  pmcid = {PMC2846110},
  pmid = {19471271},
  keywords = {_tablet},
  file = {/Users/jonny/Zotero/storage/HDR2LZ83/rauschecker_2009_maps_and_streams_in_the_auditory_cortex.pdf;/Users/jonny/Zotero/storage/IPDN6E49/rauschecker_2009_maps_and_streams_in_the_auditory_cortex.pdf}
}

@article{remezPerceptualOrganizationSpeech1994,
  title = {On the Perceptual Organization of Speech},
  author = {Remez, Robert E. and Rubin, Philip E. and Berns, Stefanie M. and Pardo, Jennifer S. and Lang, Jessica M.},
  year = {1994},
  month = jan,
  journal = {Psychological Review},
  volume = {101},
  number = {1},
  pages = {129--156},
  issn = {0033-295X},
  doi = {10.1037/0033-295X.101.1.129},
  abstract = {A general account of auditory perceptual organization has developed in the past 2 decades. It relies on primitive devices akin to the Gestalt principles of organization to assign sensory elements to probable groupings and invokes secondary schematic processes to confirm or to repair the possible organization. Although this conceptualization is intended to apply universally, the variety and arrangement of acoustic constituents of speech violate Gestalt principles at numerous junctures, cohering perceptually, nonetheless. The authors report 3 experiments on organization in phonetic perception, using sine wave synthesis to evade the Gestalt rules and the schematic processes alike. These findings falsify a general auditory account, showing that phonetic perceptual organization is achieved by specific sensitivity to the acoustic modulations characteristic of speech signals.},
  langid = {english},
  pmid = {8121955},
  keywords = {_tablet,Adult,Attention,Female,Gestalt Theory,Humans,Male,Phonetics,Psychoacoustics,Sound Spectrography,Speech Perception},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RemezR/remez_1994_on_the_perceptual_organization_of_speech.pdf}
}

@article{remezSpeechPerceptionTraditional1981,
  title = {Speech Perception without Traditional Speech Cues},
  author = {Remez, R. E. and Rubin, P. E. and Pisoni, D. B. and Carrell, T. D.},
  year = {1981},
  month = may,
  journal = {Science},
  volume = {212},
  number = {4497},
  pages = {947--949},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7233191},
  abstract = {A three-tone sinusoidal replica of a naturally produced utterance was identified by listeners, despite the readily apparent unnatural speech quality of the signal. The time-varying properties of these highly artificial acoustic signals are apparently sufficient to support perception of the linguistic message in the absence of traditional acoustic cues for phonetic segments.},
  chapter = {Reports},
  copyright = {\textcopyright{} 1981},
  langid = {english},
  pmid = {7233191},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RemezR/remez_1981_speech_perception_without_traditional_speech_cues.pdf;/Users/jonny/Zotero/storage/7KTK7PRY/remez_1981_speech_perception_without_traditional_speech_cues.pdf}
}

@article{remezSpeechPerceptionTraditional1981a,
  title = {Speech Perception without Traditional Speech Cues},
  author = {Remez, R. E. and Rubin, P. E. and Pisoni, D. B. and Carrell, T. D.},
  year = {1981},
  month = may,
  journal = {Science},
  volume = {212},
  number = {4497},
  pages = {947--949},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7233191},
  abstract = {A three-tone sinusoidal replica of a naturally produced utterance was identified by listeners, despite the readily apparent unnatural speech quality of the signal. The time-varying properties of these highly artificial acoustic signals are apparently sufficient to support perception of the linguistic message in the absence of traditional acoustic cues for phonetic segments.},
  chapter = {Reports},
  copyright = {\textcopyright{} 1981},
  langid = {english},
  pmid = {7233191},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RemezR/false;/Users/jonny/Zotero/storage/LIE9MBIH/Remez et al. - 1981 - Speech perception without traditional speech cues.pdf}
}

@article{ritovDifferentialWeightingCommon1990,
  title = {Differential Weighting of Common and Distinctive Components},
  author = {Ritov, Ilana and Gati, Itamar and Tversky, Amos},
  year = {1990},
  journal = {Journal of Experimental Psychology: General},
  volume = {119},
  number = {1},
  pages = {30--41},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/0096-3445.119.1.30},
  abstract = {We investigated possible explanations of the finding that the relative weight (W) of common components in similarity judgments is higher for verbal than for pictorial stimuli. A serial presentation of stimulus components had no effect on verbal stimuli; it increased the impact of both common and distinctive components of pictorial stimuli but did not affect their relative weight. On the other hand, W was increased by manipulations that reduced the cohesiveness of composite pictures, such as separating, scrambling, and mixing their components. Furthermore, W was decreased by manipulations that enhanced the cohesiveness of composite verbal stimuli by imposing structure on their components. Verbal and pictorial representations of the same stimuli yielded no systematic differences in W. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {_tablet,Judgment,Pictorial Stimuli,Stimulus Similarity,Verbal Stimuli},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RitovI/ritov_1990_differential_weighting_of_common_and_distinctive_components.pdf;/Users/jonny/Zotero/storage/HJU732JI/1990-27355-001.html}
}

@misc{rooijTheoryTestHow2020,
  title = {Theory before the Test: {{How}} to Build High-Verisimilitude Explanatory Theories in Psychological Science},
  shorttitle = {Theory before the Test},
  author = {van Rooij, Iris and Baggio, Giosu{\`e}},
  year = {2020},
  month = feb,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/7qbpr},
  abstract = {Drawing on the philosophy of psychological explanation (Cummins, 1983; 2000), we suggest that psychological science, by focusing on effects, may lose sight of its primary explananda: psychological capacities. We revisit Marr's (1982) levels-of-analysis framework, which has been remarkably productive and useful for cognitive psychological explanation. We discuss ways in which Marr's framework may be extended to other areas of psychology, such as social, developmental, and evolutionary psychology, bringing new benefits to these fields. Next, we show how theoretical analyses can endow a theory with minimal plausibility even prior to contact with empirical data: we call this the theoretical cycle. Finally, we explain how our proposal may contribute to addressing critical issues in psychological science, including how to leverage effects to understand capacities better.},
  keywords = {_tablet,Cognitive Psychology,computational analysis,computational-level theory,Developmental Psychology,Evolution,formal modeling,levels of explanation,other,psychological explanation,Psychology,Social and Behavioral Sciences,Social and Personality Psychology,theoretical cycle,Theory and Philosophy of Science,theory development},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RooijI/rooij_2020_theory_before_the_test.pdf}
}

@article{rosasCausalBlanketsTheory2020,
  title = {Causal Blankets: {{Theory}} and Algorithmic Framework},
  shorttitle = {Causal Blankets},
  author = {Rosas, Fernando E. and Mediano, Pedro A. M. and Biehl, Martin and Chandaria, Shamil and Polani, Daniel},
  year = {2020},
  month = sep,
  journal = {arXiv:2008.12568 [nlin, q-bio]},
  eprint = {2008.12568},
  eprinttype = {arxiv},
  primaryclass = {nlin, q-bio},
  abstract = {We introduce a novel framework to identify perception-action loops (PALOs) directly from data based on the principles of computational mechanics. Our approach is based on the notion of causal blanket, which captures sensory and active variables as dynamical sufficient statistics -- i.e. as the "differences that make a difference." Moreover, our theory provides a broadly applicable procedure to construct PALOs that requires neither a steady-state nor Markovian dynamics. Using our theory, we show that every bipartite stochastic process has a causal blanket, but the extent to which this leads to an effective PALO formulation varies depending on the integrated information of the bipartition.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Artificial Intelligence,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Quantitative Biology - Neurons and Cognition},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RosasF/rosas_2020_causal_blankets.pdf;/Users/jonny/Zotero/storage/D7FW6EAQ/2008.html}
}

@article{roschBasicObjectsNatural1976,
  title = {Basic Objects in Natural Categories},
  author = {Rosch, Eleanor and Mervis, Carolyn B and Gray, Wayne D and Johnson, David M and {Boyes-Braem}, Penny},
  year = {1976},
  month = jul,
  journal = {Cognitive Psychology},
  volume = {8},
  number = {3},
  pages = {382--439},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(76)90013-X},
  abstract = {Categorizations which humans make of the concrete world are not arbitrary but highly determined. In taxonomies of concrete objects, there is one level of abstraction at which the most basic category cuts are made. Basic categories are those which carry the most information, possess the highest category cue validity, and are, thus, the most differentiated from one another. The four experiments of Part I define basic objects by demonstrating that in taxonomies of common concrete nouns in English based on class inclusion, basic objects are the most inclusive categories whose members: (a) possess significant numbers of attributes in common, (b) have motor programs which are similar to one another, (c) have similar shapes, and (d) can be identified from averaged shapes of members of the class. The eight experiments of Part II explore implications of the structure of categories. Basic objects are shown to be the most inclusive categories for which a concrete image of the category as a whole can be formed, to be the first categorizations made during perception of the environment, to be the earliest categories sorted and earliest named by children, and to be the categories most codable, most coded, and most necessary in language.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RoschE/rosch_1976_basic_objects_in_natural_categories.pdf;/Users/jonny/Zotero/storage/RNPNLUH9/rosch_1976_basic_objects_in_natural_categories.pdf}
}

@article{roschFamilyResemblancesStudies1975,
  title = {Family Resemblances: {{Studies}} in the Internal Structure of Categories},
  shorttitle = {Family Resemblances},
  author = {Rosch, Eleanor and Mervis, Carolyn B},
  year = {1975},
  month = oct,
  journal = {Cognitive Psychology},
  volume = {7},
  number = {4},
  pages = {573--605},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(75)90024-9},
  abstract = {Six experiments explored the hypothesis that the members of categories which are considered most prototypical are those with most attributes in common with other members of the category and least attributes in common with other categories. In probabilistic terms, the hypothesis is that prototypicality is a function of the total cue validity of the attributes of items. In Experiments 1 and 3, subjects listed attributes for members of semantic categories which had been previously rated for degree of prototypicality. High positive correlations were obtained between those ratings and the extent of distribution of an item's attributes among the other items of the category. In Experiments 2 and 4, subjects listed superordinates of category members and listed attributes of members of contrasting categories. Negative correlations were obtained between prototypicality and superordinates other than the category in question and between prototypicality and an item's possession of attributes possessed by members of contrasting categories. Experiments 5 and 6 used artificial categories and showed that family resemblance within categories and lack of overlap of elements with contrasting categories were correlated with ease of learning, reaction time in identifying an item after learning, and rating of prototypicality of an item. It is argued that family resemblance offers an alternative to criterial features in defining categories.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RoschE/rosch_1975_family_resemblances.pdf;/Users/jonny/Zotero/storage/CHYYK6JF/0010028575900249.html}
}

@incollection{roschWittgensteinCategorizationResearch1987,
  title = {Wittgenstein and {{Categorization Research}} in {{Cognitive Psychology}}},
  booktitle = {Meaning and the {{Growth}} of {{Understanding}}: {{Wittgenstein}}'s {{Significance}} for {{Developmental Psychology}}},
  author = {Rosch, Eleanor},
  editor = {Chapman, Michael and Dixon, Roger A.},
  year = {1987},
  pages = {151--166},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-83023-5_9},
  abstract = {Research in psychology tends to reflect, sometimes self-consciously, prevailing philosophical viewpoints. Categorization is the area in cognitive psychology which deals with the ancient problem of universals, that is, with the fact that unique particular objects or events can be treated equivalently. Prior to the 1970s, categorization research tended to mirror the simplified worlds described in early Wittgenstein and in logical positivism. However, Wittgenstein's later philosophy has revolutionary implications for many aspects of human thought, among them issues in categorization. In this paper, I will argue that modern research in natural categories is actually derived from Wittgensteinian insights, but ambivalently so: It has tended to work with the symptoms rather than the root of his challenge.},
  isbn = {978-3-642-83023-5},
  langid = {english},
  keywords = {_tablet,Category Membership,Color Category,Family Resemblance,Natural Category,Semantic Memory},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RoschE/rosch_1987_wittgenstein_and_categorization_research_in_cognitive_psychology.pdf}
}

@article{rosenRangeFrequencyEffects1979,
  title = {Range and Frequency Effects in Consonant Categorization},
  author = {Rosen, Stuart M.},
  year = {1979},
  journal = {Journal of Phonetics},
  volume = {7},
  number = {4},
  pages = {393--402},
  publisher = {{Elsevier Science}},
  address = {{Netherlands}},
  issn = {1095-8576(Electronic),0095-4470(Print)},
  abstract = {It is commonly thought that the existence of shifts in the judgment of a speech continuum after repeated presentation of one of the stimuli is due to the fatiguing ("adaptation") of detectors sensitive to some aspect ("feature") of the stimuli. Results of the present experiments with a total of 12 normally hearing adults show a shift in judgment of a speech continuum with changes only in the range (Exp I) or relative presentation frequencies (Exp II) of the stimuli. It is argued that these shifts, as well as those in "adaptation" experiments, have the same perceptual origins as those in the nonspeech continua; i.e., they are due to a general phenomenon common to all perceptual continua. (13 ref) (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Auditory Discrimination,Phonemes,Speech Perception,Stimulus Frequency,Stimulus Variability}
}

@article{rutishauserComputationDynamicallyBounded2015,
  title = {Computation in {{Dynamically Bounded Asymmetric Systems}}},
  author = {Rutishauser, Ueli and Slotine, Jean-Jacques and Douglas, Rodney},
  editor = {Sporns, Olaf},
  year = {2015},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {11},
  number = {1},
  pages = {e1004039},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004039},
  langid = {english},
  file = {/Users/jonny/Papers/RutishauserU/2015/Rutishauser_2015_Computation in Dynamically Bounded Asymmetric Systems.pdf}
}

@techreport{saddlerDeepNeuralNetwork2020,
  type = {Preprint},
  title = {Deep Neural Network Models Reveal Interplay of Peripheral Coding and Stimulus Statistics in Pitch Perception},
  author = {Saddler, Mark R. and Gonzalez, Ray and McDermott, Josh H.},
  year = {2020},
  month = nov,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/2020.11.19.389999},
  abstract = {Computations on receptor responses enable behavior in the environment. Behavior is plausibly shaped by both the sensory receptors and the environments for which organisms are optimized, but their roles are often opaque. One classic example is pitch perception, whose properties are commonly linked to peripheral neural coding limits rather than environmental acoustic constraints. We trained artificial neural networks to estimate fundamental frequency from simulated cochlear representations of natural sounds. The best-performing networks replicated many characteristics of human pitch judgments. To probe how our ears and environment shape these characteristics, we optimized networks given altered cochleae or sound statistics. Human-like behavior emerged only when cochleae had high temporal fidelity and when models were optimized for natural sounds. The results suggest pitch perception is critically shaped by the constraints of natural environments in addition to those of the cochlea, illustrating the use of contemporary neural networks to reveal underpinnings of behavior.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SaddlerM/saddler_2020_deep_neural_network_models_reveal_interplay_of_peripheral_coding_and_stimulus.pdf}
}

@article{saundersMiceCanLearn2019,
  title = {Mice Can Learn Phonetic Categories},
  author = {Saunders, Jonny L. and Wehr, Michael},
  year = {2019},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {145},
  number = {3},
  pages = {1168--1177},
  issn = {0001-4966},
  doi = {10.1121/1.5091776},
  file = {/Users/jonny/Papers/SaundersJ/2019/Saunders_2019_Mice can learn phonetic categories2.pdf;/Users/jonny/Zotero/storage/W3FSZK8G/1.html}
}

@article{schertzPhoneticCueWeighting2020,
  title = {Phonetic Cue Weighting in Perception and Production},
  author = {Schertz, Jessamyn and Clare, Emily J.},
  year = {2020},
  journal = {WIREs Cognitive Science},
  volume = {11},
  number = {2},
  pages = {e1521},
  issn = {1939-5086},
  doi = {10.1002/wcs.1521},
  abstract = {Speech sound contrasts differ along multiple phonetic dimensions. During speech perception, listeners must decide which cues are relevant, and determine the relative importance of each cue, while also integrating other, signal-external cues. The comparison of cue weighting in perception and production bears on a range of theoretical issues including the processes underlying sound change, the time course of learning, the nature of cues, and the perception-production interface. Research examining the relative alignment of cue weighting across the modalities, on both a community and individual level, has revealed both parallels and asymmetries between the modalities. The extraordinarily wide range of ways that have been used to conceptualize and quantify cue weights reflects the inherent theoretical, methodological, and analytical differences between the two modalities. More consideration of the choices of analytical metrics, explicit discussion of the theoretical assumptions that underlie them, and systematic investigations of different types of cues will lead to more generalizable findings that can be incorporated into computational implementable models of speech processing. This article is categorized under: Linguistics {$>$} Language in Mind and Brain Psychology {$>$} Language},
  langid = {english},
  keywords = {_tablet,cue weighting,phonetics,speech perception,speech production},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.1521},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SchertzJ/schertz_2020_phonetic_cue_weighting_in_perception_and_production.pdf}
}

@article{schiavoCapacitiesNeuralMechanisms2019,
  title = {Capacities and Neural Mechanisms for Auditory Statistical Learning across Species},
  author = {Schiavo, Jennifer K. and Froemke, Robert C.},
  year = {2019},
  month = may,
  journal = {Hearing Research},
  series = {Annual {{Reviews}} 2019},
  volume = {376},
  pages = {97--110},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2019.02.002},
  abstract = {Statistical learning has been proposed as a possible mechanism by which individuals can become sensitive to the structures of language fundamental for speech perception. Since its description in human infants, statistical learning has been described in human adults and several non-human species as a general process by which animals learn about stimulus-relevant statistics. The neurobiology of statistical learning is beginning to be understood, but many questions remain about the underlying mechanisms. Why is the developing brain particularly sensitive to stimulus and environmental statistics, and what neural processes are engaged in the adult brain to enable learning from statistical regularities in the absence of external reward or instruction? This review will survey the statistical learning abilities of humans and non-human animals with a particular focus on communicative vocalizations. We discuss the neurobiological basis of statistical learning, and specifically what can be learned by exploring this process in both humans and laboratory animals. Finally, we describe advantages of studying vocal communication in rodents as a means to further our understanding of the cortical plasticity mechanisms engaged during statistical learning. We examine the use of rodents in the context of pup retrieval, which is an auditory-based and experience-dependent form of maternal behavior.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SchiavoJ/schiavo_2019_capacities_and_neural_mechanisms_for_auditory_statistical_learning_across.pdf;/Users/jonny/Zotero/storage/VZURHLXR/S0378595518304441.html}
}

@article{schuesslerInterplayRandomnessStructure2020,
  title = {The Interplay between Randomness and Structure during Learning in {{RNNs}}},
  author = {Schuessler, Friedrich and Mastrogiuseppe, Francesca and Dubreuil, Alexis and Ostojic, Srdjan and Barak, Omri},
  year = {2020},
  month = oct,
  journal = {arXiv:2006.11036 [q-bio]},
  eprint = {2006.11036},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  abstract = {Recurrent neural networks (RNNs) trained on low-dimensional tasks have been widely used to model functional biological networks. However, the solutions found by learning and the effect of initial connectivity are not well understood. Here, we examine RNNs trained using gradient descent on different tasks inspired by the neuroscience literature. We find that the changes in recurrent connectivity can be described by low-rank matrices, despite the unconstrained nature of the learning algorithm. To identify the origin of the low-rank structure, we turn to an analytically tractable setting: training a linear RNN on a simplified task. We show how the low-dimensional task structure leads to low-rank changes to connectivity. This low-rank structure allows us to explain and quantify the phenomenon of accelerated learning in the presence of random initial connectivity. Altogether, our study opens a new perspective to understanding trained RNNs in terms of both the learning process and the resulting network structure.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Quantitative Biology - Neurons and Cognition},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SchuesslerF/schuessler_2020_the_interplay_between_randomness_and_structure_during_learning_in_rnns.pdf;/Users/jonny/Zotero/storage/GLNPQIX8/2006.html}
}

@article{schwarzReciprocityProximityRelations1980,
  title = {On the Reciprocity of Proximity Relations},
  author = {Schwarz, Gideon and Tversky, Amos},
  year = {1980},
  month = dec,
  journal = {Journal of Mathematical Psychology},
  volume = {22},
  number = {3},
  pages = {157--175},
  issn = {0022-2496},
  doi = {10.1016/0022-2496(80)90017-6},
  abstract = {The degree of reciprocity of a proximity order is the proportion, P(1), of elements for which the closest neighbor relation is symmetric, and the R value of each element is its rank in the proximity order from its closest neighbor. Assuming a random sampling of points, we show that Euclidean n-spaces produce a very high degree of reciprocity, P(1) {$\geq$} 12, and correspondingly low R values, E(R) {$\leq$} 2, for all n. The same bounds also apply to homogeneous graphs, in which the same number of edges meet at every node. Much less reciprocity and higher R values, however, can be attained in finite tree models and in the contrast model in which the ``distance'' between objects is a linear function of the numbers of their common and distinctive features.},
  langid = {english},
  file = {/Users/jonny/Zotero/storage/59DSN4VV/0022249680900176.html}
}

@article{sharpeeAnalyzingNeuralResponses2004,
  title = {Analyzing Neural Responses to Natural Signals: Maximally Informative Dimensions},
  shorttitle = {Analyzing Neural Responses to Natural Signals},
  author = {Sharpee, Tatyana and Rust, Nicole C. and Bialek, William},
  year = {2004},
  month = feb,
  journal = {Neural Computation},
  volume = {16},
  number = {2},
  pages = {223--250},
  issn = {0899-7667},
  doi = {10.1162/089976604322742010},
  abstract = {We propose a method that allows for a rigorous statistical analysis of neural responses to natural stimuli that are nongaussian and exhibit strong correlations. We have in mind a model in which neurons are selective for a small number of stimulus dimensions out of a high-dimensional stimulus space, but within this subspace the responses can be arbitrarily nonlinear. Existing analysis methods are based on correlation functions between stimuli and responses, but these methods are guaranteed to work only in the case of gaussian stimulus ensembles. As an alternative to correlation functions, we maximize the mutual information between the neural responses and projections of the stimulus onto low-dimensional subspaces. The procedure can be done iteratively by increasing the dimensionality of this subspace. Those dimensions that allow the recovery of all of the information between spikes and the full unprojected stimuli describe the relevant subspace. If the dimensionality of the relevant subspace indeed is small, it becomes feasible to map the neuron's input-output function even under fully natural stimulus conditions. These ideas are illustrated in simulations on model visual and auditory neurons responding to natural scenes and sounds, respectively.},
  langid = {english},
  pmid = {15006095},
  keywords = {_tablet,Acoustic Stimulation,Action Potentials,Algorithms,Animals,Artifacts,Auditory Cortex,Brain,Humans,Models; Neurological,Neurons; Afferent,Normal Distribution,Photic Stimulation,Sensation,Signal Processing; Computer-Assisted,Visual Cortex},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SharpeeT/sharpee_2004_analyzing_neural_responses_to_natural_signals.pdf;/Users/jonny/Zotero/storage/FXTIHIFK/sharpee_2004_analyzing_neural_responses_to_natural_signals.pdf}
}

@article{sharpeeHierarchicalRepresentationsAuditory2011b,
  title = {Hierarchical Representations in the Auditory Cortex},
  author = {Sharpee, Tatyana O and Atencio, Craig A and Schreiner, Christoph E},
  year = {2011},
  month = oct,
  journal = {Current Opinion in Neurobiology},
  series = {Networks, Circuits and Computation},
  volume = {21},
  number = {5},
  pages = {761--767},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2011.05.027},
  abstract = {Understanding the neural mechanisms of invariant object recognition remains one of the major unsolved problems in neuroscience. A common solution that is thought to be employed by diverse sensory systems is to create hierarchical representations of increasing complexity and tolerance. However, in the mammalian auditory system many aspects of this hierarchical organization remain undiscovered, including the prominent classes of high-level representations (that would be analogous to face selectivity in the visual system or selectivity to bird's own song in the bird) and the dominant types of invariant transformations. Here we review the recent progress that begins to probe the hierarchy of auditory representations, and the computational approaches that can be helpful in achieving this feat.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SharpeeT/sharpee_2011_hierarchical_representations_in_the_auditory_cortex.pdf;/Users/jonny/Zotero/storage/AVZJYWYW/sharpee_2011_hierarchical_representations_in_the_auditory_cortex.pdf}
}

@article{Shepard1962,
  title = {The Analysis of Proximities: {{Multidimensional}} Scaling with an Unknown Distance Function. {{II}}},
  author = {Shepard, Roger N.},
  year = {1962},
  month = sep,
  journal = {Psychometrika},
  volume = {27},
  number = {3},
  pages = {219--246},
  publisher = {{Springer-Verlag}},
  issn = {0033-3123},
  doi = {10.1007/BF02289621},
  file = {/Users/jonny/Dropbox/papers/zotero/S/ShepardR/false}
}

@article{Shepard1962a,
  title = {The Analysis of Proximities: {{Multidimensional}} Scaling with an Unknown Distance Function. {{I}}.},
  author = {Shepard, Roger N.},
  year = {1962},
  month = jun,
  journal = {Psychometrika},
  volume = {27},
  number = {2},
  pages = {125--140},
  publisher = {{Springer-Verlag}},
  issn = {0033-3123},
  doi = {10.1007/BF02289630},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/S/ShepardR/shepard_1962_the_analysis_of_proximities.pdf}
}

@article{Shepard1970,
  title = {Second-Order Isomorphism of Internal Representations: {{Shapes}} of States},
  author = {Shepard, Roger N. and Chipman, Susan},
  year = {1970},
  journal = {Cognitive Psychology},
  volume = {1},
  number = {1},
  pages = {1--17},
  issn = {00100285},
  doi = {10.1016/0010-0285(70)90002-2},
  abstract = {It is argued that, while there is no structural resemblance between an individual internal representation and its corresponding external object, an approximate parallelism should nevertheless hold between the relations among different internal representations and the relations among their corresponding external objects. In support of this "second-order" type of isomorphism, subjective judgments of the similarities among the shapes of 15 states of the U. S. are found (a) to be very much the same whether the states to be compared are pictorially displayed or only imagined, and (b) to be related, in both cases to identifiable properties of their actual cartographic shapes. \textcopyright{} 1970.},
  keywords = {_tablet,\#nosource},
  file = {/Users/jonny/Dropbox/papers/zotero/S/ShepardR/shepard_1970_second-order_isomorphism_of_internal_representations.pdf}
}

@article{shiAnteriorAuditoryField2019,
  title = {Anterior {{Auditory Field Is Needed}} for {{Sound Categorization}} in {{Fear Conditioning Task}} of {{Adult Rat}}},
  author = {Shi, Zhiyue and Yan, Sumei and Ding, Yu and Zhou, Chang and Qian, Shaowen and Wang, Zhaoqun and Gong, Chen and Zhang, Meng and Zhang, Yanjie and Zhao, Yandong and Wen, Huizhong and Chen, Penghui and Deng, Qiyue and Luo, Tiantian and Xiong, Ying and Zhou, Yi},
  year = {2019},
  journal = {Frontiers in Neuroscience},
  volume = {13},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2019.01374},
  abstract = {Both primary auditory cortex (A1) and anterior auditory field (AAF) are core regions of auditory cortex of many mammalians. While the function of A1 has been well documented, the role of AAF in sound related behavioral remain largely unclear. Here in adult rats, sound evoked fear conditioning paradigm, surgical ablation and chemogenetic manipulations were used to examine the role of AAF in fear related sound context recognition. Precise surgical ablation of AAF cannot block sound evoked freezing behavior but the fear conditioning became non-selective to acoustic cue. Reversible inhibition of AAF using chemogenetic activation at either training or testing phase can both lead to strong yet non-selective sound evoked freezing behavior. These simple yet clear results suggested that in sound evoked fear conditioning, sound cue and detailed content in the cue (e.g. frequency) are processed through distinct neural circuits and AAF is a critical part in the cortex dependent pathway. In addition, AAF is needed and playing a gating role for precise recognition of sound content in fear conditioning task through inhibiting fear to harmless cues.},
  langid = {english},
  keywords = {_tablet,anterior auditory field,Auditory Cortex,chemogenetic deactivation,Fear conditioning,sound recognition},
  file = {/Users/jonny/Dropbox/papers/zotero/S/ShiZ/shi_2019_anterior_auditory_field_is_needed_for_sound_categorization_in_fear_conditioning.pdf;/Users/jonny/Zotero/storage/ZVBS9EBM/shi_2019_anterior_auditory_field_is_needed_for_sound_categorization_in_fear_conditioning.pdf}
}

@article{sjerpsSpeakernormalizedSoundRepresentations2019a,
  title = {Speaker-Normalized Sound Representations in the Human Auditory Cortex},
  author = {Sjerps, Matthias J. and Fox, Neal P. and Johnson, Keith and Chang, Edward F.},
  year = {2019},
  month = jun,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {2465},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10365-z},
  abstract = {The acoustic dimensions that distinguish speech sounds (like the vowel differences in "boot" and "boat") also differentiate speakers' voices. Therefore, listeners must normalize across speakers without losing linguistic information. Past behavioral work suggests an important role for auditory contrast enhancement in normalization: preceding context affects listeners' perception of subsequent speech sounds. Here, using intracranial electrocorticography in humans, we investigate whether and how such context effects arise in auditory cortex. Participants identified speech sounds that were preceded by phrases from two different speakers whose voices differed along the same acoustic dimension as target words (the lowest resonance of the vocal tract). In every participant, target vowels evoke a speaker-dependent neural response that is consistent with the listener's perception, and which follows from a contrast enhancement model. Auditory cortex processing thus displays a critical feature of normalization, allowing listeners to extract meaningful content from the voices of diverse speakers.},
  langid = {english},
  pmid = {31165733},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SjerpsM/sjerps_2019_speaker-normalized_sound_representations_in_the_human_auditory_cortex.pdf}
}

@article{smithEfficientAuditoryCoding2006a,
  title = {Efficient Auditory Coding},
  author = {Smith, Evan C. and Lewicki, Michael S.},
  year = {2006},
  month = feb,
  journal = {Nature},
  volume = {439},
  number = {7079},
  pages = {978--982},
  issn = {1476-4687},
  doi = {10.1038/nature04485},
  abstract = {The auditory neural code must serve a wide range of auditory tasks that require great sensitivity in time and frequency and be effective over the diverse array of sounds present in natural acoustic environments. It has been suggested that sensory systems might have evolved highly efficient coding strategies to maximize the information conveyed to the brain while minimizing the required energy and neural resources. Here we show that, for natural sounds, the complete acoustic waveform can be represented efficiently with a nonlinear model based on a population spike code. In this model, idealized spikes encode the precise temporal positions and magnitudes of underlying acoustic features. We find that when the features are optimized for coding either natural sounds or speech, they show striking similarities to time-domain cochlear filter estimates, have a frequency-bandwidth dependence similar to that of auditory nerve fibres, and yield significantly greater coding efficiency than conventional signal representations. These results indicate that the auditory code might approach an information theoretic optimum and that the acoustic structure of speech might be adapted to the coding capacity of the mammalian auditory system.},
  langid = {english},
  pmid = {16495999},
  keywords = {_tablet,Acoustic Stimulation,Adaptation; Physiological,Algorithms,Animals,Auditory Perception,Cochlea,Hearing,Humans,Models; Neurological,Noise,Sensitivity and Specificity,Sound,Speech,Time Factors},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SmithE/smith_2006_efficient_auditory_coding.pdf}
}

@article{souzaReliabilityRepeatabilitySpeech2018,
  title = {Reliability and {{Repeatability}} of the {{Speech Cue Profile}}},
  author = {Souza, Pamela and Wright, Richard and Gallun, Frederick and Reinhart, Paul},
  year = {2018},
  month = aug,
  journal = {Journal of speech, language, and hearing research: JSLHR},
  volume = {61},
  number = {8},
  pages = {2126--2137},
  issn = {1558-9102},
  doi = {10.1044/2018_JSLHR-H-17-0341},
  abstract = {Purpose: Researchers have long noted speech recognition variability that is not explained by the pure-tone audiogram. Previous work (Souza, Wright, Blackburn, Tatman, \& Gallun, 2015) demonstrated that a small number of listeners with sensorineural hearing loss utilized different types of acoustic cues to identify speechlike stimuli, specifically the extent to which the participant relied upon spectral (or temporal) information for identification. Consistent with recent calls for data rigor and reproducibility, the primary aims of this study were to replicate the pattern of cue use in a larger cohort and to verify stability of the cue profiles over time. Method: Cue-use profiles were measured for adults with sensorineural hearing loss using a syllable identification task consisting of synthetic speechlike stimuli in which spectral and temporal dimensions were manipulated along continua. For the first set, a static spectral shape varied from alveolar to palatal, and a temporal envelope rise time varied from affricate to fricative. For the second set, formant transitions varied from labial to alveolar and a temporal envelope rise time varied from approximant to stop. A discriminant feature analysis was used to determine to what degree spectral and temporal information contributed to stimulus identification. A subset of participants completed a 2nd visit using the same stimuli and procedures. Results: When spectral information was static, most participants were more influenced by spectral than by temporal information. When spectral information was dynamic, participants demonstrated a balanced distribution of cue-use patterns, with nearly equal numbers of individuals influenced by spectral or temporal cues. Individual cue profile was repeatable over a period of several months. Conclusion: In combination with previously published data, these results indicate that listeners with sensorineural hearing loss are influenced by different cues to identify speechlike sounds and that those patterns are stable over time.},
  langid = {english},
  pmcid = {PMC6198918},
  pmid = {30073277},
  keywords = {_tablet,Acoustic Stimulation,Adult,Audiometry; Pure-Tone,Cues,Female,Hearing Loss; Sensorineural,Humans,Male,Phonetics,Reproducibility of Results,Speech Perception},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SouzaP/souza_2018_reliability_and_repeatability_of_the_speech_cue_profile.pdf;/Users/jonny/Zotero/storage/966Q6XAW/souza_2018_reliability_and_repeatability_of_the_speech_cue_profile.pdf}
}

@article{SpeechSoundTraining,
  title = {Speech Sound Training Alters Auditory Processing in Rats},
  pages = {6},
  abstract = {Speech sounds evoke unique neural activity patterns in the primary auditory cortex (A1) of rats. Like humans, rats can easily learn to discriminate between speech sounds. Behavioral discrimination accuracy can be predicted by the neural similarity of the A1 response pattern to pairs of speech sounds. For example, pairs of sounds that evoke very similar A1 activity patterns are difficult for rats to discriminate, while pairs of sounds that evoke very distinct A1 activity patterns are easy for rats to discriminate. Rat models of autism exhibit many of the classic neural and behavioral deficits observed in individuals with autism. Extensive speech discrimination training alters auditory cortex responses in both experimentally na\"ive rats and in rat models of autism. However, in some autism models, training alone is insufficient to reverse neural processing deficits. Vagus nerve stimulation (VNS) triggers rapid, phasic release of plasticity promoting neuromodulators, which enhances plasticity in the auditory network when delivered with sound presentation. For example, pairing the sounds `rad' and `lad' with VNS increases the A1 response strength to the paired sounds. Ongoing work involves pairing VNS with auditory training in rat models of autism to improve both sound discrimination ability and the neural processing of sounds.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Zotero/storage/LLNDTVJP/speech_sound_training_alters_auditory_processing_in_rats.pdf}
}

@article{stilpEfficientCodingStatistically2012,
  title = {Efficient {{Coding}} and {{Statistically Optimal Weighting}} of {{Covariance}} among {{Acoustic Attributes}} in {{Novel Sounds}}},
  author = {Stilp, Christian E. and Kluender, Keith R.},
  editor = {Vicario, David S.},
  year = {2012},
  month = jan,
  journal = {PLoS ONE},
  volume = {7},
  number = {1},
  pages = {e30845},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0030845},
  abstract = {To the extent that sensorineural systems are efficient, redundancy should be extracted to optimize transmission of information, but perceptual evidence for this has been limited. Stilp and colleagues recently reported efficient coding of robust correlation (r = .97) among complex acoustic attributes (attack/decay, spectral shape) in novel sounds. Discrimination of sounds orthogonal to the correlation was initially inferior but later comparable to that of sounds obeying the correlation. These effects were attenuated for less-correlated stimuli (r = .54) for reasons that are unclear. Here, statistical properties of correlation among acoustic attributes essential for perceptual organization are investigated. Overall, simple strength of the principal correlation is inadequate to predict listener performance. Initial superiority of discrimination for statistically consistent sound pairs was relatively insensitive to decreased physical acoustic/psychoacoustic range of evidence supporting the correlation, and to more frequent presentations of the same orthogonal test pairs. However, increased range supporting an orthogonal dimension has substantial effects upon perceptual organization. Connectionist simulations and Eigenvalues from closed-form calculations of principal components analysis (PCA) reveal that perceptual organization is near-optimally weighted to shared versus unshared covariance in experienced sound distributions. Implications of reduced perceptual dimensionality for speech perception and plausible neural substrates are discussed.},
  langid = {english},
  file = {/Users/jonny/Dropbox/papers/zotero/S/StilpC/false;/Users/jonny/Papers/StilpC/2012/Stilp_2012_Efficient Coding and Statistically Optimal Weighting of Covariance among.pdf}
}

@article{stilpRapidEfficientCoding2010,
  title = {Rapid Efficient Coding of Correlated Complex Acoustic Properties},
  author = {Stilp, C. E. and Rogers, T. T. and Kluender, K. R.},
  year = {2010},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {107},
  number = {50},
  pages = {21914--21919},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1009020107},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/S/StilpC/stilp_2010_rapid_efficient_coding_of_correlated_complex_acoustic_properties.pdf;/Users/jonny/Papers/StilpC/2010/Stilp_2010_Rapid efficient coding of correlated complex acoustic properties.pdf}
}

@misc{StructureEveryScale,
  title = {Structure at Every Scale: {{A}} Semantic Network Account of the Similarities between Unrelated Concepts. - {{PsycNET}}},
  shorttitle = {Structure at Every Scale},
  abstract = {APA PsycNet DoiLanding page},
  howpublished = {/doiLanding?doi=10.1037\%2Fxge0000192},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Zotero/storage/UERS9MHF/structure_at_every_scale.pdf;/Users/jonny/Zotero/storage/5BYAQ4JY/doiLanding.html}
}

@article{takahashiLearningstagedependentFieldspecificMap2011,
  title = {Learning-Stage-Dependent, Field-Specific, Map Plasticity in the Rat Auditory Cortex during Appetitive Operant Conditioning},
  author = {Takahashi, H. and Yokota, R. and Funamizu, A. and Kose, H. and Kanzaki, R.},
  year = {2011},
  month = dec,
  journal = {Neuroscience},
  volume = {199},
  pages = {243--258},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2011.09.046},
  abstract = {Cortical reorganizations during acquisition of motor skills and experience-dependent recovery after deafferentation consist of several distinct phases, in which expansion of receptive fields is followed by the shrinkage and use-dependent refinement. In perceptual learning, however, such non-monotonic, stage-dependent plasticity remains elusive in the sensory cortex. In the present study, microelectrode mapping characterized plasticity in the rat auditory cortex, including primary, anterior, and ventral/suprarhinal auditory fields (A1, AAF, and VAF/SRAF), at the early and late stages of appetitive operant conditioning. We first demonstrate that most plasticity at the early stage was tentative, and that long-lasting plasticity after extended training was able to be categorized into either early- or late-stage-dominant plasticity. Second, training-induced plasticity occurred both locally and globally with a specific temporal order. Conditioned-stimulus (CS) frequency used in the task tended to be locally over-represented in AAF at the early stage and in VAF/SRAF at the late stage. The behavioral relevance of neural responses suggests that the local plasticity also occurred in A1 at the early stage. In parallel, the tone-responsive area globally shrank at the late stage independently of CS frequency, and this shrinkage was also correlated with the behavioral improvements. Thus, the stage-dependent plasticity may commonly underlie cortical reorganization in the perceptual learning, yet the interactions of local and global plasticity have led to more complicated reorganization than previously thought. Field-specific plasticity has important implications for how each field subserves in the learning; for example, consistent with recent notions, A1 should construct filters to better identify auditory objects at the early stage, while VAF/SRAF contribute to hierarchical computation and storage at the late stage.},
  langid = {english},
  keywords = {_tablet,auditory cortex,disinhibition,homeostasis,learning,operant conditioning,plasticity},
  file = {/Users/jonny/Dropbox/papers/zotero/T/TakahashiH/takahashi_2011_learning-stage-dependent,_field-specific,_map_plasticity_in_the_rat_auditory.pdf}
}

@article{Tversky1970,
  title = {The Dimensional Representation and the Metric Structure of Similarity Data},
  author = {Tversky, Amos and Krantz, David H.},
  year = {1970},
  month = oct,
  journal = {Journal of Mathematical Psychology},
  volume = {7},
  number = {3},
  pages = {572--596},
  issn = {00222496},
  doi = {10.1016/0022-2496(70)90041-6},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/T/TverskyA/tversky_1970_the_dimensional_representation_and_the_metric_structure_of_similarity_data.pdf}
}

@article{Tversky1977,
  title = {Features of {{Similarity}}},
  author = {Tversky, Amos},
  year = {1977},
  volume = {84},
  number = {4},
  abstract = {The metric and dimensional assumptions that underlie the geometric represen-tation of similarity are questioned on both theoretical and empirical grounds. A new set-theoretical approach to similarity is developed in which objects are represented as collections of features, and similarity is described as a feature-matching process. Specifically, a set of qualitative assumptions is shown to imply the contrast model, which expresses the similarity between objects as a linear combination of the measures of their common and distinctive features. Several predictions of the contrast model are tested in studies of similarity with both semantic and perceptual stimuli. The model is used to uncover, analyze, and explain a variety of empirical phenomena such as the role of common and distinctive features, the relations between judgments of similarity and differ-ence, the presence of asymmetric similarities, and the effects of context on judgments of similarity. The contrast model generalizes standard representa-tions of similarity data in terms of clusters and trees. It is also used to analyze the relations of prototypicality and family resemblance. Similarity plays a fundamental role in errors of substitution, and correlation between theories of knowledge and behavior. It serves occurrences. Analyses of these data attempt to as an organizing principle by which individuals explain the observed similarity relations and classify objects, form concepts, and make gen-to capture the underlying structure of the ob-eralizations. Indeed, the concept of similarity jects under study. is ubiquitous in psychological theory. It under-The theoretical analysis of similarity rela-lies the accounts of stimulus and response tions has been dominated by geometric generalization in learning, it is employed to models. These models represent objects as explain errors in memory and pattern recogni-points in some coordinate space such that the tion, and it is central to the analysis of con-observed dissimilarities between objects cor-notative meaning. respond to the metric distances between the Similarity or dissimilarity data appear in respective points. Practically all analyses of different forms: ratings of pairs, sorting of proximity data have been metric in nature, objects, communality between associations, although some (e.g., hierarchical clustering) yield tree-like structures rather than dimen-This paper benefited from fruitful discussions with sionallv},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/T/TverskyA/tversky_1977_features_of_similarity.pdf}
}

@article{Tversky1982,
  title = {Similarity, Separability, and the Triangle Inequality.},
  author = {Tversky, Amos and Gati, Itamar},
  year = {1982},
  journal = {Psychological Review},
  volume = {89},
  number = {2},
  pages = {123--154},
  issn = {0033-295X},
  doi = {10.1037/0033-295X.89.2.123},
  abstract = {An alternative analysis of geometric models of proximity data, based on a feature-matching model, leads to the coincidence hypothesis that the dissimilarity between objects that differ on 2 separable dimensions is larger than predicted from their unidimensional differences on the basis of the triangle inequality and segmental additivity. A series of studies of 2-dimensional stimuli with separable attributes (including house plants, parallelograms, schematic faces, and histograms), using judgments of similarity and dissimilarity, classification, inference, and recognition errors, all support the coincidence hypothesis. The size of the effect is determined by the separability of the stimuli, the transparency of the dimensional structure, and the discriminability of the levels within each dimension. Applications of the coincidence effect to inductive inference are investigated, and its relations to selective attention and spatial density are discussed. It is concluded that the triangle inequality and segmental additivity cannot be jointly satisfied for separable attributes. The implications of this result for multidimensional scaling are discussed. (57 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {1939-1471(Electronic);0033-295X(Print)},
  pmid = {7089125},
  keywords = {_tablet,*Classification (Cognitive Process),*Inductive Deductive Reasoning,*Inference,\#nosource,Multidimensional Scaling},
  file = {/Users/jonny/Dropbox/papers/zotero/T/TverskyA/tversky_1982_similarity,_separability,_and_the_triangle_inequality.pdf}
}

@article{tverskyInformationRewardBinary1966,
  title = {Information versus Reward in Binary Choices},
  author = {Tversky, Amos and Edwards, Ward},
  year = {1966},
  journal = {Journal of Experimental Psychology},
  volume = {71},
  number = {5},
  pages = {680--683},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {0022-1015(Print)},
  doi = {10.1037/h0023123},
  abstract = {This binary prediction experiment isolates the rewarding characteristics of outcomes from their informative characteristics. On each of 1000 trials S had to choose between 2 acts. He could guess which of 2 lights was correct, whereupon a nickel was added to or subtracted from a sum to be paid to him after the experiment, but he received no immediate feedback. Or he could observe which light was correct with no financial consequences. All Ss deviated from optimal strategy by seeking far too much information. Nonstationary instructions hindered performance, hypothetical predictions improved it. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Information,Rewards},
  file = {/Users/jonny/Zotero/storage/X8B9AK3B/1966-06311-001.html}
}

@misc{tverskyStudiesSimilarity1978,
  title = {Studies of Similarity},
  author = {Tversky, A. and Gati, Itamar},
  year = {1978},
  journal = {undefined},
  abstract = {Any event in the history of the organism is, in a sense, unique. Consequently, recognition, learning, and judgment presuppose an ability to categorize stimuli and classify situations by similarity . As Quine (1969) puts it: \&quot;There is nothing more basic to thought and language than our sense of similarity ; our sorting of things into kinds [p . 1161 .\&quot; Indeed, the notion of similarity that appears under such different names as proximity, resemblance, communality, representativeness, and psychological distance is fundamental to theories of perception, learning, and judgment . This chapter outlines a new theoretical analysis of similarity and investigates some of its empirical consequences . The theoretical analysis of similarity relations has been dominated by geometric models. Such models represent each object as a point in some coordinate space so that the metric distances between the points reflect the observed similarities between the respective objects . In general, the space is assumed to be Euclidean, and the purpose of the analysis is to embed the objects in a space of minimum dimensionality on the basis of the observed similarities, see Shepard (1974) . In a recent paper (Tversky, 1977), the first author challenged the dimensionalmetric assumptions that underlie the geometric approach to similarity and developed an alternative feature-theoretical approach to the analysis of similarity relations. In this approach, each object a is characterized by a set of features, denoted A, and the observed similarity of a to b, denoted s(a, b), is expressed as a function of their common and distinctive features (see Fig . 4.1) . That is, the observed similarity s(a, b) is expressed as a function of three arguments : A f1B, the features shared by a and b ;A B, the features of a that are not shared by b ; B A, the features of b that are not shared by a . Thus the similarity between},
  howpublished = {/paper/Studies-of-similarity-Tversky-Gati/93ad5669cc4d8300de0d5d1f9e2c0ed2479d9596},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/T/TverskyA/tversky_1978_studies_of_similarity.pdf;/Users/jonny/Zotero/storage/5BSVHT47/93ad5669cc4d8300de0d5d1f9e2c0ed2479d9596.html}
}

@article{tverskySubstitutabilitySimilarityBinary1969,
  title = {Substitutability and Similarity in Binary Choices},
  author = {Tversky, Amos and Edward Russo, J.},
  year = {1969},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  volume = {6},
  number = {1},
  pages = {1--12},
  issn = {0022-2496},
  doi = {10.1016/0022-2496(69)90027-3},
  abstract = {The assumption that a binary choice probability is expressible as a monotone function of the scale values of the two alternatives is investigated. Four different conditions are shown to be equivalent forms of the same substitutability, or independence, principle which underlies most probabilistic theories of choice behavior. In a study of judgments of relative size, the independence principle is contrasted with the hypothesis that interstimulus similarity facilitates discrimination. The data reject the independence principle while supporting the similarity hypothesis.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/T/TverskyA/tversky_1969_substitutability_and_similarity_in_binary_choices.pdf;/Users/jonny/Zotero/storage/N6SNGV3M/0022249669900273.html}
}

@article{tverskySubstitutabilitySimilarityBinary1969a,
  title = {Substitutability and Similarity in Binary Choices},
  author = {Tversky, Amos and Edward Russo, J.},
  year = {1969},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  volume = {6},
  number = {1},
  pages = {1--12},
  issn = {0022-2496},
  doi = {10.1016/0022-2496(69)90027-3},
  abstract = {The assumption that a binary choice probability is expressible as a monotone function of the scale values of the two alternatives is investigated. Four different conditions are shown to be equivalent forms of the same substitutability, or independence, principle which underlies most probabilistic theories of choice behavior. In a study of judgments of relative size, the independence principle is contrasted with the hypothesis that interstimulus similarity facilitates discrimination. The data reject the independence principle while supporting the similarity hypothesis.},
  langid = {english},
  file = {/Users/jonny/Dropbox/papers/zotero/T/TverskyA/false;/Users/jonny/Zotero/storage/FXRRZEYD/0022249669900273.html}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jonny/Dropbox/papers/zotero/V/VaswaniA/vaswani_2017_attention_is_all_you_need.pdf}
}

@misc{vongAdditionalFeaturesHelp2018,
  title = {Do Additional Features Help or Hurt Category Learning? {{The}} Curse of Dimensionality in Human Learners},
  shorttitle = {Do Additional Features Help or Hurt Category Learning?},
  author = {Vong, Wai Keen and Hendrickson, Andrew T. and Navarro, Danielle and Perfors, Andrew},
  year = {2018},
  month = jun,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/bjh68},
  abstract = {The curse of dimensionality, which has been widely studied in statistics and machine learning, occurs when additional features causes the size of the feature space to grow so quickly that learning classification rules becomes increasingly difficult. How do people overcome the curse of dimensionality when acquiring real-world categories that have many different features? Here we investigate the possibility that the structure of categories can help. We show that when categories follow a family resemblance structure, people are unaffected by the presence of additional features in learning. However, when categories are based on a single feature, they fall prey to the curse and having additional irrelevant features hurts performance. We compare and contrast these results to three different computational models to show that a model with limited computational capacity best captures human performance across almost all of the conditions in both experiments.},
  keywords = {_tablet,category learning,Cognitive Psychology,curse of dimensionality,Social and Behavioral Sciences,supervised learning},
  file = {/Users/jonny/Dropbox/papers/zotero/V/VongW/vong_2018_do_additional_features_help_or_hurt_category_learning.pdf}
}

@article{walkerMultiplexedRobustRepresentations2011,
  title = {Multiplexed and Robust Representations of Sound Features in Auditory Cortex},
  author = {Walker, Kerry M. M. and Bizley, Jennifer K. and King, Andrew J. and Schnupp, Jan W. H.},
  year = {2011},
  month = oct,
  journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  volume = {31},
  number = {41},
  pages = {14565--14576},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.2074-11.2011},
  abstract = {We can recognize the melody of a familiar song when it is played on different musical instruments. Similarly, an animal must be able to recognize a warning call whether the caller has a high-pitched female or a lower-pitched male voice, and whether they are sitting in a tree to the left or right. This type of perceptual invariance to "nuisance" parameters comes easily to listeners, but it is unknown whether or how such robust representations of sounds are formed at the level of sensory cortex. In this study, we investigate whether neurons in both core and belt areas of ferret auditory cortex can robustly represent the pitch, formant frequencies, or azimuthal location of artificial vowel sounds while the other two attributes vary. We found that the spike rates of the majority of cortical neurons that are driven by artificial vowels carry robust representations of these features, but the most informative temporal response windows differ from neuron to neuron and across five auditory cortical fields. Furthermore, individual neurons can represent multiple features of sounds unambiguously by independently modulating their spike rates within distinct time windows. Such multiplexing may be critical to identifying sounds that vary along more than one perceptual dimension. Finally, we observed that formant information is encoded in cortex earlier than pitch information, and we show that this time course matches ferrets' behavioral reaction time differences on a change detection task.},
  langid = {english},
  pmcid = {PMC3272412},
  pmid = {21994373},
  keywords = {_tablet,Acoustic Stimulation,Action Potentials,Animals,Auditory Cortex,Auditory Pathways,Bias,Evoked Potentials; Auditory,Female,Ferrets,Neurons,Reaction Time,Sound,Sound Localization,Spectrum Analysis,Statistics; Nonparametric},
  file = {/Users/jonny/Dropbox/papers/zotero/W/WalkerK/walker_2011_multiplexed_and_robust_representations_of_sound_features_in_auditory_cortex.pdf;/Users/jonny/Zotero/storage/WWJ9UA83/walker_2011_multiplexed_and_robust_representations_of_sound_features_in_auditory_cortex.pdf}
}

@article{Wang2005a,
  title = {Sustained Firing in Auditory Cortex Evoked by Preferred Stimuli.},
  author = {Wang, Xiaoqin and Lu, Thomas and Snider, Ross K and Liang, Li},
  year = {2005},
  journal = {Nature},
  volume = {435},
  number = {7040},
  pages = {341--6},
  issn = {1476-4687},
  doi = {10.1038/nature03565},
  abstract = {It has been well documented that neurons in the auditory cortex of anaesthetized animals generally display transient responses to acoustic stimulation, and typically respond to a brief stimulus with one or fewer action potentials. The number of action potentials evoked by each stimulus usually does not increase with increasing stimulus duration. Such observations have long puzzled researchers across disciplines and raised serious questions regarding the role of the auditory cortex in encoding ongoing acoustic signals. Contrary to these long-held views, here we show that single neurons in both primary (area A1) and lateral belt areas of the auditory cortex of awake marmoset monkeys (Callithrix jacchus) are capable of firing in a sustained manner over a prolonged period of time, especially when they are driven by their preferred stimuli. In contrast, responses become more transient or phasic when auditory cortex neurons respond to non-preferred stimuli. These findings suggest that when the auditory cortex is stimulated by a sound, a particular population of neurons fire maximally throughout the duration of the sound. Responses of other, less optimally driven neurons fade away quickly after stimulus onset. This results in a selective representation of the sound across both neuronal population and time.},
  isbn = {1476-4687 (Electronic)\textbackslash n0028-0836 (Linking)},
  pmid = {15902257},
  keywords = {Acoustic Stimulation,Action Potentials,Action Potentials: physiology,Animals,Auditory Cortex,Auditory Cortex: cytology,Auditory Cortex: physiology,Auditory Perception,Auditory Perception: physiology,Callithrix,Callithrix: physiology,Models,Neurological,Neurons,Neurons: physiology,Sound,Time Factors,Wakefulness,Wakefulness: physiology},
  file = {/Users/jonny/Dropbox/papers/zotero/W/WangX/wang_2005_sustained_firing_in_auditory_cortex_evoked_by_preferred_stimuli.pdf;/Users/jonny/Zotero/storage/BJQDPFTC/Wang et al. - 2005 - Sustained firing in auditory cortex evoked by preferred stimuli(6).pdf}
}

@article{wangNeuralCodingStrategies2007,
  title = {Neural Coding Strategies in Auditory Cortex},
  author = {Wang, Xiaoqin},
  year = {2007},
  month = jul,
  journal = {Hearing Research},
  series = {Auditory {{Cortex}} 2006 - {{The Listening Brain}}},
  volume = {229},
  number = {1},
  pages = {81--93},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2007.01.019},
  abstract = {In contrast to the visual system, the auditory system has longer subcortical pathways and more spiking synapses between the peripheral receptors and the cortex. This unique organization reflects the needs of the auditory system to extract behaviorally relevant information from a complex acoustic environment using strategies different from those used by other sensory systems. The neural representations of acoustic information in auditory cortex can be characterized by three types: (1) isomorphic (faithful) representations of acoustic structures; (2) non-isomorphic transformations of acoustic features and (3) transformations from acoustical to perceptual dimensions. The challenge facing auditory neurophysiologists is to understand the nature of the latter two transformations. In this article, I will review recent studies from our laboratory regarding temporal discharge patterns in auditory cortex of awake marmosets and cortical representations of time-varying signals. Findings from these studies show that (1) firing patterns of neurons in auditory cortex are dependent on stimulus optimality and context and (2) the auditory cortex forms internal representations of sounds that are no longer faithful replicas of their acoustic structures.},
  langid = {english},
  keywords = {_tablet,Auditory cortex,Marmoset,Neural coding,Temporal processing},
  file = {/Users/jonny/Dropbox/papers/zotero/W/WangX/wang_2007_neural_coding_strategies_in_auditory_cortex.pdf}
}

@article{warburtonGettingStuckRut2020,
  title = {Getting Stuck in a Rut as an Emergent Feature of a Dynamic Decision-Making System},
  author = {Warburton, Matthew and Brookes, Jack and Hasan, Mohamed and Leonetti, Matteo and Dogar, Mehmet and Wang, He and Cohn, Anthony G. and Mushtaq, Faisal and {Mon-Williams}, Mark A.},
  year = {2020},
  month = jun,
  journal = {bioRxiv},
  pages = {2020.06.02.127860},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.06.02.127860},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Human sensorimotor decision-making has a tendency to get `stuck in a rut', being biased towards selecting a previously implemented action structure (`hysteresis'). Existing explanations cannot provide a principled account of when hysteresis will occur. We propose that hysteresis is an emergent property of a dynamical system learning from the consequences of its actions. To examine this, 152 participants moved a cursor to a target on a tablet device whilst avoiding an obstacle. Hysteresis was observed when the obstacle moved sequentially across the screen between trials, but not with random obstacle placement. Two further experiments (n = 20) showed an attenuation when time and resource constraints were eased. We created a simple computational model capturing dynamic probabilistic estimate updating that showed the same patterns of results. This provides the first computational demonstration of how sensorimotor decision-making can get `stuck in a rut' through the dynamic updating of its probability estimates.{$<$}/p{$><$}h3{$>$}Significance Statement{$<$}/h3{$>$} {$<$}p{$>$}Humans show a bias to select the organisational structure of a recently carried out action, even when an alternative option is available with lower costs. This `hysteresis' is said to be more efficient than creating a new plan and it has been interpreted as a `design feature' within decision-making systems. We suggest such teleological arguments are redundant, with hysteresis being a naturally emergent property of a dynamic control system that evolved to operate effectively in an uncertain and partially observable world. Empirical experimentation and simulations from a `first principle' computational model of decision-making were consistent with our hypothesis. The identification of such a mechanism can inform robotics research, suggesting how robotic agents can show human-like flexibility in complex dynamic environments.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/W/WarburtonM/warburton_2020_getting_stuck_in_a_rut_as_an_emergent_feature_of_a_dynamic_decision-making.pdf;/Users/jonny/Zotero/storage/KTR4FZLL/2020.06.02.html}
}

@article{winklerModelingAuditoryScene2009a,
  title = {Modeling the Auditory Scene: Predictive Regularity Representations and Perceptual Objects},
  shorttitle = {Modeling the Auditory Scene},
  author = {Winkler, Istv{\'a}n and Denham, Susan L. and Nelken, Israel},
  year = {2009},
  month = dec,
  journal = {Trends in Cognitive Sciences},
  volume = {13},
  number = {12},
  pages = {532--540},
  issn = {13646613},
  doi = {10.1016/j.tics.2009.09.003},
  langid = {english},
  file = {/Users/jonny/Papers/WinklerI/2009/Winkler_2009_Modeling the auditory scene.pdf;/Users/jonny/Zotero/storage/XHWBFAHL/winkler2009.pdf}
}

@book{wittgensteinPhilosophicalInvestigations1968,
  title = {Philosophical Investigations},
  author = {Wittgenstein, Ludwig},
  year = {1968},
  publisher = {{Basil Blackwell}},
  address = {{Oxford}},
  isbn = {978-0-631-11900-5},
  langid = {english},
  lccn = {B3376.W563 P53 1968b},
  keywords = {_tablet},
  file = {/Users/jonny/Zotero/storage/QIGSD7AK/wittgenstein_1968_philosophical_investigations.pdf}
}

@article{wuFeaturedependentIntrinsicFunctional2018,
  title = {Feature-Dependent Intrinsic Functional Connectivity across Cortical Depths in the Human Auditory Cortex},
  author = {Wu, Pu-Yeh and Chu, Ying-Hua and Lin, Jo-Fu Lotus and Kuo, Wen-Jui and Lin, Fa-Hsuan},
  year = {2018},
  month = sep,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {13287},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-31292-x},
  abstract = {Frequency preference and spectral tuning are two cardinal features of information processing in the auditory cortex. However, sounds should not only be processed in separate frequency bands because information needs to be integrated to be meaningful. One way to better understand the integration of acoustic information is to examine the functional connectivity across cortical depths, as neurons are already connected differently across laminar layers. Using a tailored receiver array and surface-based cortical depth analysis, we revealed the frequency\textendash preference as well as tuning\textendash width dependent intrinsic functional connectivity (iFC) across cortical depths in the human auditory cortex using functional magnetic resonance imaging (fMRI). We demonstrated feature-dependent iFC in both core and noncore regions at all cortical depths. The selectivity of frequency\textendash preference dependent iFC was higher at deeper depths than at intermediate and superficial depths in the core region. Both the selectivity of frequency\textendash preference and tuning\textendash width dependent iFC were stronger in the core than in the noncore region at deep cortical depths. Taken together, our findings provide evidence for a cortical depth-specific feature-dependent functional connectivity in the human auditory cortex.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/W/WuP/wu_2018_feature-dependent_intrinsic_functional_connectivity_across_cortical_depths_in.pdf;/Users/jonny/Zotero/storage/QWG9MJV2/wu_2018_feature-dependent_intrinsic_functional_connectivity_across_cortical_depths_in.pdf}
}

@article{xuLaminarSpecificityFunctional2009,
  title = {Laminar {{Specificity}} of {{Functional Input}} to {{Distinct Types}} of {{Inhibitory Cortical Neurons}}},
  author = {Xu, Xiangmin and Callaway, Edward M.},
  year = {2009},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {29},
  number = {1},
  pages = {70--85},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4104-08.2009},
  abstract = {Despite the presence of numerous inhibitory cell types, laminar excitatory input has only been characterized for limited identified types, and it is unknown whether there are differences between cell types in their laminar sources of inhibitory input. In the present study, we characterized sources of local input to nine distinct types of layer 2/3 inhibitory neurons in living slices of mouse somatosensory cortex. Whole-cell recordings from identified cell types, facilitated by use of transgenic mice expressing green fluorescent protein in limited inhibitory neuron populations, were combined with laser scanning photostimulation. We found that each inhibitory cell type received distinct excitatory and inhibitory laminar input patterns. Excitatory inputs could be grouped into three categories. All inhibitory cell types received strong excitation from layer 2/3, and for calretinin (CR)-positive Martinotti cells and burst-spiking interneurons, this was their dominant source of excitatory input. Three other cell types, including fast-spiking basket cells, CR-negative Martinotti cells, and bipolar interneurons, also received strong excitatory input from layer 4. The remaining four inhibitory cell types, including chandelier cells, neurogliaform cells, irregular spiking basket cells, and regular spiking presumptive basket cells, received strong excitatory input from layer 5A and not layer 4. Laminar sources of inhibitory input varied between cell types and could not be predicted from the sources of excitatory input. Thus, there are cell-type specific differences in laminar sources of both excitation and inhibition, and complementary input patterns from layer 4 versus layer 5A suggest cell type differences in their relationships to lemniscal versus paralemniscal pathways.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2009 Society for Neuroscience 0270-6474/09/290070-16\$15.00/0},
  langid = {english},
  pmid = {19129386},
  keywords = {_tablet,barrel cortex,GFP,interneurons,photostimulation,somatosensory cortex,transgenic},
  file = {/Users/jonny/Dropbox/papers/zotero/X/XuX/xu_2009_laminar_specificity_of_functional_input_to_distinct_types_of_inhibitory.pdf;/Users/jonny/Zotero/storage/VQWUFX2U/xu_2009_laminar_specificity_of_functional_input_to_distinct_types_of_inhibitory.pdf}
}

@article{yiEncodingSpeechSounds2019,
  title = {The {{Encoding}} of {{Speech Sounds}} in the {{Superior Temporal Gyrus}}},
  author = {Yi, Han Gyol and Leonard, Matthew K. and Chang, Edward F.},
  year = {2019},
  month = jun,
  journal = {Neuron},
  volume = {102},
  number = {6},
  pages = {1096--1110},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.04.023},
  langid = {english},
  pmid = {31220442},
  keywords = {_tablet,acoustic-phonetic features,auditory cortex,context-dependent representation,electrocorticography,phonological sequence,speech processing,superior temporal gyrus,temporal integration,temporal landmarks,temporally recurrent connections},
  file = {/Users/jonny/Dropbox/papers/zotero/Y/YiH/yi_2019_the_encoding_of_speech_sounds_in_the_superior_temporal_gyrus.pdf;/Users/jonny/Zotero/storage/ZSWH63WD/S0896-6273(19)30380-0.html}
}

@article{zadorCritiquePureLearning2019,
  title = {A {{Critique}} of {{Pure Learning}}: {{What Artificial Neural Networks}} Can {{Learn}} from {{Animal Brains}}},
  shorttitle = {A {{Critique}} of {{Pure Learning}}},
  author = {Zador, Anthony M.},
  year = {2019},
  month = mar,
  journal = {bioRxiv},
  pages = {582643},
  doi = {10.1101/582643},
  abstract = {{$<$}h3{$>$}ABSTRACT{$<$}/h3{$>$} {$<$}p{$>$}Over the last decade, artificial neural networks (ANNs), have undergone a revolution, catalyzed in large part by better tools for supervised learning. However, training such networks requires enormous data sets of labeled examples, whereas young animals (including humans) typically learn with few or no labeled examples. This stark contrast with biological learning has led many in the ANN community posit that instead of supervised paradigms, animals must rely instead primarily on unsupervised learning, leading the search for better unsupervised algorithms. Here we argue that much of an animal's behavioral repertoire is not the result of clever learning algorithms\textemdash supervised or unsupervised\textemdash but arises instead from behavior programs already present at birth. These programs arise through evolution, are encoded in the genome, and emerge as a consequence of wiring up the brain. Specifically, animals are born with highly structured brain connectivity, which enables them learn very rapidly. Recognizing the importance of the highly structured connectivity suggests a path toward building ANNs capable of rapid learning.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/jonny/Papers/ZadorA/2019/Zador_2019_A Critique of Pure Learning.pdf;/Users/jonny/Zotero/storage/AXVRU6GP/582643v1.html}
}


