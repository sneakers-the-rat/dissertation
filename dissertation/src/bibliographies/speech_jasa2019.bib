Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Werker1988,
author= {Werker, Janet F and Lalonde, Chris E},
title = {{Cross-language speech perception: Initial capabilities and developmental change.}},
journal = {Developmental psychology},
volume = {24},
number = {5},
pages = {672},
year = {1988}
}

@article{Wright2004,
author = {Wright, R.},
title = {{A review of perceptual cues and cue robustness}},
journal = {Phonetically based phonology},
pages = {34--57},
year = {2004}
}

@article{Mathworks,
title = {{Pitch Shifting and Time Dilation Using a Phase Vocoder in MATLAB}},
url = {https://www.mathworks.com/help/audio/examples/pitch-shifting-and-time-dilation-using-a-phase-vocoder-in-matlab.html}
}


@article{Wickham2007,
author = {Wickham, H.},
title = {{Reshaping data with the reshape package}},
journal = {Journal of Statistical Software} ,
volume = {21},
number = {12},
year = {2007},
url = {http://www.jstatsoft.org/v21/i12/paper}
}


@article{Weible2014,
abstract = {Auditory cortex is necessary for the perceptual detection of brief gaps in noise, but is not necessary for many other auditory tasks such as frequency discrimination, prepulse inhibition of startle responses, or fear conditioning with pure tones. It remains unclear why auditory cortex should be necessary for some auditory tasks but not others. One possibility is that auditory cortex is causally involved in gap detection and other forms of temporal processing in order to associate meaning with temporally structured sounds. This predicts that auditory cortex should be necessary for associating meaning with gaps. To test this prediction, we developed a fear conditioning paradigm for mice based on gap detection. We found that pairing a 10 or 100 ms gap with an aversive stimulus caused a robust enhancement of gap detection measured 6 h later, which we refer to as fear potentiation of gap detection. Optogenetic suppression of auditory cortex during pairing abolished this fear potentiation, indicating that auditory cortex is critically involved in associating temporally structured sounds with emotionally salient events.},
annote = {From Duplicate 1 (Auditory cortex is required for fear potentiation of gap detection. - Weible, Aldis P; Liu, Christine; Niell, Cristopher M; Wehr, Michael)

10.1523/JNEUROSCI.3408-14.2014},
author = {Weible, Aldis P and Liu, Christine and Niell, Cristopher M and Wehr, Michael},
doi = {10.1523/JNEUROSCI.3408-14.2014},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Weible et al. - 2014 - Auditory cortex is required for fear potentiation of gap detection(3).pdf:pdf;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Weible et al. - 2014 - Auditory cortex is required for fear potentiation of gap detection(4).pdf:pdf},
isbn = {1529-2401 (Electronic)$\backslash$r0270-6474 (Linking)},
issn = {1529-2401},
journal = {The Journal of neuroscience},
keywords = {auditory cortex,fear conditioning,gap detection,optogenetics},
number = {46},
pages = {15437--45},
pmid = {25392510},
title = {{Auditory cortex is required for fear potentiation of gap detection.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25392510},
volume = {34},
year = {2014}
}
@article{Giraud2012a,
author = {Giraud, Anne-Lise and Poeppel, David},
doi = {10.1038/nn.3063},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {mar},
number = {4},
pages = {511--517},
publisher = {Nature Research},
title = {{Cortical oscillations and speech processing: emerging computational principles and operations}},
url = {http://www.nature.com/doifinder/10.1038/nn.3063},
volume = {15},
year = {2012}
}
@article{Stevens1978a,
abstract = {In a series of experiments, identification responses for place of articulation were obtained for synthetic stop consonants in consonant–vowel syllables with different vowels. The acoustic attributes of the consonants were systematically manipulated, the selection of stimulus characteristics being guided in part by theoretical considerations concerning the expected properties of the sound generated in the vocal tract as place of articulation is varied. Several stimulus series were generated with and without noise bursts at the onset, and with and without formant transitions following consonantal release. Stimuli with transitions only, and with bursts plus transitions, were consistently classified according to place of articulation, whereas stimuli with bursts only and no transitions were not consistently identified. The acoustic attributes of the stimuli were examined to determine whether invariant properties characterized each place of atriculation independent of vowel context. It was determined that the ...},
author = {Stevens, K. N. and Blumstein, S. E.},
doi = {10.1121/1.382102},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {nov},
number = {5},
pages = {1358--1368},
publisher = {Acoustical Society of America},
title = {{Invariant cues for place of articulation in stop consonants}},
url = {http://asa.scitation.org/doi/10.1121/1.382102},
volume = {64},
year = {1978}
}
@inproceedings{Salakhutdinov2011a,
author = {Salakhutdinov, Ruslan and Torralba, Antonio and Tenenbaum, Josh},
booktitle = {CVPR 2011},
doi = {10.1109/CVPR.2011.5995720},
file = {::},
isbn = {978-1-4577-0394-2},
month = {jun},
pages = {1481--1488},
publisher = {IEEE},
title = {{Learning to share visual appearance for multiclass object detection}},
url = {http://ieeexplore.ieee.org/document/5995720/},
year = {2011}
}
@article{Bornkessel-Schlesewsky2015,
author = {Bornkessel-Schlesewsky, Ina and Schlesewsky, Matthias and Small, Steven L. and Rauschecker, Josef P.},
doi = {10.1016/j.tics.2014.12.008},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
month = {mar},
number = {3},
pages = {142--150},
title = {{Neurobiological roots of language in primate audition: common computational properties}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661314002757},
volume = {19},
year = {2015}
}
@unpublished{Gagnepain2012,
abstract = {Humans can recognize spoken words with unmatched speed and accuracy. Hearing the initial portion of a word such as “formu{\ldots}” is sufficient for the brain to identify “formula” from the thousands of other words that partially match [1–6]. Two alternative computational accounts propose that partially matching words (1) inhibit each other until a single word is selected (“formula” inhibits “formal” by lexical competition [7–9]) or (2) are used to predict upcoming speech sounds more accurately (segment prediction error is minimal after sequences like “formu{\ldots}” [10–12]). To distinguish these theories we taught participants novel words (e.g., “formubo”) that sound like existing words (“formula”) on two successive days [13–16]. Computational simulations show that knowing “formubo” increases lexical competition when hearing “formu{\ldots}”, but reduces segment prediction error. Conversely, when the sounds in “formula” and “formubo” diverge, the reverse is observed. The time course of magnetoencephalographic brain responses in the superior temporal gyrus (STG) is uniquely consistent with a segment prediction account. We propose a predictive coding model of spoken word recognition in which STG neurons represent the difference between predicted and heard speech sounds. This prediction error signal explains the efficiency of human word recognition and simulates neural responses in auditory regions.},
author = {Gagnepain, Pierre and Henson, Richard N. and Davis, Matthew H.},
booktitle = {Current Biology},
doi = {10.1016/j.cub.2012.02.015},
file = {::},
issn = {09609822},
number = {7},
pages = {615--621},
title = {{Temporal Predictive Codes for Spoken Words in Auditory Cortex}},
volume = {22},
year = {2012}
}
@article{Patterson1971,
author = {Patterson, H. D. and Thompson, R.},
doi = {10.2307/2334389},
issn = {00063444},
journal = {Biometrika},
month = {dec},
number = {3},
pages = {545},
title = {{Recovery of Inter-Block Information when Block Sizes are Unequal}},
url = {http://www.jstor.org/stable/2334389?origin=crossref},
volume = {58},
year = {1971}
}
@article{Ashby2005,
abstract = {Much recent evidence suggests some dramatic differences in the way people learn perceptual categories, depending on exactly how the categories were constructed. Four different kinds of category-learning tasks are currently popular—rule-based tasks, information-integration tasks, prototype distortion tasks, and the weather prediction task. The cognitive, neuropsychological, and neuroimaging results obtained using these four tasks are qualitatively different. Success in rule-based (explicit reasoning) tasks depends on frontal-striatal circuits and requires working memory and executive attention. Success in information-integration tasks requires a form of procedural learning and is sensitive to the nature and timing of feedback. Prototype distortion tasks induce perceptual (visual cortical) learning. A variety of different strategies can lead to success in the weather prediction task. Collectively, results from these four tasks provide strong evidence that human category learning is mediated by multiple, qua...},
author = {Ashby, F. Gregory and Maddox, W. Todd},
doi = {10.1146/annurev.psych.56.091103.070217},
file = {::},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {decision bound,exemplar,multiple systems,prototype,striatum},
month = {feb},
number = {1},
pages = {149--178},
publisher = { Annual Reviews },
title = {{Human Category Learning}},
url = {http://www.annualreviews.org/doi/10.1146/annurev.psych.56.091103.070217},
volume = {56},
year = {2005}
}
@article{Polley2006,
abstract = {The primary sensory cortex is positioned at a confluence of bottom-up dedicated sensory inputs and top-down inputs related to higher-order sensory features, attentional state, and behavioral reinforcement. We tested whether topographic map plasticity in the adult primary auditory cortex and a secondary auditory area, the suprarhinal auditory field, was controlled by the statistics of bottom-up sensory inputs or by top-down task-dependent influences. Rats were trained to attend to independent parameters, either frequency or intensity, within an identical set of auditory stimuli, allowing us to vary task demands while holding the bottom-up sensory inputs constant. We observed a clear double-dissociation in map plasticity in both cortical fields. Rats trained to attend to frequency cues exhibited an expanded representation of the target frequency range within the tonotopic map but no change in sound intensity encoding compared with controls. Rats trained to attend to intensity cues expressed an increased proportion of nonmonotonic intensity response profiles preferentially tuned to the target intensity range but no change in tonotopic map organization relative to controls. The degree of topographic map plasticity within the task-relevant stimulus dimension was correlated with the degree of perceptual learning for rats in both tasks. These data suggest that enduring receptive field plasticity in the adult auditory cortex may be shaped by task-specific top-down inputs that interact with bottom-up sensory inputs and reinforcement-based neuromodulator release. Top-down inputs might confer the selectivity necessary to modify a single feature representation without affecting other spatially organized feature representations embedded within the same neural circuitry.},
author = {Polley, D. B.},
doi = {10.1523/JNEUROSCI.3771-05.2006},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Polley, Steinberg, Merzenich - 2006 - Perceptual learning directs auditory cortical map reorganization through top-down influences(2).pdf:pdf},
isbn = {1529-2401 (Electronic)},
issn = {0270-6474},
journal = {Journal of Neuroscience},
keywords = {attention,conditioning,cortex,plasticity,reward,topographic map},
number = {18},
pages = {4970--4982},
pmid = {16672673},
title = {{Perceptual Learning Directs Auditory Cortical Map Reorganization through Top-Down Influences}},
url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3771-05.2006},
volume = {26},
year = {2006}
}
@article{Mottonen2006,
abstract = {The left superior temporal cortex shows greater responsiveness to speech than to non-speech sounds according to previous neuroimaging studies, suggesting that this brain region has a special role in speech processing. However, since speech sounds differ acoustically from the non-speech sounds, it is possible that this region is not involved in speech perception per se, but rather in processing of some complex acoustic features. "Sine wave speech" (SWS) provides a tool to study neural speech specificity using identical acoustic stimuli, which can be perceived either as speech or non-speech, depending on previous experience of the stimuli. We scanned 21 subjects using 3T functional MRI in two sessions, both including SWS and control stimuli. In the pre-training session, all subjects perceived the SWS stimuli as non-speech. In the post-training session, the identical stimuli were perceived as speech by 16 subjects. In these subjects, SWS stimuli elicited significantly stronger activity within the left posterior superior temporal sulcus (STSp) in the post- vs. pre-training session. In contrast, activity in this region was not enhanced after training in 5 subjects who did not perceive SWS stimuli as speech. Moreover, the control stimuli, which were always perceived as non-speech, elicited similar activity in this region in both sessions. Altogether, the present findings suggest that activation of the neural speech representations in the left STSp might be a pre-requisite for hearing sounds as speech. ?? 2005 Elsevier Inc. All rights reserved.},
author = {M{\"{o}}tt{\"{o}}nen, Riikka and Calvert, Gemma A. and J{\"{a}}{\"{a}}skel{\"{a}}inen, Iiro P. and Matthews, Paul M. and Thesen, Thomas and Tuomainen, Jyrki and Sams, Mikko},
doi = {10.1016/j.neuroimage.2005.10.002},
file = {::},
isbn = {1053-8119 (Print)$\backslash$n1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Sine wave speech,Speech,Superior temporal sulcus,fMRI},
month = {apr},
number = {2},
pages = {563--569},
pmid = {16275021},
title = {{Perceiving identical sounds as speech or non-speech modulates activity in the left posterior superior temporal sulcus}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811905007676},
volume = {30},
year = {2006}
}
@article{Engineer2008,
abstract = {Neural activity in the cerebral cortex can explain many aspects of sensory perception. Extensive psychophysical and neurophysiological studies of visual motion and vibrotactile processing show that the firing rate of cortical neurons averaged across 50-500 ms is well correlated with discrimination ability. In this study, we tested the hypothesis that primary auditory cortex (A1) neurons use temporal precision on the order of 1-10 ms to represent speech sounds shifted into the rat hearing range. Neural discrimination was highly correlated with behavioral performance on 11 consonant-discrimination tasks when spike timing was preserved and was not correlated when spike timing was eliminated. This result suggests that spike timing contributes to the auditory cortex representation of consonant sounds.},
author = {Engineer, Crystal T and Perez, Claudia A and Chen, YeTing H and Carraway, Ryan S and Reed, Amanda C and Shetake, Jai A and Jakkamsetti, Vikram and Chang, Kevin Q and Kilgard, Michael P},
doi = {10.1038/nn.2109},
file = {::},
issn = {1097-6256},
journal = {Nature neuroscience},
month = {may},
number = {5},
pages = {603--8},
pmid = {18425123},
publisher = {NIH Public Access},
title = {{Cortical activity patterns predict speech discrimination ability.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18425123 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2951886},
volume = {11},
year = {2008}
}
@article{Bates2015,
author = {Bates, Douglas and Machler, Martin and Bolker, Ben and Walker, Steve},
doi = {10.18637/jss.v067.i01},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--48},
title = {{Fitting Linear Mixed-Effects Models Using lme4}},
volume = {67},
year = {2015}
}
@misc{Team2016,
author = {Team, R Core},
booktitle = {R Foundation for Statistical Computing, Vienna, Austria.},
title = {{R: A language and environment for statistical computing.}},
year = {2016}
}
@article{Miyawaki1975,
abstract = {To test the effect of linguistic experience on the perception of a cue that is known to be effective in distinguishing between [1') and [I) in English, 21 Japanese and 39 American adults were tested on discrimination of a set of synthetic speech-like stimuli. The 13 "speech" stimuli in this set varied in the initial stationary frequency of the third formant (F3) and its subsequent transition into the vowel over a range sufficient to produce the perception of [1' a) and [I a) for American subjects and to produce [1' a) (which is not in phonemic contrast to [I a)) for Japanese subjects. Discrimination tests of a comparable set of stimuli consisting of the isolated F3 components provided a "nonspeech" control. For Americans, the discrimination of the speech stimuli was nearly categorical, i.e., comparison pairs which were identified as different phonemes were discriminated with high accuracy, while pairs which were identified as the same phoneme were discriminated relatively poorly. In comparison, discrimination of speech stimuli by Japanese subjects was only slightly better than chance for all comparison pairs. Performance on nonspeech stimuli, however, was virtually identical for Japanese and American subjects; both groups showed highly accurate discrimination of all comparison pairs. These results suggest that the effect of linguistic experience is specific to perception in the "speech mode." One way to examine the effect of linguistic experience on the perception of speech is to compare the discrimination of phonetic segments by two groups of speakers: one group speaks a language in},
author = {Miyawaki, Kuniko and Strange, Winifred and Verbrugge, Robert and Liberman, Alvin M and Jenkins, James J and Fujimura, Osamu},
doi = {10.3758/BF03211209},
isbn = {0031-5117},
issn = {0031-5117},
journal = {Perception {\&} Psychophysics},
month = {sep},
number = {5},
pages = {331--340},
publisher = {Springer-Verlag},
title = {{An effect of linguistic experience: The discrimination of [r] and [1] by native speakers of Japanese and English}},
url = {http://www.springerlink.com/index/10.3758/BF03211209},
volume = {18},
year = {1975}
}
@article{KewleyPort1983,
abstract = {Two recent accounts of the acoustic cues which specify place of articulation in syllable‐initial stop consonants claim that they are located in the initial portions of the CV waveform and are context‐free. Stevens and Blumstein [J. Acoust. Soc. Am. 64, 1358–1368 (1978)] have described the perceptually relevant spectral properties of these cues as static, while Kewley‐Port [J. Acoust. Soc. Am. 73, 322–335 (1983)] describes these cues as dynamic. Three perceptual experiments were conducted to test predictions derived from these accounts. Experiment 1 confirmed that acoustic cues for place of articulation are located in the initial 20–40 ms of natural stop‐vowel syllables. Next, short synthetic CV's modeled after natural syllables were generated using either a digital, parallel‐resonance synthesizer in experiment 2 or linear prediction synthesis in experiment 3. One set of synthetic stimuli preserved the static spectral properties proposed by Stevens and Blumstein. Another set of synthetic stimuli preserved ...},
author = {Kewley‐Port, Diane and Pisoni, David B. and Studdert‐Kennedy, Michael},
doi = {10.1121/1.389402},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {may},
number = {5},
pages = {1779--1793},
publisher = {Acoustical Society of America},
title = {{Perception of static and dynamic acoustic cues to place of articulation in initial stop consonants}},
url = {http://asa.scitation.org/doi/10.1121/1.389402},
volume = {73},
year = {1983}
}
@misc{Dahl2016,
author = {Dahl, David B.},
title = {{xtable: Export Tables to LaTeX or HTML}},
year = {2016}
}
@misc{Hlavac2015,
address = {Cambridge, MA},
author = {Hlavac, Marek},
title = {{stargazer: Well-Formatted Regression and Summary Statistics Tables}},
year = {2015}
}
@article{Chevillet2013,
author = {Chevillet, Mark A. and Jiang, Xiong and Rauschecker, Josef P. and Riesenhuber, Maximilian},
journal = {Journal of Neuroscience},
number = {12},
title = {{Automatic Phoneme Category Selectivity in the Dorsal Auditory Stream}},
url = {http://www.jneurosci.org/content/33/12/5208.short},
volume = {33},
year = {2013}
}
@article{Ashby1992,
author = {Ashby, F. Gregory and Maddox, W. Todd},
doi = {10.1037/0096-1523.18.1.50},
file = {::},
issn = {1939-1277},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
keywords = {experience level,naive vs experienced adults},
number = {1},
pages = {50--71},
publisher = {American Psychological Association},
title = {{Complex decision rules in categorization: Contrasting novice and experienced performance.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.18.1.50},
volume = {18},
year = {1992}
}
@book{Wickham2009,
address = {New York, NY},
author = {Wickham, Hadley},
isbn = {978-0-387-98140-6},
publisher = {Springer-Verlag},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
year = {2009}
}
@article{KewleyPort1983a,
abstract = {Running spectral displays derived from linear prediction analysis were used to examine the initial 40 ms of stop‐vowel CV syllables for possible acoustic correlates to place of articulation. Known spectral and temporal properties associated with the stop consonant release gesture were used to define a set of three‐time‐varying features observable in the visual displays. Judges identified place of articulation using these proposed features from running spectra of the syllables /b,d,g/ paired with eight vowels produced by three talkers. Average correct identification of place was 88{\%}; identification was better for the male talkers (92{\%}) than the one female talker (78{\%}). Post hoc analyses suggested, however, that simple rules could be incorporated in the feature definitions to account for differences in vocal tract size. The nature of the information contained in linear prediction running spectra was analyzed further to take account of known properties of the peripheral auditory system. The three proposed ti...},
author = {Kewley‐Port, Diane},
doi = {10.1121/1.388813},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {jan},
number = {1},
pages = {322--335},
publisher = {Acoustical Society of America},
title = {{Time‐varying features as correlates of place of articulation in stop consonants}},
url = {http://asa.scitation.org/doi/10.1121/1.388813},
volume = {73},
year = {1983}
}
@article{Lachlan2015a,
abstract = {Some of the psychological abilities that underlie human speech are shared with other species. One hallmark of speech is that linguistic context affects both how speech sounds are categorized into phonemes, and how different versions of phonemes are produced. We here confirm earlier findings that swamp sparrows categorically perceive the notes that constitute their learned songs and then investigate how categorical boundaries differ according to context. We clustered notes according to their acoustic structure, and found statistical evidence for clustering into 10 population-wide note types. Examining how three related types were perceived, we found, in both discrimination and labeling tests, that an "intermediate" note type is categorized with a "short" type when it occurs at the beginning of a song syllable, but with a "long" type at the end of a syllable. In sum, three produced note-type clusters appear to be underlain by two perceived categories. Thus, in birdsong, as in human speech, categorical perception is context-dependent, and as is the case for human phonology, there is a complex relationship between underlying categorical representations and surface forms. Our results therefore suggest that complex phonology can evolve even in the absence of rich linguistic components, like syntax and semantics.},
author = {Lachlan, Robert F and Nowicki, Stephen},
doi = {10.1073/pnas.1410844112},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Acoustic Stimulation,Analysis of Variance,Animal,Animal: physiology,Animals,Cluster Analysis,Discrimination (Psychology),Discrimination (Psychology): physiology,Flight,Linear Models,New York,Pennsylvania,Principal Component Analysis,Sound Spectrography,Sparrows,Sparrows: physiology,Speech Perception,Speech Perception: physiology,Vocalization,Wetlands,Wing,Wing: physiology},
number = {6},
pages = {1892--7},
pmid = {25561538},
title = {{Context-dependent categorical perception in a songbird.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25561538},
volume = {112},
year = {2015}
}
@article{Kluender1987,
abstract = {Japanese quail (Coturnix coturnix) learned a category for syllable-initial [d] followed by a dozen different vowels. After learning to categorize syllables consisting of [d], [b], or [g] followed by four different vowels, quail correctly categorized syllables in which the same consonants preceded eight novel vowels. Acoustic analysis of the categorized syllables revealed no single feature or pattern of features that could support generalization, suggesting that the quail adopted a more complex mapping of stimuli into categories. These results challenge theories of speech sound classification that posit uniquely human capacities.},
author = {Kluender, K R and Diehl, R L and Killeen, P R},
doi = {10.1126/science.3629235},
isbn = {0036-8075 (Print)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
keywords = {Animals,Coturnix,Coturnix: physiology,Female,Humans,Learning,Phonetics,Quail,Quail: physiology,Reinforcement (Psychology),Speech Perception},
language = {en},
month = {sep},
number = {4819},
pages = {1195--1197},
pmid = {3629235},
publisher = {American Association for the Advancement of Science},
title = {{Japanese quail can learn phonetic categories.}},
url = {http://science.sciencemag.org/content/237/4819/1195.abstract},
volume = {237},
year = {1987}
}
@article{Poeppel2008,
abstract = {Speech perception consists of a set of computations that take continuously varying acoustic waveforms as input and generate discrete representations that make contact with the lexical representations stored in long-term memory as output. Because the perceptual objects that are recognized by the speech perception enter into subsequent linguistic computation, the format that is used for lexical representation and processing fundamentally constrains the speech perceptual processes. Consequently, theories of speech perception must, at some level, be tightly linked to theories of lexical representation. Minimally, speech perception must yield representations that smoothly and rapidly interface with stored lexical items. Adopting the perspective of Marr, we argue and provide neurobiological and psychophysical evidence for the following research programme. First, at the implementational level, speech perception is a multi-time resolution process, with perceptual analyses occurring concurrently on at least two time scales (approx. 20-80 ms, approx. 150-300 ms), commensurate with (sub)segmental and syllabic analyses, respectively. Second, at the algorithmic level, we suggest that perception proceeds on the basis of internal forward models, or uses an 'analysis-by-synthesis' approach. Third, at the computational level (in the sense of Marr), the theory of lexical representation that we adopt is principally informed by phonological research and assumes that words are represented in the mental lexicon in terms of sequences of discrete segments composed of distinctive features. One important goal of the research programme is to develop linking hypotheses between putative neurobiological primitives (e.g. temporal primitives) and those primitives derived from linguistic inquiry, to arrive ultimately at a biologically sensible and theoretically satisfying model of representation and computation in speech.},
author = {Poeppel, D. and Idsardi, W. J and van Wassenhove, V.},
doi = {10.1098/rstb.2007.2160},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
month = {mar},
number = {1493},
pages = {1071--1086},
pmid = {17890189},
title = {{Speech perception at the interface of neurobiology and linguistics}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17890189 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2606797 http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2007.2160},
volume = {363},
year = {2008}
}
@article{Clerkin2016a,
author = {Clerkin, Elizabeth M. and Hart, Elizabeth and Rehg, James M. and Yu, Chen and Smith, Linda B.},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
number = {1711},
title = {{Real-world visual statistics and infants' first-learned object names}},
url = {http://rstb.royalsocietypublishing.org/content/372/1711/20160055.long},
volume = {372},
year = {2016}
}
@article{King2009,
author = {King, Andrew J and Nelken, Israel},
doi = {10.1038/nn.2308},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {jun},
number = {6},
pages = {698--701},
publisher = {Nature Publishing Group},
title = {{Unraveling the principles of auditory cortical processing: can we learn from the visual system?}},
url = {http://www.nature.com/doifinder/10.1038/nn.2308},
volume = {12},
year = {2009}
}
@article{Kleinschmidt2015,
author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
doi = {10.1037/a0038695},
file = {::},
issn = {1939-1471},
journal = {Psychological Review},
keywords = {adaptation,generalization,lack of invariance,speech perception,statistical learning},
number = {2},
pages = {148--203},
publisher = {American Psychological Association},
title = {{Robust speech perception: Recognize the familiar, generalize to the similar, and adapt to the novel.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0038695},
volume = {122},
year = {2015}
}
@article{Mines1978,
abstract = {The phoneme identification process of an automatic speech recognition system may be aided through the use of statistics of phoneme occurrence in conversational English. These statistics are also applicable to the fields of linguistics and speech, to teaching English as a foreign language and to speech pathology. In this study a data base containing 103,887 phoneme occurrences taken from casual conversational American English was obtained through interviews of sixteen adult males and ten adult females. The speech was transcribed using a quasi-phonemic system, known as ARPAbet, plus selected phoneme alternates and was analysed with computer assistance to obtain the rank order of phonemes according to frequency of occurrence. Also, the radius of the confidence interval for the observed frequency of occurrence was calculated at the 95{\%} level for each phoneme. The top ten phonemes (in order, / a, n, t, i, s, r, i, l, d, $\epsilon$ /) account for 47{\%} of all the data. As expected, the results of the present study correla...},
author = {Mines, M. Ardussi and Hanson, Barbara F. and Shoup, June E.},
doi = {10.1177/002383097802100302},
file = {::},
journal = {Language and Speech},
number = {3},
pages = {221=241},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{Frequency of Occurrence of Phonemes in Conversational English}},
url = {http://journals.sagepub.com/doi/abs/10.1177/002383097802100302},
volume = {21},
year = {1978}
}
@article{Suga1978,
author = {Suga, N and O'Neill, WE and Manabe, T},
journal = {Science},
number = {4343},
title = {{Cortical neurons sensitive to combinations of information-bearing elements of biosonar signals in the mustache bat}},
url = {http://science.sciencemag.org/content/200/4343/778},
volume = {200},
year = {1978}
}
@inproceedings{Petek1993,
author = {Petek, B. and Ferligoj, A.},
booktitle = {IEEE International Conference on Acoustics Speech and Signal Processing},
doi = {10.1109/ICASSP.1993.319287},
isbn = {0-7803-0946-4},
pages = {267--270 vol.2},
publisher = {IEEE},
title = {{Exploiting prediction error in a predictive-based connectionist speech recognition system}},
url = {http://ieeexplore.ieee.org/document/319287/},
year = {1993}
}
@incollection{Lindblom1988,
author = {Lindblom, Bjorn and Maddieson, Ian},
booktitle = {Language, speech, and mind},
isbn = {0-12-524940-3 (alk. paper) 0-12-524941-3 (paperback)},
pages = {62--78},
pmid = {308},
title = {{Phonetic Universals in Consonant Systems}},
url = {https://www.researchgate.net/profile/Ian{\_}Maddieson/publication/30352344{\_}Phonetic{\_}Universals{\_}in{\_}Consonant{\_}Systems/links/0912f5089cc36c5b0c000000.pdf},
year = {1988}
}
@article{Shannon1948,
abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, Claude E},
doi = {10.1145/584091.584093},
eprint = {9411012},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Shannon - 1948 - A mathematical theory of communication(3).pdf:pdf;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Shannon - 1948 - A mathematical theory of communication(4).pdf:pdf},
isbn = {0252725484},
issn = {15591662},
journal = {The Bell System Technical Journal},
number = {July 1928},
pages = {379--423},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{A mathematical theory of communication}},
url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html},
volume = {27},
year = {1948}
}
@article{Radziwon2009,
abstract = {Tone detection and temporal gap detection thresholds were determined in CBA/CaJ mice using a Go/No-go procedure and the psychophysical method of constant stimuli. In the first experiment, audiograms were constructed for five CBA/CaJ mice. Thresholds were obtained for eight pure tones ranging in frequency from 1 to 42 kHz. Audiograms showed peak sensitivity between 8 and 24 kHz, with higher thresholds at lower and higher frequencies. In the second experiment, thresholds for gap detection in broadband and narrowband noise bursts were measured at several sensation levels. For broadband noise, gap thresholds were between 1 and 2 ms, except at very low sensation levels, where thresholds increased significantly. Gap thresholds also increased significantly for low pass-filtered noise bursts with a cutoff frequency below 18 kHz. Our experiments revised absolute auditory thresholds in the CBA/CaJ mouse strain and demonstrated excellent gap detection ability in the mouse. These results add to the baseline behavioral data from normal-hearing mice which have become increasingly important for assessing auditory abilities in genetically altered mice.},
author = {Radziwon, Kelly E and June, Kristie M and Stolzberg, Daniel J and Xu-Friedman, Matthew A and Salvi, Richard J and Dent, Micheal L},
doi = {10.1007/s00359-009-0472-1},
file = {::},
issn = {1432-1351},
journal = {Journal of comparative physiology. A, Neuroethology, sensory, neural, and behavioral physiology},
month = {oct},
number = {10},
pages = {961--9},
pmid = {19756650},
publisher = {NIH Public Access},
title = {{Behaviorally measured audiograms and gap detection thresholds in CBA/CaJ mice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19756650 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2813807},
volume = {195},
year = {2009}
}
@incollection{Lindblom1986,
author = {Lindblom, Bjorn},
booktitle = {Experimental Phonology},
file = {::},
isbn = {0-12-524940-3 (alk. paper) 0-12-524941-3 (paperback)},
pages = {13--44},
pmid = {308},
title = {{Phonetic Universals in Vowel Systems}},
url = {https://pdfs.semanticscholar.org/68c4/3489716b60bd2fc3ea842ec653547caa76a6.pdf},
year = {1986}
}
@article{Margoliash1992,
abstract = {Song learning shapes the response properties of auditory neurons in the song system to become highly selective for the individual bird's own ("autogenous") song. The auditory representation of autogenous song is achieved in part by neurons that exhibit facilitated responses to combinations of components of song. To understand the circuits that underlie these complex properties, the combination sensitivity of single units in the hyperstriatum ventrale, pars caudale (HVc) of urethane-anesthetized zebra finches was studied. Some neurons exhibited nonlinear temporal summation, spectral summation, or both. The majority of these neurons exhibited low spontaneous rates and phasic responses. Most combination-sensitive neurons required highly accurate copies of sounds derived from the autogenous song and responded weakly to tone bursts, combinations of simple stimuli, or conspecific songs. Temporal combination-sensitive (TCS) neurons required either two or more segments of a single syllable, or two or more syllables of the autogenous song, to elicit a facilitated, excitatory response. TCS neurons integrated auditory input over periods ranging from 80 to 350 msec, although this represents a lower limit. Harmonic combination-sensitive (HCS) neurons required combinations of two harmonics with particular frequency and temporal characteristics that were similar to autogenous song syllables. Both TCS and HCS neurons responded much more weakly when the dynamical spectral features of the autogenous song or syllables were modified than when the dynamical amplitude (waveform) features of the songs were modified. These results suggest that understanding the temporal dynamics of auditory responses in HVc may provide insight into neuronal circuits modified by song learning.},
author = {Margoliash, D and Fortune, E S},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
month = {nov},
number = {11},
pages = {4309--26},
pmid = {1432096},
title = {{Temporal and harmonic combination-sensitive neurons in the zebra finch's HVc.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1432096},
volume = {12},
year = {1992}
}
@article{Hubel1962,
abstract = {Images$\backslash$nnull},
author = {Hubel, D. H. and Wiesel, T. N.},
doi = {10.1523/JNEUROSCI.1991-09.2009},
file = {::},
isbn = {0270-6474},
issn = {0022-3751},
journal = {The Journal of Physiology},
keywords = {CEREBRAL CORTEX/physiology},
month = {jan},
number = {1},
pages = {106--154.2},
pmid = {19776262},
publisher = {Wiley-Blackwell},
title = {{Receptive fields, binocular interaction and functional architecture in the cat's visual cortex}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14449617 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1359523},
volume = {160},
year = {1962}
}
@article{Vapnik1999,
author = {Vapnik, V.N.},
doi = {10.1109/72.788640},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
number = {5},
pages = {988--999},
title = {{An overview of statistical learning theory}},
url = {http://ieeexplore.ieee.org/document/788640/},
volume = {10},
year = {1999}
}
@article{Schultz2016,
abstract = {Reward prediction errors consist of the differences between received and predicted rewards. They are crucial for basic forms of learning about rewards and make us strive for more rewards-an evolutionary beneficial trait. Most dopamine neurons in the midbrain of humans, monkeys, and rodents signal a reward prediction error; they are activated by more reward than predicted (positive prediction error), remain at baseline activity for fully predicted rewards, and show depressed activity with less reward than predicted (negative prediction error). The dopamine signal increases nonlinearly with reward value and codes formal economic utility. Drugs of addiction generate, hijack, and amplify the dopamine reward signal and induce exaggerated, uncontrolled dopamine effects on neuronal plasticity. The striatum, amygdala, and frontal cortex also show reward prediction error coding, but only in subpopulations of neurons. Thus, the important concept of reward prediction errors is implemented in neuronal hardware.$\backslash$n$\backslash$nAbstract available from the publisher.$\backslash$n$\backslash$nAbstract available from the publisher.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Schultz, Wolfram},
doi = {10.1038/nrn.2015.26},
eprint = {9809069v1},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Schultz - 2016 - Dopamine reward prediction error coding(2).pdf:pdf},
isbn = {3-540-27590-8},
issn = {12948322},
journal = {Dialogues in Clinical Neuroscience},
keywords = {Dopamine,Neuron,Neurophysiology,Prediction,Reward,Striatum,Substantia nigra,Ventral tegmental area},
number = {1},
pages = {23--32},
pmid = {27069377},
primaryClass = {arXiv:gr-qc},
publisher = {Nature Publishing Group},
title = {{Dopamine reward prediction error coding}},
url = {http://dx.doi.org/10.1038/nrn.2015.26},
volume = {18},
year = {2016}
}
@article{Rauschecker1998b,
author = {Rauschecker, Josef P},
doi = {10.1016/S0959-4388(98)80040-8},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
month = {aug},
number = {4},
pages = {516--521},
title = {{Cortical processing of complex sounds}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0959438898800408},
volume = {8},
year = {1998}
}
@misc{Team2015,
author = {Team, RStudio},
publisher = {RStudio, Inc., Boston, MA},
title = {{RStudio: Integrated Development for R.}},
url = {http://www.rstudio.com/},
year = {2015}
}
@article{Leaver2010,
author = {Leaver, Amber M. and Rauschecker, Josef P.},
journal = {Journal of Neuroscience},
number = {22},
title = {{Cortical Representation of Natural Complex Sounds: Effects of Acoustic Features and Auditory Object Category}},
url = {http://www.jneurosci.org/content/30/22/7604.short},
volume = {30},
year = {2010}
}
@article{Kuhl1978,
abstract = {In an attempt to clearly differentiate perceptual effects that are attributable to ''auditory'' and ''phonetic'' levels of processing in speech perception we have undertaken a series of experiments with animal listeners. Four chinchillas (Chinchilla laniger) were trained to respond differently to the ''endpoints'' of a synthetic alveolar speech continuum (0 ms VOT and +80 ms VOT) and were then tested in a generalization paradigm with the VOT stimuli between these endpoints. The resulting identification functions were nearly identical to those obtained with adult English‐speaking listeners. To test the generality of this agreement, the animals were then tested with synthetic stimuli that had labial and velar places of articulation. As a whole, the functions produced by the two species were very similar; the same relative locations of the phonetic boundaries, with lowest VOT boundaries for labial stimuli and highest for velar stimuli, were obtained for each animal and human subject. No significant differences between species on the absolute values of the phonetic boundaries were obtained, but chinchillas produced identification functions that were slightly, but significantly, less steep. These results are discussed with regard to theories of speech perception, the evolution of a speech‐sound repertoire, and current interpretations of the human infant's perceptual proclivities with regard to speech‐sound perception.},
author = {Kuhl, Patricia K. and Miller, James D.},
doi = {10.1121/1.381770},
isbn = {0001-4966 (Print)$\backslash$r0001-4966 (Linking)},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {905--917},
pmid = {670558},
title = {{Speech perception by the chinchilla: Identification functions for synthetic VOT stimuli}},
url = {http://link.aip.org/link/?JAS/63/905/1{\%}5Cnhttp://asadl.org/jasa/resource/1/jasman/v63/i3/p905{\_}s1?isAuthorized=no},
volume = {63},
year = {1978}
}
@article{Ng2002a,
abstract = {Comparison of generative and discriminative classifiers is an ever-lasting topic. As an important contribution to this topic, based on their theoretical and empirical comparisons between the naive Bayes classifier and linear logistic regression, Ng and Jordan (NIPS 841-848, 2001) claimed that there exist two distinct regimes of performance between the generative and discriminative classifiers with regard to the training-set size. In this paper, our empirical and simulation studies, as a complement of their work, however, suggest that the existence of the two distinct regimes may not be so reliable. In addition, for real world datasets, so far there is no theoretically correct, general criterion for choosing between the discriminative and the generative approaches to classification of an observation x into a class y; the choice depends on the relative confidence we have in the correctness of the specification of either p(y vertical bar x) or p(x, y) for the data. This can be to some extent a demonstration of why Efron (J Am Stat Assoc 70(352):892-898, 1975) and O'Neill (J Am Stat Assoc 75(369):154-160, 1980) prefer normal-based linear discriminant analysis (LDA) when no model mis-specification occurs but other empirical studies may prefer linear logistic regression instead. Furthermore, we suggest that pairing of either LDA assuming a common diagonal covariance matrix (LDA-A) or the naive Bayes classifier and linear logistic regression may not be perfect, and hence it may not be reliable for any claim that was derived from the comparison between LDA-A or the naive Bayes classifier and linear logistic regression to be generalised to all generative and discriminative classifiers.},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1007/s11063-008-9088-7},
author = {Ng, Andrew and Jordan, Michael I.},
doi = {10.1007/s11063-008-9088-7},
eprint = {/dx.doi.org/10.1007/s11063-008-9088-7},
isbn = {1106300890},
issn = {13704621},
journal = {Proceedings of Advances in Neural Information Processing},
keywords = {asymptotic relative efficiency,discriminative classifiers,generative classifiers,logistic regression,naive bayes classifier,normal based discriminant analysis},
number = {3},
pages = {169--187},
pmid = {25246403},
primaryClass = {http:},
title = {{On generative vs. discriminative classifiers: A comparison of logistic regression and naive bayes}},
volume = {28},
year = {2002}
}
@article{Wickham2011,
author = {Wickham, Hadley},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--29},
title = {{The Split-Apply-Combine Strategy for Data Analysis}},
url = {http://www.jstatsoft.org/v40/i01/},
volume = {40},
year = {2011}
}
@book{Lieberman1984,
abstract = {Introduction : The biological framework -- Neurophysiology, neural models, and language -- Distributed neural computers and feature detectors -- Automatization and syntax -- Syntax, words, and meaning -- Respiration, speech, and meaning -- Elephant ears, frogs, and human speech -- Speech is special -- Linguistic distinctions and auditory processes -- Man on the flying trapeze : the acquisition of speech -- Apes and children -- Evolution of human speech : comparative studies -- Evolution of human speech : the fossil record -- Conclusion : On the nature and evolution of the biological bases of language.},
author = {Lieberman, Philip.},
isbn = {0674074130},
pages = {379},
publisher = {Harvard University Press},
title = {{The biology and evolution of language}},
year = {1984}
}
@article{Liberman1985a,
abstract = {A motor theory of speech perception, initially proposed to account for results of early experiments with synthetic speech, is now extensively revised to accommodate recent findings, and to relate the assumptions of the theory to those that might be made about other perceptual modes. According to the revised theory, phonetic information is perceived in a biologically distinct system, a ‘module' specialized to detect the intended gestures of the speaker that are the basis for phonetic categories. Built into the structure of this module is the unique but lawful relationship between the gestures and the acoustic patterns in which they are variously overlapped. In consequence, the module causes perception of phonetic structure without translation from preliminary auditory impressions. Thus, it is comparable to such other modules as the one that enables an animal to localize sound. Peculiar to the phonetic module are the relation between perception and production it incorporates and the fact that it must compete with other modules for the same stimulus variations. Une th{\'{e}}orie motrice de la perception propos{\'{e}}e initialement pour rendre compte des r{\'{e}}sultats des premi{\`{e}}res exp{\'{e}}riences avec de la parole synth{\'{e}}tique a {\'{e}}t{\'{e}} largement r{\'{e}}vis{\'{e}}e afin d'interpr{\'{e}}ter les donn{\'{e}}es r{\'{e}}centes et de relier les propositions de cette th{\'{e}}orie {\`{a}} celles que l'on peut faire pour d'autres modalit{\'{e}}s de perception. La r{\'{e}}vision de cette th{\'{e}}orie stipule que l'information phon{\'{e}}tique est fournie par un syst{\`{e}}me biologique distinct, un ‘module' sp{\'{e}}cialis{\'{e}} pour d{\'{e}}tecter les gestes que le locuteur a eu l'intention de faire: ces gestes fondent les cat{\'{e}}gories phon{\'{e}}tiques. La relation entre les gestes et les patterns acoustiques dans lesquels ceux-ci sont imbriqu{\'{e}}s de facon vari{\'{e}}e est unique mais r{\'{e}}gul{\'{e}}e. Cette relation est construite dans la structure du module. En cons{\'{e}}quence le module provoque la perception de la structure phon{\'{e}}tique sans traduction {\`{a}} partir d'impressions auditives pr{\'{e}}liminaires. Ce module est ainsi comparable {\`{a}} d'autres modules tels que celui qui permet {\`{a}} l'animal de localiser les sons. La particularit{\'{e}} de ce module tient {\`{a}} la relation entre perception et production qu'il incorpore et an fait qu'il doit rivaliser avec d'autres modules pour de m{\^{e}}mes variations de stimulus.},
author = {Liberman, Alvin M. and Mattingly, Ignatius G.},
doi = {10.1016/0010-0277(85)90021-6},
file = {::},
issn = {00100277},
journal = {Cognition},
number = {1},
pages = {1--36},
title = {{The motor theory of speech perception revised}},
volume = {21},
year = {1985}
}
@article{Tremblay2016,
abstract = {With the advancement of cognitive neuroscience and neuropsychological research, the field of language neurobiology is at a cross-roads with respect to its framing theories. The central thesis of this article is that the major historical framing model, the Classic “Wernicke-Lichtheim-Geschwind” model, and associated terminology, is no longer adequate for contemporary investigations into the neurobiology of language. We argue that the Classic model (1) is based on an outdated brain anatomy; (2) does not adequately represent the distributed connectivity relevant for language, (3) offers a modular and “language centric” perspective, and (4) focuses on cortical structures, for the most part leaving out subcortical regions and relevant connections. To make our case, we discuss the issue of anatomical specificity with a focus on the contemporary usage of the terms “Broca's and Wernicke's area”, including results of a survey that was conducted within the language neurobiology community. We demonstrate that there is no consistent anatomical definition of “Broca's and Wernicke's Areas”, and propose to replace these terms with more precise anatomical definitions. We illustrate the distributed nature of the language connectome, which extends far beyond the single-pathway notion of arcuate fasciculus connectivity established in Geschwind's version of the Classic Model. By illustrating the definitional confusion surrounding “Broca's and Wernicke's areas”, and by illustrating the difficulty integrating the emerging literature on perisylvian white matter connectivity into this model, we hope to expose the limits of the model, argue for its obsolescence, and suggest a path forward in defining a replacement.},
author = {Tremblay, Pascale and Dick, Anthony Steven},
doi = {10.1016/j.bandl.2016.08.004},
issn = {0093934X},
journal = {Brain and Language},
pages = {60--71},
title = {{Broca and Wernicke are dead, or moving past the classic model of language neurobiology}},
url = {http://www.sciencedirect.com/science/article/pii/S0093934X16300475},
volume = {162},
year = {2016}
}
@article{Elman1988,
abstract = {In the work described here, the backpropagation neural network learning procedure is applied to the analysis and recognition of speech. This procedure takes a set of input/output pattern pairs and attempts to learn their functional relationship; it develops the necessary representational features during the course of learning. A series of computer simulation studies was carried out to assess the ability of these networks to accurately label sounds, to learn to recognize sounds without labels, and to learn feature representations of continuous speech. These studies demonstrated that the networks can learn to label presegmented test tokens with accuracies of up to 95{\%}. Networks trained on segmented sounds using a strategy that requires no external labels were able to recognize and delineate sounds in continuous speech. These networks developed rich internal representations that included units which corresponded to such traditional distinctions as vowels and consonants, as well as units that were sensitive to novel and nonstandard features. Networks trained on a large corpus of unsegmented, continuous speech without labels also developed interesting feature representations, which may be useful in both segmentation and label learning. The results of these studies, while preliminary, demonstrate that backpropagation learning can be used with complex, natural data to identify a feature structure that can serve as the basis for both analysis and nontrivial pattern recognition.},
author = {Elman, J L and Zipser, D},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {apr},
number = {4},
pages = {1615--26},
pmid = {3372872},
title = {{Learning the hidden structure of speech.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3372872},
volume = {83},
year = {1988}
}
@article{Fox2016,
abstract = {Although much evidence suggests that the identification of phonetically ambiguous target words can be biased by preceding sentential context, interactive and autonomous models of speech perception disagree as to the mechanism by which higher level information affects subjects' responses. Some have suggested that the time course of context effects is incompatible with interactive models (e.g., TRACE). Two experiments examine this issue. In Experiment 1, subjects heard noun- and verb-biasing sentence contexts (e.g., Valerie hated the . . . vs. Brett hated to . . .), followed by stimuli from 2 voice-onset time continua: bay-pay (noun-verb) versus buy-pie (verb-noun). Consistent with prior research, identification of phonetically ambiguous targets was biased by the preceding context, and the size of this bias diminished in slower compared with faster responses. In Experiment 2, tokens from the same continua were embedded among filler target words beginning with /b/ or /p/ to elicit phonemically driven identification decisions and discourage word-level strategies. Results again revealed contextually biased responding, but this bias was as strong in slow as in fast responses. Together, these results suggest that phoneme identification decisions reflect robust, lasting top-down effects of lexical feedback on prelexical representations, as predicted by interactive models of speech perception.},
author = {Fox, Neal P. and Blumstein, Sheila E.},
doi = {10.1037/a0039965},
issn = {1939-1277},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
month = {may},
number = {5},
pages = {730--741},
pmid = {26689310},
title = {{Top-down effects of syntactic sentential context on phonetic processing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26689310 http://doi.apa.org/getdoi.cfm?doi=10.1037/a0039965},
volume = {42},
year = {2016}
}
@article{Kluender1994,
abstract = {When F1-onset frequency is lower, longer F1 cut-back (VOT) is required for human listeners to perceive synthesized stop consonants as voiceless. K. R. Kluender [J. Acoust. Soc. Am. 90, 83-96 (1991)] found comparable effects of F1-onset frequency on the "labeling" of stop consonants by Japanese quail (coturnix coturnix japonica) trained to distinguish stop consonants varying in F1 cut-back. In that study, CVs were synthesized with natural-like rising F1 transitions, and endpoint training stimuli differed in the onset frequency of F1 because a longer cut-back resulted in a higher F1 onset. In order to assess whether earlier results were due to auditory predispositions or due to animals having learned the natural covariance between F1 cut-back and F1-onset frequency, the present experiment was conducted with synthetic continua having either a relatively low (375 Hz) or high (750 Hz) constant-frequency F1. Six birds were trained to respond differentially to endpoint stimuli from three series of synthesized /CV/s varying in duration of F1 cut-back. Second and third formant transitions were appropriate for labial, alveolar, or velar stops. Despite the fact that there was no opportunity for animal subjects to use experienced covariation of F1-onset frequency and F1 cut-back, quail typically exhibited shorter labeling boundaries (more voiceless stops) for intermediate stimuli of the continua when F1 frequency was higher. Responses by human subjects listening to the same stimuli were also collected. Results lend support to the earlier conclusion that part or all of the effect of F1 onset frequency on perception of voicing may be adequately explained by general auditory processes.(ABSTRACT TRUNCATED AT 250 WORDS)},
author = {Kluender, K R and Lotto, a J},
doi = {10.1121/1.408466},
isbn = {0001-4966 (Print)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {Animals,Coturnix,Female,Humans,Male,Phonetics,Speech Acoustics,Speech Perception},
number = {2},
pages = {1044--52},
pmid = {8132898},
title = {{Effects of first formant onset frequency on [-voice] judgments result from auditory processes not specific to humans.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8132898},
volume = {95},
year = {1994}
}
@article{Pasley2012,
abstract = {How the human auditory system extracts perceptually relevant acoustic features of speech is unknown. To address this question, we used intracranial recordings from nonprimary auditory cortex in the human superior temporal gyrus to determine what acoustic information in speech sounds can be reconstructed from population neural activity. We found that slow and intermediate temporal fluctuations, such as those corresponding to syllable rate, were accurately reconstructed using a linear model based on the auditory spectrogram. However, reconstruction of fast temporal fluctuations, such as syllable onsets and offsets, required a nonlinear sound representation based on temporal modulation energy. Reconstruction accuracy was highest within the range of spectro-temporal fluctuations that have been found to be critical for speech intelligibility. The decoded speech representations allowed readout and identification of individual words directly from brain activity during single trial sound presentations. These findings reveal neural encoding mechanisms of speech acoustic parameters in higher order human auditory cortex.},
author = {Pasley, Brian N. and David, Stephen V. and Mesgarani, Nima and Flinker, Adeen and Shamma, Shihab A. and Crone, Nathan E. and Knight, Robert T. and Chang, Edward F.},
doi = {10.1371/journal.pbio.1001251},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Pasley et al. - 2012 - Reconstructing speech from human auditory cortex(2).pdf:pdf},
isbn = {1545-7885},
issn = {15449173},
journal = {PLoS Biology},
number = {1},
pmid = {22303281},
title = {{Reconstructing speech from human auditory cortex}},
volume = {10},
year = {2012}
}
@article{Schindelin2012,
abstract = {Fiji is a distribution of the popular open-source software ImageJ focused on biological-image analysis. Fiji uses modern software engineering practices to combine powerful software libraries with a broad range of scripting languages to enable rapid prototyping of image-processing algorithms. Fiji facilitates the transformation of new algorithms into ImageJ plugins that can be shared with end users through an integrated update system. We propose Fiji as a platform for productive collaboration between computer science and biology research communities.},
archivePrefix = {arXiv},
arxivId = {1081-8693},
author = {Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert},
doi = {10.1038/nmeth.2019},
eprint = {1081-8693},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
keywords = {*Software,Algorithms,Animals,Brain/ultrastructure,Computational Biology/*methods,Drosophila melanogaster/ultrastructure,Image Enhancement/methods,Image Processing, Computer-Assisted/*methods,Imaging, Three-Dimensional/methods,Information Dissemination,Software Design},
number = {7},
pages = {676--682},
pmid = {22743772},
title = {{Fiji: an open-source platform for biological-image analysis}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2019},
volume = {9},
year = {2012}
}
@misc{Schindelin2015,
abstract = {Technology in microscopy advances rapidly, enabling increasingly affordable, faster, and more precise quantitative biomedical imaging, which necessitates correspondingly more-advanced image processing and analysis techniques. A wide range of software is available—from commercial to academic, special-purpose to Swiss army knife, small to large—but a key characteristic of software that is suitable for scientific inquiry is its accessibility. Open-source software is ideal for scientific endeavors because it can be freely inspected, modified, and redistributed; in particular, the open-software platform ImageJ has had a huge impact on the life sciences, and continues to do so. From its inception, ImageJ has grown significantly due largely to being freely available and its vibrant and helpful user community. Scientists as diverse as interested hobbyists, technical assistants, students, scientific staff, and advanced biology researchers use ImageJ on a daily basis, and exchange knowledge via its dedicated mailing list. Uses of ImageJ range from data visualization and teaching to advanced image processing and statistical analysis. The software's extensibility continues to attract biologists at all career stages as well as computer scientists who wish to effectively implement specific image-processing algorithms. In this review, we use the ImageJ project as a case study of how open-source software fosters its suites of software tools, making multitudes of image-analysis technology easily accessible to the scientific community. We specifically explore what makes ImageJ so popular, how it impacts the life sciences, how it inspires other projects, and how it is self-influenced by coevolving projects within the ImageJ ecosystem. Mol. Reprod. Dev. 82: 518–529, 2015. {\textcopyright} 2015 Wiley Periodicals, Inc.},
author = {Schindelin, Johannes and Rueden, Curtis T. and Hiner, Mark C. and Eliceiri, Kevin W.},
booktitle = {Molecular Reproduction and Development},
doi = {10.1002/mrd.22489},
isbn = {1098-2795},
issn = {10982795},
number = {7-8},
pages = {518--529},
pmid = {26153368},
title = {{The ImageJ ecosystem: An open platform for biomedical image analysis}},
volume = {82},
year = {2015}
}
@article{Lowe2004,
author = {Lowe, D.},
journal = {International Journal of Computer Vision},
number = {2},
pages = {91--110},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
volume = {60},
year = {2004}
}
@article{Stevens1978,
abstract = {In a series of experiments, identification responses for place of articulation were obtained for synthetic stop consonants in consonant-vowel syllables with different vowels. The acoustic attributes of the consonants were systematically manipulated, the selection of stimulus characteristics being guided in part by theoretical considerations concerning the expected properties of the sound generated in the vocal tract as place of articulation is varied. Several stimulus series were generated with and without noise bursts at the onset, and with and without formant transitions following consonantal release. Stimuli with transitions only, and with bursts plus transitions, were consistently classified according to place of articulation, whereas stimuli with bursts only and no transitions were not consistently identified. The acoustic attributes of the stimuli were examined to determine whether invariant properties characterized each place of atriculation independent of vowel context. It was determined that the gross shape of the spectrum sampled at the consonantal release showed a distinctive shape for each place of articulation: a prominent midfrequency spectral peak for velars, a diffuse-rising spectrum for alveolars, and a diffuse-falling spectrum for labials. These attributes are evident for stimuli containing transitions only, but are enhanced by the presence of noise bursts at the onset.},
author = {Stevens, K N and Blumstein, S E},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {nov},
number = {5},
pages = {1358--68},
pmid = {744836},
title = {{Invariant cues for place of articulation in stop consonants.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/744836},
volume = {64},
year = {1978}
}
@article{Hickok2012a,
abstract = {Speech recognition is an active process that involves some form of predictive coding. This statement is relatively uncontroversial. What is less clear is the source of the prediction. The dual-stream model of speech processing suggests that there are two possible sources of predictive coding in speech perception: the motor speech system and the lexical-conceptual system. Here I provide an overview of the dual-stream model of speech processing and then discuss evidence concerning the source of predictive coding during speech recognition. I conclude that, in contrast to recent theoretical trends, the dorsal sensory-motor stream is not a source of forward prediction that can facilitate speech recognition. Rather, it is forward prediction coming out of the ventral stream that serves this function.Learning outcomes: Readers will (1) be able to explain the dual route model of speech processing including the function of the dorsal and ventral streams in language processing, (2) describe how disruptions to certain components of the dorsal stream can cause conduction aphasia, (3) be able to explain the fundamental principles of state feedback control in motor behavior, and (4) identify the role of predictive coding in motor control and in perception and how predictive coding coming out of the two streams may have different functional consequences. {\textcopyright} 2012 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hickok, Gregory},
doi = {10.1016/j.jcomdis.2012.06.004},
eprint = {NIHMS150003},
isbn = {1873-7994 (Electronic)$\backslash$n0021-9924 (Linking)},
issn = {00219924},
journal = {Journal of Communication Disorders},
keywords = {Aphasia,Language,Motor control,Speech perception,Speech production},
number = {6},
pages = {393--402},
pmid = {22766458},
title = {{The cortical organization of speech processing: Feedback control and predictive coding the context of a dual-stream model}},
volume = {45},
year = {2012}
}
@article{Kewley-Port1983,
abstract = {Running spectral displays derived from linear prediction analysis were used to examine the initial 40 ms of stop-vowel CV syllables for possible acoustic correlates to place of articulation. Known spectral and temporal properties associated with the stop consonant release gesture were used to define a set of three-time-varying features observable in the visual displays. Judges identified place of articulation using these proposed features from running spectra of the syllables /b,d,g/paired with eight vowels produced by three talkers. Average correct identification of place was 88{\%}; identification was better for the male talkers (92{\%}) than the one female talker (78{\%}). Post hoc analyses suggested, however, that simple rules could be incorporated in the feature definitions to account for differences in vocal tract size. The nature of the information contained in linear prediction running spectra was analyzed further to take account of known properties of the peripheral auditory system. The three proposed time-varying features were shown to be displayed robustly in auditory filtered running spectra. The advantages of describing acoustic correlates for place from the dynamically varying temporal and spectral information in running spectra is discussed with regard to the static template matching approach advocated recently by Blumstein and Stevens [J. Acoust. Soc. Am. 66, 1001-1017 (1979)].},
author = {Kewley-Port, D},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {jan},
number = {1},
pages = {322--35},
pmid = {6826902},
title = {{Time-varying features as correlates of place of articulation in stop consonants.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6826902},
volume = {73},
year = {1983}
}
@article{Kewley-Port1982a,
abstract = {Formant transitions have been considered important context-dependent acoustic cues to place of articulation in stop-vowel syllables. However, the bulk of earlier research supporting their perceptual importance has been conducted primarily with synthetic speech stimuli. The present study examined the acoustic correlates of place of articulation in the voiced formant transitions from natural speech. Linear prediction analysis was used to provide detailed temporal and spectral measurements of the formant transitions for /b,d,g/ paired with eight vowels produced by one talker. Measurements of the transition onset and steady state frequencies, durations, and derived formant loci for F1, F2, and F3 are reported. Analysis of these measures showed little evidence of context invariant acoustic correlates of place. When vowel context was known, most transition parameters were not reliable acoustic correlates of place except for the F2 transition and a two-dimensional representation of F2 X F3 onset frequencies. The results indicated that the information contained in the formant transitions in these natural stop-vowel syllables was not sufficient to distinguish place across all the vowel contexts studied.},
author = {Kewley-Port, D},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {aug},
number = {2},
pages = {379--89},
pmid = {7119280},
title = {{Measurement of formant transitions in naturally produced stop consonant-vowel syllables.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7119280},
volume = {72},
year = {1982}
}
@article{Rosenblum2008,
abstract = {Speech perception is inherently multimodal. Visual speech (lip-reading) information is used by all perceivers and readily integrates with auditory speech. Imaging research suggests that the brain treats auditory and visual speech similarly. These findings have led some researchers to consider that speech perception works by extracting amodal information that takes the same form across modalities. From this perspective, speech integration is a property of the input information itself. Amodal speech information could explain the reported automaticity, immediacy, and completeness of audiovisual speech integration. However, recent findings suggest that speech integration can be influenced by higher cognitive properties such as lexical status and semantic context. Proponents of amodal accounts will need to explain these results.},
author = {Rosenblum, Lawrence D.},
doi = {10.1111/j.1467-8721.2008.00615.x},
file = {::},
issn = {0963-7214},
journal = {Current Directions in Psychological Science},
keywords = {audiovisual,lip reading,multimodal,speech},
month = {dec},
number = {6},
pages = {405--409},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Speech Perception as a Multimodal Phenomenon}},
url = {http://journals.sagepub.com/doi/10.1111/j.1467-8721.2008.00615.x},
volume = {17},
year = {2008}
}
@article{Strauss2007,
author = {Strauss, Ted J. and Harris, Harlan D. and Magnuson, James S.},
doi = {10.3758/BF03192840},
file = {::},
issn = {1554-351X},
journal = {Behavior Research Methods},
month = {feb},
number = {1},
pages = {19--30},
publisher = {Springer-Verlag},
title = {{jTRACE: A reimplementation and extension of the TRACE model of speech perception and spoken word recognition}},
url = {http://www.springerlink.com/index/10.3758/BF03192840},
volume = {39},
year = {2007}
}
@article{Ohl,
abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
issn = {1072-0502},
journal = {Learning {\&} memory (Cold Spring Harbor, N.Y.)},
number = {4},
pages = {347--62},
pmid = {10509706},
title = {{Bilateral ablation of auditory cortex in Mongolian gerbil affects discrimination of frequency modulated tones but not of pure tones.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC311295},
volume = {6}
}
@article{Kuhl1983,
abstract = {Discrimination of speech-sound pairs drawn from a computer-generated continuum in which syllables varied along the place of articulation phonetic feature (/b,d,g/) was tested with macaques. The acoustic feature that was varied along the two-formant 15-step continuum was the starting frequency of the second-formant transition. Discrimination of stimulus pairs separated by two steps was tested along the entire continuum in a same-different task. Results demonstrated that peaks in the discrimination functions occur for macaques at the "phonetic boundaries" which separate the /b-d/ and /d-g/ categories for human listeners. The data support two conclusions. First, although current theoretical accounts of place perception by human adults suggest that isolated second-formant transitions are "secondary" cues, learned by association with primary cues, the animal data are more compatible with the notion that second-formant transitions are sufficient to allow the appropriate partitioning of a place continuum in the absence of associative pairing with other more complex cues. Second, we discuss two potential roles played by audition in the evolution of the acoustics of language. One is that audition provided a set of "natural psychophysical boundaries," based on rather simple acoustic properties, which guided the selection of the phonetic repertoire but did not solely determine it; the other is that audition provided a set of rules for the formation of "natural classes" of sound and that phonetic units met those criteria. The data provided in this experiment provide support for the former. Experiments that could more clearly differentiate the two hypotheses are described.},
author = {Kuhl, P K and Padden, D M},
doi = {10.3758/BF03204208},
isbn = {0001-4966 (Print)$\backslash$r0001-4966 (Linking)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {1003--1010},
pmid = {6221040},
title = {{Enhanced discriminability at the phonetic boundaries for the place feature in macaques.}},
volume = {73},
year = {1983}
}
@article{Diehl2004,
abstract = {This chapter focuses on one of the first steps in comprehending spoken language: How do listeners extract the most fundamental linguistic elements-consonants and vowels, or the distinctive features which compose them-from the acoustic signal? We begin by describing three major theoretical perspectives on the perception of speech. Then we review several lines of research that are relevant to distinguishing these perspectives. The research topics surveyed include categorical perception, phonetic context effects, learning of speech and related nonspeech categories, and the relation between speech perception and production. Finally, we describe challenges facing each of the major theoretical perspectives on speech perception.},
author = {Diehl, Randy L. and Lotto, Andrew J. and Holt, Lori L.},
doi = {10.1146/annurev.psych.55.090902.142028},
issn = {0066-4308},
journal = {Annual Review of Psychology},
month = {feb},
number = {1},
pages = {149--179},
pmid = {14744213},
title = {{Speech Perception}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14744213 http://www.annualreviews.org/doi/10.1146/annurev.psych.55.090902.142028},
volume = {55},
year = {2004}
}
@book{Perkell1986,
abstract = {Proceedings, in honor of Kenneth Stevens, of the Symposium on Invariance and Variability of Speech Processes, held Oct. 8-10, 1983 at M.I.T.},
author = {Perkell, Joseph S. and Klatt, Dennis H. and Stevens, Kenneth N. and {Symposium on Invariance and Variability of Speech Processes (1983 : Massachusetts Institute of Technology)}},
isbn = {0898595452},
pages = {604},
publisher = {Lawrence Erlbaum Associates},
title = {{Invariance and variability in speech processes}},
year = {1986}
}
@article{Dresher2008,
abstract = {I will show that phonologists have vacillated between two different and incompatible approaches to determining whether a feature is contrastive in any particular phoneme. One approach involves extracting contrastive features from fully-specified minimal pairs. I will show that this approach is provably untenable. A second approach arrives at contrastive specifications by ordering features into a hierarchy, and splitting up the inventory by successive divisions until all phonemes have been distinguished. I will show that this hierarchical approach solves the problems encountered by the minimal-pairs method. Moreover, a hierarchical approach to contrastiveness is implicit in much descriptive phonological practice, and can be found even in the work of theorists who argue against it. Given the centrality of the issue, it is remarkable that it has received almost no attention in the literature. Recovering this missing chapter of phonological theory sheds new light on a number of controversies over contrast in phonology.},
author = {Dresher, B Elan},
doi = {10.1017/CBO9780511642005},
isbn = {9780521889735},
issn = {1718-3510},
journal = {Contrast in phonology: theory, perception, acquisition},
pages = {11},
title = {{The contrastive hierarchy in phonology}},
volume = {13},
year = {2008}
}
@article{Liberman1967,
author = {Liberman, A M and Cooper, F S and Shankweiler, D P and Studdert-Kennedy, M},
issn = {0033-295X},
journal = {Psychological review},
month = {nov},
number = {6},
pages = {431--61},
pmid = {4170865},
title = {{Perception of the speech code.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/4170865},
volume = {74},
year = {1967}
}
@article{Ohla,
abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
issn = {1072-0502},
journal = {Learning {\&} memory (Cold Spring Harbor, N.Y.)},
number = {4},
pages = {347--62},
pmid = {10509706},
title = {{Bilateral ablation of auditory cortex in Mongolian gerbil affects discrimination of frequency modulated tones but not of pure tones.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC311295},
volume = {6}
}
@book{Saussure1916,
address = {Lausanne, Paris},
author = {de Saussure, Ferdinand},
editor = {Bally, C and Sechehaye, A and Reidlinger, A},
publisher = {Payot},
title = {{Cours de linguistique générale.}},
year = {1916}
}
@book{Feynman1998,
abstract = {From 1983 to 1986, the legendary physicist and teacher Richard Feynman gave a course at Caltech called "Potentialities and Limitations of Computing Machines." Although the lectures are over ten years old, most of the material is timeless and presents a "Feynmanesque" overview of many standard and some not-so-standard topics in computer science. These include compatibility, Turing machines (or as Feynman said, "Mr. Turing's machines"), information theory, Shannon's Theorem, reversible computation, the thermodynamics of computation, the quantum limits to computation, and the physics of VLSI devices. Taken together, these lectures represent a unique exploration of the fundamental limitations of digital computers. Feynman's philosophy of learning and discovery comes through strongly in these lectures. He constantly points out the benefits of playing around with concepts and working out solutions to problems on your own - before looking at the back of the book for the answers. As Feynman says in the lectures: "If you keep proving stuff that others have done, getting confidence, increasing the complexities of your solutions - for the fun of it - then one day you'll turn around and discover that nobody actually did that one! And that's the way to become a computer scientist."},
address = {Boston, MA},
author = {Feynman, Richard P.},
editor = {Hey, J.G. and Allen, Robin W.},
isbn = {0201386283},
publisher = {Addison-Wesley Longman Publishing Co.},
title = {{Feynman Lectures on Computation}},
year = {1998}
}
@article{Buchman1986,
abstract = {Since its original description the diagnosis of word deafness has been greatly expanded. Confusion has arisen with regard to the usage of the related terms pure word deafness, auditory agnosia, and cortical deafness. Three new cases of word deafness are presented including one case with CT and necropsy correlation. These cases are compared with 34 previously reported cases of various cortical auditory disorders. Our review establishes that patients with word deafness who have had formal testing of linguistic and non-linguistic sound comprehension and musical abilities always demonstrated a more pervasive auditory agnosia. Despite the spectrum of auditory deficits and associated language abnormalities, patients with word deafness share common features including aetiology, pathology, clinical presentation and course. These common features justify inclusion of heterogeneous cortical auditory disorders under the rubric of word deafness. Despite some limitations the term "word deafness" should be retained for this syndrome, since inability to comprehend spoken words is the most distinctive clinical deficit. Word deafness is most frequently caused by cerebrovascular accidents of presumed cardiac embolisation, with bitemporal cortico-subcortical lesions. The sequence of cerebral injury is not predictive of resulting auditory deficits. Impairment of musical abilities parallels the severity of the auditory disorder.},
author = {Buchman, A S and Garron, D C and Trost-Cardamone, J E and Wichter, M D and Schwartz, M},
doi = {10.1136/jnnp.49.5.489},
isbn = {0022-3050 (Print)$\backslash$r0022-3050 (Linking)},
issn = {0022-3050},
journal = {Journal of Neurology, Neurosurgery {\&} Psychiatry},
number = {5},
pages = {489--499},
pmid = {2423648},
title = {{Word deafness: one hundred years later.}},
url = {http://jnnp.bmj.com/cgi/doi/10.1136/jnnp.49.5.489},
volume = {49},
year = {1986}
}
@article{Bizley2013,
abstract = {The fundamental perceptual unit in hearing is the 'auditory object'. Similar to visual objects, auditory objects are the computational result of the auditory system's capacity to detect, extract, segregate and group spectrotemporal regularities in the acoustic environment; the multitude of acoustic stimuli around us together form the auditory scene. However, unlike the visual scene, resolving the component objects within the auditory scene crucially depends on their temporal structure. Neural correlates of auditory objects are found throughout the auditory system. However, neural responses do not become correlated with a listener's perceptual reports until the level of the cortex. The roles of different neural structures and the contribution of different cognitive states to the perception of auditory objects are not yet fully understood.},
author = {Bizley, Jennifer K and Cohen, Yale E},
doi = {10.1038/nrn3565},
file = {::},
issn = {1471-0048},
journal = {Nature reviews. Neuroscience},
month = {oct},
number = {10},
pages = {693--707},
pmid = {24052177},
publisher = {NIH Public Access},
title = {{The what, where and how of auditory-object perception.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24052177 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4082027},
volume = {14},
year = {2013}
}
@article{Lein2007,
abstract = {Molecular approaches to understanding the functional circuitry of the nervous system promise new insights into the relationship between genes, brain and behaviour. The cellular diversity of the brain necessitates a cellular resolution approach towards understanding the functional genomics of the nervous system. We describe here an anatomically comprehensive digital atlas containing the expression patterns of approximately 20,000 genes in the adult mouse brain. Data were generated using automated high-throughput procedures for in situ hybridization and data acquisition, and are publicly accessible online. Newly developed image-based informatics tools allow global genome-scale structural analysis and cross-correlation, as well as identification of regionally enriched genes. Unbiased fine-resolution analysis has identified highly specific cellular markers as well as extensive evidence of cellular heterogeneity not evident in classical neuroanatomical atlases. This highly standardized atlas provides an open, primary data resource for a wide variety of further studies concerning brain organization and function.},
author = {Lein, Ed S and Hawrylycz, M J and Ao, N and Ayres, M and Bensinger, A and Bernard, A and Boe, A F and Boguski, M S and Brockway, K S and Byrnes, E J and Chen, L and Chen, T M and Chin, M C and Chong, J and Crook, B E and Czaplinska, A and Dang, C N and Datta, S and Dee, N R and Desaki, A L and Desta, T and Diep, E and Dolbeare, T A and Donelan, M J and Dong, H W and Dougherty, J G and Duncan, B J and Ebbert, A J and Eichele, G and Estin, L K and Faber, C and Facer, B A and Fields, R and Fischer, S R and Fliss, T P and Frensley, C and Gates, S N and Glattfelder, K J and Halverson, K R and Hart, M R and Hohmann, J G and Howell, M P and Jeung, D P and Johnson, R A and Karr, P T and Kawal, R and Kidney, J M and Knapik, R H and Kuan, C L and Lake, J H and Laramee, A R and Larsen, K D and Lau, C and Lemon, T A and Liang, A J and Liu, Y and Luong, L T and Michaels, J and Morgan, J J and Morgan, R J and Mortrud, M T and Mosqueda, N F and Ng, L L and Ng, R and Orta, G J and Overly, C C and Pak, T H and Parry, S E and Pathak, S D and Pearson, O C and Puchalski, R B and Riley, Z L and Rockett, H R and Rowland, S A and Royall, J J and Ruiz, M J and Sarno, N R and Schaffnit, K and Shapovalova, N V and Sivisay, T and Slaughterbeck, C R and Smith, S C and Smith, K A and Smith, B I and Sodt, A J and Stewart, N N and Stumpf, K R and Sunkin, S M and Sutram, M and Tam, A and Teemer, C D and Thaller, C and Thompson, C L and Varnam, L R and Visel, A and Whitlock, R M and Wohnoutka, P E and Wolkey, C K and Wong, V Y and Wood, M and Yaylaoglu, M B and Young, R C and Youngstrom, B L and Yuan, X F and Zhang, B and Zwingman, T A and Jones, A R},
doi = {10.1038/nature05453},
isbn = {1476-4687 (Electronic)},
issn = {0028-0836},
journal = {Nature},
keywords = {*Gene Expression Profiling,*Gene Expression Regulation,Animals,Brain/anatomy {\&} histology/cytology/*metabolism,Computational Biology,Genome/*genetics,Genomics,Hippocampus/anatomy {\&} histology/metabolism,Inbred C57BL,Male,Messenger/genetics/metabolism,Mice,Organ Specificity,RNA},
number = {7124},
pages = {168--176},
pmid = {17151600},
title = {{Genome-wide atlas of gene expression in the adult mouse brain}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve{\&}db=PubMed{\&}dopt=Citation{\&}list{\_}uids=17151600},
volume = {445},
year = {2007}
}
@article{Holt2010,
author = {Holt, L. L. and Lotto, A. J.},
doi = {10.3758/APP.72.5.1218},
file = {::},
issn = {1943-3921},
journal = {Attention, Perception {\&} Psychophysics},
month = {jul},
number = {5},
pages = {1218--1227},
publisher = {Springer-Verlag},
title = {{Speech perception as categorization}},
url = {http://www.springerlink.com/index/10.3758/APP.72.5.1218},
volume = {72},
year = {2010}
}
@misc{Dooling1995,
abstract = {Discrimination of three synthetic versions of a /ra–la/ speech continuum was studied in two species of birds. The stimuli used in these experiments were identical to those used in a previous study of speech perception by humans [Best et al., Percept. Psychophys. 45, 237–250 (1989)]. Budgerigars and zebra finches were trained using operant conditioning and tested on three different series of acoustic stimuli: three‐formant synthetic speech, sinewave versions of those tokens, and isolated F3 tones from the sinewave speech. Both species showed enhanced discrimination performance near the /l/–/r/ boundary in the full‐formant speech continuum, whereas for the F3 continuum, neither species showed a peak near this boundary. These results are similar to human discrimination of the same continua. Budgerigars also showed a peak in discrimination of the sinewave analog continuum paralleling that for full‐formant syllables, similar to humans who are induced to perceive sinewave speech as speech. Zebra finches, by contrast, showed a relatively flat function mirroring their performance for F3 sinewaves, similar to humans who are induced to perceive sinewave speech as nonspeech. These data provide new evidence of species similarities and differences in the discrimination of speech and speechlike sounds. These data also strengthen and refine previous findings on the sensitivities of the vertebrate auditory system to the acoustic distinctions between speechsound categories.},
author = {Dooling, Robert J and Best, Catherine T. and Brown, Susan D.},
booktitle = {The Journal of the Acoustical Society of America},
doi = {10.1121/1.412058},
isbn = {0001-4966 (Print)},
issn = {00014966},
number = {3},
pages = {1839--1846},
pmid = {7699165},
title = {{Discrimination of synthetic full-formant and sinewave /ra–la/ continua by budgerigars (Melopsittacus undulatus) and zebra finches (Taeniopygia guttata)}},
url = {http://scitation.aip.org/content/asa/journal/jasa/97/3/10.1121/1.412058},
volume = {97},
year = {1995}
}
@article{Rauschecker2009a,
abstract = {Speech and language are considered uniquely human abilities: animals have communication systems, but they do not match human linguistic skills in terms of recursive structure and combinatorial power. Yet, in evolution, spoken language must have emerged from neural mechanisms at least partially available in animals. In this paper, we will demonstrate how our understanding of speech perception, one important facet of language, has profited from findings and theory in nonhuman primate studies. Chief among these are physiological and anatomical studies showing that primate auditory cortex, across species, shows patterns of hierarchical structure, topographic mapping and streams of functional processing. We will identify roles for different cortical areas in the perceptual processing of speech and review functional imaging work in humans that bears on our understanding of how the brain decodes and monitors speech. A new model connects structures in the temporal, frontal and parietal lobes linking speech perception and production.},
author = {Rauschecker, Josef P and Scott, Sophie K},
doi = {10.1038/nn.2331},
file = {::},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Anatomy,Animals,Auditory Cortex,Auditory Cortex: anatomy {\&} histology,Auditory Cortex: physiology,Auditory Pathways,Auditory Pathways: anatomy {\&} histology,Auditory Pathways: physiology,Biological Evolution,Brain Mapping,Comparative,Humans,Models,Nerve Net,Nerve Net: anatomy {\&} histology,Nerve Net: physiology,Neurological,Primates,Primates: anatomy {\&} histology,Primates: physiology,Speech Perception,Speech Perception: physiology},
month = {jun},
number = {6},
pages = {718--724},
pmid = {19471271},
title = {{Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing}},
url = {http://www.nature.com/doifinder/10.1038/nn.2331},
volume = {12},
year = {2009}
}
@article{Wang2005a,
abstract = {It has been well documented that neurons in the auditory cortex of anaesthetized animals generally display transient responses to acoustic stimulation, and typically respond to a brief stimulus with one or fewer action potentials. The number of action potentials evoked by each stimulus usually does not increase with increasing stimulus duration. Such observations have long puzzled researchers across disciplines and raised serious questions regarding the role of the auditory cortex in encoding ongoing acoustic signals. Contrary to these long-held views, here we show that single neurons in both primary (area A1) and lateral belt areas of the auditory cortex of awake marmoset monkeys (Callithrix jacchus) are capable of firing in a sustained manner over a prolonged period of time, especially when they are driven by their preferred stimuli. In contrast, responses become more transient or phasic when auditory cortex neurons respond to non-preferred stimuli. These findings suggest that when the auditory cortex is stimulated by a sound, a particular population of neurons fire maximally throughout the duration of the sound. Responses of other, less optimally driven neurons fade away quickly after stimulus onset. This results in a selective representation of the sound across both neuronal population and time.},
author = {Wang, Xiaoqin and Lu, Thomas and Snider, Ross K and Liang, Li},
doi = {10.1038/nature03565},
file = {::;::},
isbn = {1476-4687 (Electronic)$\backslash$n0028-0836 (Linking)},
issn = {1476-4687},
journal = {Nature},
keywords = {Acoustic Stimulation,Action Potentials,Action Potentials: physiology,Animals,Auditory Cortex,Auditory Cortex: cytology,Auditory Cortex: physiology,Auditory Perception,Auditory Perception: physiology,Callithrix,Callithrix: physiology,Models,Neurological,Neurons,Neurons: physiology,Sound,Time Factors,Wakefulness,Wakefulness: physiology},
number = {7040},
pages = {341--6},
pmid = {15902257},
title = {{Sustained firing in auditory cortex evoked by preferred stimuli.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15902257},
volume = {435},
year = {2005}
}
@article{Goldinger2003,
abstract = {Although speech signals are continuous and variable, listeners experience segmentation and linguistic structure in perception. For years, researchers have tried to identify the basic building-block of speech perception. In that time, experimental methods have evolved, constraints on stimulus materials have evolved, sources of variance have been identified, and computational models have been advanced. As a result, the slate of candidate units has increased, each with its own empirical support. In this article, we endorse Grossberg's adaptive resonance theory (ART), proposing that speech units are emergent properties of perceptual dynamics. By this view, units only "exist" when disparate features achieve resonance, a level of perceptual coherence that allows conscious encoding. We outline basic principles of ART, then summarize five experiments. Three experiments assessed the power of social influence to affect phonemesyllable competitions. Two other experiments assessed repetition effects in monitoring data. Together the data suggest that "primary" speech units are strongly and symmetrically affected by bottom-up and top-down knowledge sources. ?? 2003 Elsevier Ltd. All rights reserved.},
author = {Goldinger, Stephen D. and Azuma, Tamiko},
doi = {10.1016/S0095-4470(03)00030-5},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Goldinger, Azuma - 2003 - Puzzle-solving science The quixotic quest for units in speech perception(2).pdf:pdf},
isbn = {0095-4470},
issn = {00954470},
journal = {Journal of Phonetics},
number = {3-4},
pages = {305--320},
pmid = {18292779},
title = {{Puzzle-solving science: The quixotic quest for units in speech perception}},
volume = {31},
year = {2003}
}
@article{LIBERMAN1957,
author = {LIBERMAN, A M and HARRIS, K S and HOFFMAN, H S and GRIFFITH, B C},
issn = {0022-1015},
journal = {Journal of experimental psychology},
keywords = {HEARING,SPEECH},
month = {nov},
number = {5},
pages = {358--68},
pmid = {13481283},
title = {{The discrimination of speech sounds within and across phoneme boundaries.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/13481283},
volume = {54},
year = {1957}
}
@article{Howard2009,
author = {Howard, James D and Plailly, Jane and Grueschow, Marcus and Haynes, John-Dylan and Gottfried, Jay A},
doi = {10.1038/nn.2324},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {jul},
number = {7},
pages = {932--938},
publisher = {Nature Publishing Group},
title = {{Odor quality coding and categorization in human posterior piriform cortex}},
url = {http://www.nature.com/doifinder/10.1038/nn.2324},
volume = {12},
year = {2009}
}
@incollection{Farnetani1990,
address = {Dordrecht},
author = {Farnetani, E.},
booktitle = {Speech Production and Speech Modelling},
doi = {10.1007/978-94-009-2037-8_5},
file = {::},
pages = {93--130},
publisher = {Springer Netherlands},
title = {{V-C-V Lingual Coarticulation and Its Spatiotemporal Domain}},
url = {http://www.springerlink.com/index/10.1007/978-94-009-2037-8{\_}5},
year = {1990}
}
@misc{Stevens1989,
abstract = {This is a review of regularities which we have observed in the analysis of text reading, mostly Swedish, directed to the timing of vowels and consonants, syllables, inter-stress intervals and pauses. We have found tendencies of quantal aspects of temporal structure, superimposed on more gradual variations, which add quasi-rhythmical elements to speech. A local average of inter-stress intervals of the order of 0.5 sec. appears to function as a reference quantum for the planning of pause durations. A recent study, confirming our previous findings of multiple peaks with about 0.5 sec. spacing in histograms of pause durations, provides support for this model. It is well established that pause durations tend to increase with increasing syntactic level of boundaries. However, these variations tend to be quantally scaled, even within a specific boundary category, e.g. between sentences or between paragraphs. Relatively short pauses, as between phrases or clauses, show durations in complementary relation to terminal lengthening. There are indications of approximately 1, 1/2, 1/4 and 1/8 ratios of the average durations of inter-stress intervals, stressed syllables, unstressed syllables and phoneme segments, which add to the observed regularities. The timing of syllables and phonetic segments, with due regard to relative distinctiveness and reading speed, is discussed, and also tempo variations within a sentence},
author = {Stevens, K. N},
booktitle = {Journal of Phonetics},
doi = {10.1109/ICSLP.1996.607202},
isbn = {0-7803-3555-4},
issn = {00954470},
pages = {3--45},
pmid = {982},
title = {{On the quantal nature of speech}},
volume = {17},
year = {1989}
}
@article{Newell2002,
abstract = {We report three experiments where the categorical perception of familiar, three-dimensional objects was investigated. A continuum of shape change between 15 pairs of objects was created and the images along the continuum were used as stimuli. In Experiment 1 participants were first required to discriminate pairs of images of objects that lay along the shape continuum. Then participants were asked to classify each morph-image into one of two pre-specified classes. We found evidence for categorical perception in some but not all of our object pairs. In Experiment 2 we varied the viewpoint of the objects in the discrimination task and found that effects of categorical perception generalized across changes in view. In Experiment 3 similarity ratings for each object pair were collected. These similarity scores correlated with the degree of perceptual categorization found for the object pairs. Our findings suggest that some familiar objects are perceived categorically and that categorical perception is closely tied to inter-object perceptual similarity.},
author = {Newell, Fiona N and B{\"{u}}lthoff, Heinrich H},
doi = {10.1016/S0010-0277(02)00104-X},
file = {::},
issn = {00100277},
journal = {Cognition},
number = {2},
pages = {113--143},
title = {{Categorical perception of familiar objects}},
volume = {85},
year = {2002}
}
@article{Davis2007,
abstract = {This paper focuses on the cognitive and neural mechanisms of speech perception: the rapid, and highly automatic processes by which complex time-varying speech signals are perceived as sequences of meaningful linguistic units. We will review four processes that contribute to the perception of speech: perceptual grouping, lexical segmentation, perceptual learning and categorical perception, in each case presenting perceptual evidence to support highly interactive processes with top-down information flow driving and constraining interpretations of spoken input. The cognitive and neural underpinnings of these interactive processes appear to depend on two distinct representations of heard speech: an auditory, echoic representation of incoming speech, and a motoric/somatotopic representation of speech as it would be produced. We review the neuroanatomical system supporting these two key properties of speech perception and discuss how this system incorporates interactive processes and two parallel echoic and somato-motoric representations, drawing on evidence from functional neuroimaging studies in humans and from comparative anatomical studies. We propose that top-down interactive mechanisms within auditory networks play an important role in explaining the perception of spoken language. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Davis, Matthew H. and Johnsrude, Ingrid S.},
doi = {10.1016/j.heares.2007.01.014},
file = {::},
isbn = {0378-5955 (Print)$\backslash$r0378-5955 (Linking)},
issn = {03785955},
journal = {Hearing Research},
keywords = {Auditory cortex,Categorical perception,Feedback,Frontal lobe,Lexical segmentation,Perceptual grouping,Perceptual learning,Speech perception,Temporal lobe,fMRI},
number = {1-2},
pages = {132--147},
pmid = {17317056},
title = {{Hearing speech sounds: Top-down influences on the interface between audition and speech perception}},
volume = {229},
year = {2007}
}
@unpublished{Knight2014,
abstract = {The tactile surface forms a continuous sheet covering the body. And yet, the perceived distance between two touches varies across stimulation sites. Perceived tactile distance is larger when stimuli cross over the wrist, compared to when both fall on either the hand or the forearm. This effect could reflect a categorical distortion of tactile space across body-part boundaries (in which stimuli crossing the wrist boundary are perceptually elongated) or may simply reflect a localised increased in acuity surrounding anatomical landmarks (in which stimuli near the wrist are perceptually elongated). We tested these two interpretations across two experiments, by comparing a well-documented bias to perceive mediolateral tactile distances across the forearm/hand as larger than proximodistal ones along the forearm/hand at three different sites (hand, wrist, and forearm). According to the ‘categorical' interpretation, tactile distances should be elongated selectively in the proximodistal axis thus reducing the anisotropy. According to the ‘localised acuity' interpretation, distances will be perceptually elongated in the vicinity of the wrist regardless of orientation, leading to increased overall size without affecting anisotropy. Consistent with the categorical account, we found a reduction in the magnitude of anisotropy at the wrist, with no evidence of a corresponding localised increase in precision. These findings demonstrate that we reference touch to a representation of the body that is categorically segmented into discrete parts, which consequently influences the perception of tactile distance.},
author = {Knight, Frances Le Cornu and Longo, Matthew R. and Bremner, Andrew J.},
booktitle = {Cognition},
doi = {10.1016/j.cognition.2014.01.005},
file = {::},
issn = {00100277},
number = {2},
pages = {254--262},
title = {{Categorical perception of tactile distance}},
volume = {131},
year = {2014}
}
@article{Repp1989,
abstract = {This study focuses on the initial component of the stop consonant release burst, the release transient. In theory, the transient, because of its impulselike source, should contain much information about the vocal tract configuration at release, but it is usually weak in intensity and difficult to isolate from the accompanying frication in natural speech. For this investigation, a human talker produced isolated release transients of /b,d,g/ in nine vocalic contexts by whispering these syllables very quietly. He also produced the corresponding CV syllables with regular phonation for comparison. Spectral analyses showed the isolated transients to have a clearly defined formant structure, which was not seen in natural release bursts, whose spectra were dominated by the frication noise. The formant frequencies varied systematically with both consonant place of articulation and vocalic context. Perceptual experiments showed that listeners can identify both consonants and vowels from isolated transients, though not very accurately. Knowing one of the two segments in advance did not help, but when the transients were followed by a compatible synthetic, steady-state vowel, consonant identification improved somewhat. On the whole, isolated transients, despite their clear formant structure, provided only partial information for consonant identification, but no less so, it seems, than excerpted natural release bursts. The information conveyed by artificially isolated transients and by natural (frication-dominated) release bursts appears to be perceptually equivalent.},
author = {Repp, B H and Lin, H B},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {jan},
number = {1},
pages = {379--96},
pmid = {2921420},
title = {{Acoustic properties and perception of stop consonant release transients.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2921420},
volume = {85},
year = {1989}
}
@book{James1890,
address = {New York},
author = {James, William},
publisher = {H. Holt},
title = {{The Principles of Psychology, Vol. 1}},
year = {1890}
}
@incollection{Harnad1987,
abstract = {Categorization is a very basic cognitive activity. It is involved in any task that calls for differential responding, from operant discrimination to pattern recognition to naming and describing objects and states-of-affairs. Explanations of categorization range from nativist theories denying that any nontrivial categories are acquired by learning to inductivist theories claiming that most categories are learned. "Categorical perception" (CP) is the name given to a suggestive perceptual phenomenon that may serve as a useful model for categorization in general: For certain perceptual categories, within-category differences look much smaller than between-category differences even when they are of the same size physically. For example, in color perception, differences between reds and differences between yellows look much smaller than equal-sized differences that cross the red/yellow boundary; the same is true of the phoneme categories /ba/ and /da/. Indeed, the effect of the category boundary is not merely quantitative, but qualitative.},
author = {Harnad, Stevan},
booktitle = {Categorical Perception: The Groundwork of Cognition},
pages = {1--52},
publisher = {Cambridge University Press},
title = {{Psychophysical and cognitive aspects of categorical perception: A critical overview}},
year = {1987}
}
@book{Collins2003,
abstract = {Routledge English Language Introductions cover core areas of language study and are one-stop resources for students.Assuming no prior knowledge, books in the series offer an accessible overview of the subject, with activities, study questions, sample analyses, commentaries and key readings—all in the same volume. The innovative and flexible 'two-dimensional' structure is built around four sections—introduction, development, exploration and extension—which offer self-contained stages for study. Each topic can also be read across these sections, enabling the reader to build gradually on the knowledge gained. Revised and updated throughout, this third edition of Practical Phonetics and Phonology:presents the essentials of the subject and their day-to-day applications in an engaging and accessible manner covers all the core concepts of speech science, such as the phoneme, syllable structure, production of speech, vowel and consonant possibilities, glottal settings, stress, rhythm, intonation and the surprises of connected speechincorporates classic readings from key names in the discipline including David Abercrombie, David Crystal, Dennis Fry, Daniel Jones, Peter Ladefoged, Peter Trudgill and John Wellsincludes an audio CD containing a collection of samples provided by genuine speakers of 25 accent varieties from Britain, Ireland, the USA, Canada, Australia, New Zealand, South Africa, India, Singapore and West Africagives outlines of the sound systems of six key languages from around the worldcontains over a hundred activity exercises, many accompanied by audio materialis accompanied by a brand new companion website featuring additional guidance, audio files, keys to activities in the book, further exercises and activities, and extra practice in phonemic transcriptionNew features of this edition include an additional reading on teaching pronunciation, phonetic descriptions of three more languages (Japanese, Polish and Italian), expanded material on spelling/sound relationships, more information on acquiring the pronunciation of a foreign language, additional suggestions for further reading and much new illustrative material. Written by authors who are experienced teachers and researchers, this best-selling textbook will appeal to all students of English language and linguistics and those training for a certificate in TEFL.},
author = {Collins, Beverley and Mees, Inger},
booktitle = {Routledge English language introductions series},
isbn = {0415261333 (cased) 0415261341 (pbk.)},
keywords = {English language Phonetics.},
pages = {46--61},
title = {{Practical phonetics and phonology : a resource book for students}},
year = {2003}
}
@article{Fugate2013,
abstract = {Categorical perception (CP) refers to how similar things look different depending on whether they are classified as the same category. Many studies demonstrate that adult humans show CP for human emotional faces. It is widely debated whether the effect can be accounted for solely by perceptual differences (structural differences among emotional faces) or whether additional perceiver-based conceptual knowledge is required. In this review, I discuss the phenomenon of CP and key studies showing CP for emotional faces. I then discuss a new model of emotion which highlights how perceptual and conceptual knowledge interact to explain how people see discrete emotions in others' faces. In doing so, I discuss how language (emotion words included in the paradigm) contribute to CP.},
author = {Fugate, Jennifer M. B.},
doi = {10.1177/1754073912451350},
file = {::},
issn = {1754-0739},
journal = {Emotion Review},
keywords = {categorical perception,emotional faces,language},
month = {jan},
number = {1},
pages = {84--89},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Categorical Perception for Emotional Faces}},
url = {http://journals.sagepub.com/doi/10.1177/1754073912451350},
volume = {5},
year = {2013}
}
@article{Tabain2000,
abstract = {Using electropalatographic (EPG) data, the following hypothesis is tested: the slope value generated by a locus equation (LE) analysis of the F 2 transition in CV syllables is an accurate reflection of the degree of coarticulation between the consonant and the vowel in that syllable. The consonants studied are /∂ z z d g n n l r/. Comparisons between EPG and LE data suggest that the LE analysis provides an accurate reflection of the degree of coarticulation for the stop and nasal classes in English, which have an alveolar and a velar place of articulation amongst the lingual consonants. By contrast, the correlation between EPG and LE data for the fricative consonants is very poor. Two explanations are offered for the poorer results for the fricatives: (1) the fricative noise following consonant release obscures the F 2 transition, rendering formant measurement less accurate, and (2) the LE is capable of encoding gross differences in degree of coarticulation (such as that between an alveolar and a velar) but not more subtle differences such as those between the various coronal articulations.},
author = {Tabain, Marija},
doi = {10.1006/jpho.2000.0110},
file = {::},
issn = {00954470},
journal = {Journal of Phonetics},
number = {2},
pages = {137--159},
title = {{Coarticulation in CV syllables: a comparison of Locus Equation and EPG data}},
volume = {28},
year = {2000}
}
@article{Bidelman2013,
abstract = {Speech perception requires the effortless mapping from smooth, seemingly continuous changes in sound features into discrete perceptual units, a conversion exemplified in the phenomenon of categorical perception. Explaining how/when the human brain performs this acoustic-phonetic transformation remains an elusive problem in current models and theories of speech perception. In previous attempts to decipher the neural basis of speech perception, it is often unclear whether the alleged brain correlates reflect an underlying percept or merely changes in neural activity that covary with parameters of the stimulus. Here, we recorded neuroelectric activity generated at both cortical and subcortical levels of the auditory pathway elicited by a speech vowel continuum whose percept varied categorically from /u/ to /a/. This integrative approach allows us to characterize how various auditory structures code, transform, and ultimately render the perception of speech material as well as dissociate brain responses reflecting changes in stimulus acoustics from those that index true internalized percepts. We find that activity from the brainstem mirrors properties of the speech waveform with remarkable fidelity, reflecting progressive changes in speech acoustics but not the discrete phonetic classes reported behaviorally. In comparison, patterns of late cortical evoked activity contain information reflecting distinct perceptual categories and predict the abstract phonetic speech boundaries heard by listeners. Our findings demonstrate a critical transformation in neural speech representations between brainstem and early auditory cortex analogous to an acoustic-phonetic mapping necessary to generate categorical speech percepts. Analytic modeling demonstrates that a simple nonlinearity accounts for the transformation between early (subcortical) brain activity and subsequent cortical/behavioral responses to speech ({\textgreater}. 150-200. ms) thereby describing a plausible mechanism by which the brain achieves its acoustic-to-phonetic mapping. Results provide evidence that the neurophysiological underpinnings of categorical speech are present cortically by {\~{}}. 175. ms after sound enters the ear. ?? 2013 Elsevier Inc..},
author = {Bidelman, Gavin M. and Moreno, Sylvain and Alain, Claude},
doi = {10.1016/j.neuroimage.2013.04.093},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Bidelman, Moreno, Alain - 2013 - Tracing the emergence of categorical speech perception in the human auditory system(2).pdf:pdf},
isbn = {1053-8119},
issn = {10538119},
journal = {NeuroImage},
keywords = {Auditory event-related potentials (ERP),Brainstem response,Categorical perception,Neural computation,Speech perception},
pages = {201--212},
pmid = {23648960},
publisher = {Elsevier Inc.},
title = {{Tracing the emergence of categorical speech perception in the human auditory system}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2013.04.093},
volume = {79},
year = {2013}
}
@article{Feldman2009,
author = {Feldman, Naomi H. and Griffiths, Thomas L. and Morgan, James L.},
doi = {10.1037/a0017196},
file = {::},
issn = {1939-1471},
journal = {Psychological Review},
keywords = {Bayesian inference,categorical perception,noise,perceptual magnet effect,phonetic categorization,rational analysis,speech perception},
number = {4},
pages = {752--782},
publisher = {American Psychological Association},
title = {{The influence of categories on perception: Explaining the perceptual magnet effect as optimal statistical inference.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0017196},
volume = {116},
year = {2009}
}
@article{Howard2000,
author = {Howard, M.A. and Volkov, I.O. and Mirsky, R. and Garell, P.C. and Noh, M.D. and Granner, M. and Damasio, H. and Steinschneider, M. and Reale, R.A. and Hind, J.E. and Brugge, J.F.},
doi = {10.1002/(SICI)1096-9861(20000103)416:1<79::AID-CNE6>3.0.CO;2-2},
file = {::},
issn = {0021-9967},
journal = {The Journal of Comparative Neurology},
keywords = {audition,auditory cortex,hearing},
month = {jan},
number = {1},
pages = {79--92},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Auditory cortex on the human posterior superior temporal gyrus}},
url = {http://doi.wiley.com/10.1002/{\%}28SICI{\%}291096-9861{\%}2820000103{\%}29416{\%}3A1{\%}3C79{\%}3A{\%}3AAID-CNE6{\%}3E3.0.CO{\%}3B2-2},
volume = {416},
year = {2000}
}
@article{Boersma2001,
abstract = {See, stats, and : http : / / www . researchgate . net / publication / 208032992 PRAAT , a computer ARTICLE CITATIONS 942 DOWNLOADS 870 VIEWS 1 , 365 2 : Paul University 104 , 780 SEE David University 19 , 721 SEE Available : Paul Retrieved : 28},
author = {Boersma, Paul},
doi = {10.1097/AUD.0b013e31821473f7},
isbn = {1381-3439},
issn = {0196-0202},
journal = {Glot International},
number = {9/10},
pages = {341--347},
pmid = {61},
title = {{Praat, a system for doing phonetics by computer}},
volume = {5},
year = {2001}
}
@article{Kronrod2016a,
author = {Kronrod, Yakov and Coppess, Emily and Feldman, Naomi H.},
doi = {10.3758/s13423-016-1049-y},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
month = {dec},
number = {6},
pages = {1681--1712},
publisher = {Springer US},
title = {{A unified account of categorical effects in phonetic perception}},
url = {http://link.springer.com/10.3758/s13423-016-1049-y},
volume = {23},
year = {2016}
}
@article{Beale1995,
abstract = {These studies suggest categorical perception effects may be much more general than has commonly been believed and can occur in apparently similar ways at dramatically different levels of processing. To test the nature of individual face representations, a linear continuum of “morphed” faces was generated between individual exemplars of familiar faces. In separate categorization, discrimination and “better-likeness” tasks, subjects viewed pairs of faces from these continua. Subjects discriminate most accurately when face-pairs straddle apparent category boundaries; thus individual faces are perceived categorically. A high correlation is found between the familiarity of a face-pair and the magnitude of the categorization effect. Categorical perception therefore is not limited to low-level perceptual continua, but can occur at higher levels and may be acquired through experience as well.},
author = {Beale, James M. and Keil, Frank C.},
doi = {10.1016/0010-0277(95)00669-X},
file = {::},
issn = {00100277},
journal = {Cognition},
number = {3},
pages = {217--239},
title = {{Categorical effects in the perception of faces}},
volume = {57},
year = {1995}
}
@article{Grossberg2013,
abstract = {Adaptive Resonance Theory, or ART, is a cognitive and neural theory of how the brain autonomously learns to categorize, recognize, and predict objects and events in a changing world. This article reviews classical and recent developments of ART, and provides a synthesis of concepts, principles, mechanisms, architectures, and the interdisciplinary data bases that they have helped to explain and predict. The review illustrates that ART is currently the most highly developed cognitive and neural theory available, with the broadest explanatory and predictive range. Central to ART's predictive power is its ability to carry out fast, incremental, and stable unsupervised and supervised learning in response to a changing world. ART specifies mechanistic links between processes of consciousness, learning, expectation, attention, resonance, and synchrony during both unsupervised and supervised learning. ART provides functional and mechanistic explanations of such diverse topics as laminar cortical circuitry; invariant object and scenic gist learning and recognition; prototype, surface, and boundary attention; gamma and beta oscillations; learning of entorhinal grid cells and hippocampal place cells; computation of homologous spatial and temporal mechanisms in the entorhinal-hippocampal system; vigilance breakdowns during autism and medial temporal amnesia; cognitive-emotional interactions that focus attention on valued objects in an adaptively timed way; item-order-rank working memories and learned list chunks for the planning and control of sequences of linguistic, spatial, and motor information; conscious speech percepts that are influenced by future context; auditory streaming in noise during source segregation; and speaker normalization. Brain regions that are functionally described include visual and auditory neocortex; specific and nonspecific thalamic nuclei; inferotemporal, parietal, prefrontal, entorhinal, hippocampal, parahippocampal, perirhinal, and motor cortices; frontal eye fields; supplementary eye fields; amygdala; basal ganglia: cerebellum; and superior colliculus. Due to the complementary organization of the brain, ART does not describe many spatial and motor behaviors whose matching and learning laws differ from those of ART. ART algorithms for engineering and technology are listed, as are comparisons with other types of models. ?? 2012 Elsevier Ltd.},
author = {Grossberg, Stephen},
doi = {10.1016/j.neunet.2012.09.017},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Grossberg - 2013 - Adaptive Resonance Theory How a brain learns to consciously attend, learn, and recognize a changing world(2).pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Adaptive Resonance Theory,Adaptive timing,Amygdala,Attention,Basal ganglia,Consciousness,Entorhinal cortex,Expectation,Gamma and beta oscillations,Hippocampal cortex,Inferotemporal cortex,Learning,Parietal cortex,Prefrontal cortex,Recognition,Reinforcement learning,Speech perception,Synchrony,Working memory},
pages = {1--47},
pmid = {23149242},
publisher = {Elsevier Ltd},
title = {{Adaptive Resonance Theory: How a brain learns to consciously attend, learn, and recognize a changing world}},
url = {http://dx.doi.org/10.1016/j.neunet.2012.09.017},
volume = {37},
year = {2013}
}
@article{Bird2014,
abstract = {The areas of the brain that encode color categorically have not yet been reliably identified. Here, we used functional MRI adaptation to identify neuronal populations that represent color categories irrespective of metric differences in color. Two colors were successively presented within a block of trials. The two colors were either from the same or different categories (e.g., "blue 1 and blue 2" or "blue 1 and green 1"), and the size of the hue difference was varied. Participants performed a target detection task unrelated to the difference in color. In the middle frontal gyrus of both hemispheres and to a lesser extent, the cerebellum, blood-oxygen level-dependent response was greater for colors from different categories relative to colors from the same category. Importantly, activation in these regions was not modulated by the size of the hue difference, suggesting that neurons in these regions represent color categorically, regardless of metric color difference. Representational similarity analyses, which investigated the similarity of the pattern of activity across local groups of voxels, identified other regions of the brain (including the visual cortex), which responded to metric but not categorical color differences. Therefore, categorical and metric hue differences appear to be coded in qualitatively different ways and in different brain regions. These findings have implications for the long-standing debate on the origin and nature of color categories, and also further our understanding of how color is processed by the brain.},
author = {Bird, Chris M and Berens, Samuel C and Horner, Aidan J and Franklin, Anna},
doi = {10.1073/pnas.1315275111},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {categorization,chromatic,functional magnetic resonance imaging},
month = {mar},
number = {12},
pages = {4590--5},
pmid = {24591602},
publisher = {National Academy of Sciences},
title = {{Categorical encoding of color in the brain.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24591602 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3970503},
volume = {111},
year = {2014}
}
@article{Belin2000a,
abstract = {The human voice contains in its acoustic structure a wealth of information on the speaker's identity and emotional state which we perceive with remarkable ease and accuracy. Although the perception of speaker-related features of voice plays a major role in human communication, little is known about its neural basis. Here we show, using functional magnetic resonance imaging in human volunteers, that voice-selective regions can be found bilaterally along the upper bank of the superior temporal sulcus (STS). These regions showed greater neuronal activity when subjects listened passively to vocal sounds, whether speech or non-speech, than to non-vocal environmental sounds. Central STS regions also displayed a high degree of selectivity by responding significantly more to vocal sounds than to matched control stimuli, including scrambled voices and amplitude-modulated noise. Moreover, their response to stimuli degraded by frequency filtering paralleled the subjects' behavioural performance in voice-perception tasks that used these stimuli. The voice-selective areas in the STS may represent the counterpart of the face-selective areas in human visual cortex; their existence sheds new light on the functional architecture of the human auditory cortex.},
author = {Belin, P and Zatorre, R J and Lafaille, P and Ahad, P and Pike, B},
doi = {10.1038/35002078},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Belin et al. - 2000 - Voice-selective areas in human auditory cortex(3).pdf:pdf;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Belin et al. - 2000 - Voice-selective areas in human auditory cortex(4).pdf:pdf},
isbn = {0028-0836 (Print)},
issn = {0028-0836},
journal = {Nature},
number = {6767},
pages = {309--312},
pmid = {10659849},
title = {{Voice-selective areas in human auditory cortex.}},
volume = {403},
year = {2000}
}
@misc{Ghazanfar1999,
abstract = {In this article, we review behavioral and neurobiological studies of the perception and use of species-specific vocalizations by non-human primates. At the behavioral level, primate vocal perception shares many features with speech perception by humans. These features include a left-hemisphere bias towards conspecific vocalizations, the use of temporal features for identifying different calls, and the use of calls to refer to objects and events in the environment. The putative neural bases for some of these behaviors have been revealed by recent studies of the primate auditory and prefrontal cortices. These studies also suggest homologies with the human language circuitry. Thus, a synthesis of cognitive, ethological and neurobiological approaches to primate vocal behavior is likely to yield the richest understanding of the neural bases of speech perception, and might also shed light on the evolutionary precursors to language.},
author = {Ghazanfar, Asif A. and Hauser, Marc D.},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/S1364-6613(99)01379-0},
isbn = {1364-6613},
issn = {13646613},
number = {10},
pages = {377--384},
pmid = {10498928},
title = {{The neuroethology of primate vocal communication: Substrates for the evolution of speech}},
volume = {3},
year = {1999}
}
@book{Stevens1998,
abstract = {Content Description {\#}Includes bibliographical references (p.) and index. Preface -- 1. Anatomy and physiology of speech production -- 2. Source mechanisms -- 3. Basic acoustics of vocal tract resonators -- 4. Auditory processing of speechlike sounds -- 5. Phonological representation of utterances -- 6. Vowels : acoustic events with a relatively open vocal tract -- 7. The basic stop consonants : bursts and formant transitions -- 8. Obstruent consonants -- 9. Sonorant consonants -- 10. Some influences of context on speech sound production.},
author = {Stevens, Kenneth N.},
isbn = {0262692503},
pages = {607},
publisher = {MIT Press},
title = {{Acoustic phonetics}},
url = {https://books.google.com/books?id=Gej94hCGrLMC{\&}source=gbs{\_}navlinks{\_}s},
year = {1998}
}
@article{Schwartz2012,
abstract = {Vowels are by far the best understood units in human sound systems, and are well characterized at the articulatory, acoustic, and perceptual levels. This has permitted explanations of vowel systems as structured by perception, and has led to effective substance-based theories. By contrast, stops are far less thoroughly understood. In this paper we use an articulatory-acoustic model of the vocal tract to examine stop consonant place in terms of both articulation and formant values. This allows us to locate each place of articulation in the F1-F2-F3 space, and to demonstrate in "articulatory nomograms" how formants evolve while closure is displaced from the front to the back of the vocal tract. Then, in the framework of the "Perception for Action Control Theory" that we have developed in recent years, we show that the near universal labial-coronal-velar stop series (i.e., /b d g/ or /p t k/) is a perceptually optimal structure for stops just as /i a u/ is for vowels, provided that it is embedded in a suitable perceptuo-motor framework. ?? 2011 Elsevier Ltd.},
author = {Schwartz, Jean-Luc and Bo{\"{e}}, Louis-Jean and Badin, Pierre and Sawallis, Thomas R.},
doi = {10.1016/j.wocn.2011.10.004},
file = {::},
isbn = {0095-4470},
issn = {00954470},
journal = {Journal of Phonetics},
number = {1},
pages = {20--36},
title = {{Grounding stop place systems in the perceptuo-motor substance of speech: On the universality of the labial–coronal–velar stop series}},
volume = {40},
year = {2012}
}
@article{Dorman1977,
author = {Dorman, M. F. and Studdert-Kennedy, M. and Raphael, L. J.},
doi = {10.3758/BF03198744},
file = {::},
issn = {0031-5117},
journal = {Perception {\&} Psychophysics},
month = {mar},
number = {2},
pages = {109--122},
publisher = {Springer-Verlag},
title = {{Stop-consonant recognition: Release bursts and formant transitions as functionally equivalent, context-dependent cues}},
url = {http://www.springerlink.com/index/10.3758/BF03198744},
volume = {22},
year = {1977}
}
@article{Steinschneider2003,
abstract = {Voice onset time (VOT) signifies the interval between consonant onset and the start of rhythmic vocal-cord vibrations. Differential perception of consonants such as /d/ and /t/ is categorical in American English, with the boundary generally lying at a VOT of 20-40 ms. This study tests whether previously identified response patterns that differentially reflect VOT are maintained in large-scale population activity within primary auditory cortex (A1) of the awake monkey. Multiunit activity and current source density patterns evoked by the syllables /da/ and /ta/ with variable VOTs are examined. Neural representation is determined by the tonotopic organization. Differential response patterns are restricted to lower best-frequency regions. Response peaks time-locked to both consonant and voicing onsets are observed for syllables with a 40- and 60-ms VOT, whereas syllables with a 0- and 20-ms VOT evoke a single response time-locked only to consonant onset. Duration of aspiration noise is represented in higher best-frequency regions. Representation of VOT and aspiration noise in discrete tonotopic areas of A1 suggest that integration of these phonetic cues occurs in secondary areas of auditory cortex. Findings are consistent with the evolving concept that complex stimuli are encoded by synchronized activity in large-scale neuronal ensembles.},
author = {Steinschneider, Mitchell and Fishman, Yonatan I and Arezzo, Joseph C},
doi = {10.1121/1.1582449},
file = {::},
isbn = {00014966},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {307--321},
pmid = {12880043},
title = {{Representation of the voice onset time (VOT) speech parameter in population responses within primary auditory cortex of the awake monkey.}},
volume = {114},
year = {2003}
}
@article{Lindblom2012,
abstract = {A programmatic series of studies aimed at expanding our understanding of coarticulation in V 1{\textperiodcentered}CV 2 sequences is presented. The common thread was examining coarticulatory dynamics through the prism of locus equations (LEs). Multiple experimental methodologies (articulatory synthesis, X-ray film, Principal Component Analysis, and extraction of time constants for F2 transitions), guided by a few theoretical assumptions about speech motor planning and control, were used to uncover the articulatory underpinnings responsible for the trademark acoustic form of LE scatterplots. Specific findings were: (1) the concept of a stop consonantal 'target' was quantitatively derived as a vowel-neutral, 'deactivated,' tongue contour; (2) the linearity of LEs is significantly enhanced by the uniformity of F2 transition time constants, which normalize with respect to F2 transition extents, and an inherent linear bias created by the smaller frequency range of [F2 onset-F2 vowel] relative to F2 vowel frequencies; (3) realistic LE slopes and y-intercepts were derived by modeling different extents of V 2 overlap onto stop consonantal target shapes at closure; and (4) a conceptually simple model, viz. interpolation between successive articulatory target shapes, followed by derivation of their formant values expressed as LEs, came surprisingly close to matching actual LEs obtained from our speaker. {\textcopyright} 2011 Elsevier Ltd.},
author = {Lindblom, Bj{\"{o}}rn and Sussman, Harvey M.},
doi = {10.1016/j.wocn.2011.09.005},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Lindblom, Sussman - 2012 - Dissecting coarticulation How locus equations happen(2).pdf:pdf},
isbn = {0095-4470},
issn = {00954470},
journal = {Journal of Phonetics},
number = {1},
pages = {1--19},
title = {{Dissecting coarticulation: How locus equations happen}},
volume = {40},
year = {2012}
}
@article{Chang2010,
abstract = {Speech perception requires the rapid and effortless extraction of meaningful phonetic information from a highly variable acoustic signal. A powerful example of this phenomenon is categorical speech perception, in which a continuum of acoustically varying sounds is transformed into perceptually distinct phoneme categories. We found that the neural representation of speech sounds is categorically organized in the human posterior superior temporal gyrus. Using intracranial high-density cortical surface arrays, we found that listening to synthesized speech stimuli varying in small and acoustically equal steps evoked distinct and invariant cortical population response patterns that were organized by their sensitivities to critical acoustic features. Phonetic category boundaries were similar between neurometric and psychometric functions. Although speech-sound responses were distributed, spatially discrete cortical loci were found to underlie specific phonetic discrimination. Our results provide direct evidence for acoustic-to-higher order phonetic level encoding of speech sounds in human language receptive cortex.},
author = {Chang, Edward F and Rieger, Jochem W and Johnson, Keith and Berger, Mitchel S and Barbaro, Nicholas M and Knight, Robert T},
doi = {10.1038/nn.2641},
file = {::},
issn = {1546-1726},
journal = {Nature neuroscience},
month = {nov},
number = {11},
pages = {1428--32},
pmid = {20890293},
publisher = {NIH Public Access},
title = {{Categorical speech representation in human superior temporal gyrus.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20890293 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2967728},
volume = {13},
year = {2010}
}
@article{Sussman1998,
abstract = {Neuroethological investigations of mammalian and avian auditory systems have documented species-specific specializations for processing complex acoustic signals that could, if viewed in abstract terms, have an intriguing and striking relevance for human speech sound categorization and representation. Each species forms biologically relevant categories based on combinatorial analysis of information-bearing parameters within the complex input signal. This target article uses known neural models from the mustached bat and barn owl to develop, by analogy, a conceptualization of human processing of consonant plus vowel sequences that offers a partial solution to the noninvariance dilemma--the nontransparent relationship between the acoustic waveform and the phonetic segment. Critical input sound parameters used to establish species-specific categories in the mustached bat and barn owl exhibit high correlation and linearity due to physical laws. A cue long known to be relevant to the perception of stop place of articulation is the second formant (F2) transition. This article describes an empirical phenomenon--the locus equations--that describes the relationship between the F2 of a vowel and the F2 measured at the onset of a consonant-vowel (CV) transition. These variables, F2 onset and F2 vowel within a given place category, are consistently and robustly linearly correlated across diverse speakers and languages, and even under perturbation conditions as imposed by bite blocks. A functional role for this category-level extreme correlation and linearity (the "orderly output constraint") is hypothesized based on the notion of an evolutionarily conserved auditory-processing strategy. High correlation and linearity between critical parameters in the speech signal that help to cue place of articulation categories might have evolved to satisfy a preadaptation by mammalian auditory systems for representing tightly correlated, linearly related components of acoustic signals.},
author = {Sussman, Harvey M and Fruchter, David and Hilbert, Jon and Sirosh, Joseph},
doi = {10.1017/S0140525X98001174},
file = {::;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Sussman et al. - 1998 - Linear correlates in the speech signal the orderly output constraint(3).pdf:pdf},
isbn = {0140-525X},
issn = {0140-525X},
journal = {The Behavioral and brain sciences},
keywords = {Humans,Phonetics,Speech,Speech Acoustics,Speech Perception,Speech Perception: physiology,Speech: physiology,acoustic,and frustration,because they,categories,linearity,locus equations,must,must not tolerate is,neuroethology,noninvariance,perception,phoneme,place of articulation,scientists do tolerate uncertainty,sound,speech signal,the one thing that,they do not and},
number = {2},
pages = {241--59; discussion 260--99},
pmid = {10097014},
publisher = {University of Oregon Library},
title = {{Linear correlates in the speech signal: the orderly output constraint.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10097014},
volume = {21},
year = {1998}
}
@article{Kuhl1992,
author = {Kuhl, PK and Williams, KA and Lacerda, F and Stevens, KN and Lindblom, B},
journal = {Science},
number = {5044},
title = {{Linguistic experience alters phonetic perception in infants by 6 months of age}},
volume = {255},
year = {1992}
}
@article{Syka2002,
abstract = {Gap detection threshold (GDT) was measured in adult female pigmented rats (strain Long–Evans) by an operant conditioning technique with food reinforcement, before and after bilateral ablation of the auditory cortex. GDT was dependent on the frequency spectrum and intensity of the continuously present noise in which the gaps were embedded. The mean values of GDT for gaps embedded in white noise or low-frequency noise (upper cutoff frequency 3 kHz) at 70 dB sound pressure level (SPL) were 1.57±0.07 ms and 2.9±0.34 ms, respectively. Decreasing noise intensity from 80 dB SPL to 20 dB SPL produced a significant increase in GDT. The increase in GDT was relatively small in the range of 80–50 dB SPL for white noise and in the range of 80–60 dB for low-frequency noise. The minimal intensity level of the noise that enabled GDT measurement was 20 dB SPL for white noise and 30 dB SPL for low-frequency noise. Mean GDT values at these intensities were 10.6±3.9 ms and 31.3±4.2 ms, respectively. Bilateral ablation of the primary auditory cortex (complete destruction of the Te1 and partial destruction of the Te2 and Te3 areas) resulted in an increase in GDT values. The fifth day after surgery, the rats were able to detect gaps in the noise. The values of GDT observed at this time were 4.2±1.1 ms for white noise and 7.4±3.1 ms for low-frequency noise at 70 dB SPL. During the first month after cortical ablation, recovery of GDT was observed. However, 1 month after cortical ablation GDT still remained slightly higher than in controls (1.8±0.18 for white noise, 3.22±0.15 for low-frequency noise, P{\textless}0.05). A decrease in GDT values during the subsequent months was not observed.},
author = {Syka, J and Rybalko, N and Mazelov{\'{a}}, J and Druga, R},
doi = {10.1016/S0378-5955(02)00578-6},
issn = {03785955},
journal = {Hearing Research},
number = {1},
pages = {151--159},
title = {{Gap detection threshold in the rat before and after auditory cortex ablation}},
url = {http://www.sciencedirect.com/science/article/pii/S0378595502005786},
volume = {172},
year = {2002}
}
@article{Ohl1999a,
abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
doi = {10.1101/LM.6.4.347},
file = {::},
issn = {1072-0502},
journal = {Learning {\&} memory (Cold Spring Harbor, N.Y.)},
number = {4},
pages = {347--62},
pmid = {10509706},
publisher = {Cold Spring Harbor Laboratory Press},
title = {{Bilateral ablation of auditory cortex in Mongolian gerbil affects discrimination of frequency modulated tones but not of pure tones.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC311295},
volume = {6},
year = {1999}
}
@article{Stilp2010,
abstract = {Speech sounds are traditionally divided into consonants and vowels.$\backslash$r$\backslash$nWhen only vowels or only consonants are replaced by noise, listeners$\backslash$r$\backslash$nare more accurate understanding sentences in which consonants are$\backslash$r$\backslash$nreplaced but vowels remain. From such data, vowels have been$\backslash$r$\backslash$nsuggested to be more important for understanding sentences;$\backslash$r$\backslash$nhowever, such conclusions are mitigated by the fact that replaced$\backslash$r$\backslash$nconsonant segments were roughly one-third shorter than vowels.$\backslash$r$\backslash$nWe report two experiments that demonstrate listener performance$\backslash$r$\backslash$nto be better predicted by simple psychoacoustic measures of cochleascaled$\backslash$r$\backslash$nspectral change across time. First, listeners identified sentences$\backslash$r$\backslash$nin which portions of consonants (C), vowels (V), CV transitions,$\backslash$r$\backslash$nor VC transitions were replaced by noise. Relative intelligibility was$\backslash$r$\backslash$nnot well accounted for on the basis of Cs, Vs, or their transitions. In$\backslash$r$\backslash$na second experiment, distinctions between Cs and Vs were abandoned.$\backslash$r$\backslash$nInstead, portions of sentences were replaced on the basis of$\backslash$r$\backslash$ncochlea-scaled spectral entropy (CSE). Sentence segments having$\backslash$r$\backslash$nrelatively high, medium, or low entropy were replaced with noise.$\backslash$r$\backslash$nIntelligibility decreased linearly as the amount of replaced CSE$\backslash$r$\backslash$nincreased. Duration of signal replaced and proportion of consonants/vowels$\backslash$r$\backslash$nreplaced fail to account for listener data. CSE corresponds$\backslash$r$\backslash$nclosely with the linguistic construct of sonority (or vowellikeness)$\backslash$r$\backslash$nthat is useful for describing phonological systematicity,$\backslash$r$\backslash$nespecially syllable composition. Results challenge traditional distinctions$\backslash$r$\backslash$nbetween consonants and vowels. Speech intelligibility is better$\backslash$r$\backslash$npredicted by nonlinguistic sensory measures of uncertainty (potential$\backslash$r$\backslash$ninformation) than by orthodox physical acoustic measures or$\backslash$r$\backslash$nlinguistic constructs.},
author = {Stilp, Christian E. and Kluender, Keith R.},
doi = {10.1073/pnas.0913625107},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Stilp, Kluender - 2010 - Cochlea-scaled entropy, not consonants, vowels, or time, best predicts speech intelligibility(2).pdf:pdf},
isbn = {0027-8424$\backslash$r1091-6490},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {27},
pages = {12387--12392},
pmid = {20566842},
title = {{Cochlea-scaled entropy, not consonants, vowels, or time, best predicts speech intelligibility}},
volume = {107},
year = {2010}
}
@article{Hickok2007,
author = {Hickok, Gregory and Poeppel, David},
doi = {10.1038/nrn2113},
file = {::},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
month = {may},
number = {5},
pages = {393--402},
publisher = {Nature Publishing Group},
title = {{The cortical organization of speech processing}},
url = {http://www.nature.com/doifinder/10.1038/nrn2113},
volume = {8},
year = {2007}
}
@article{Gaskell1997,
abstract = {We present a new distributed connectionist model of the perception of spoken words. The model employs a representation of speech that combines lexical information with abstract phonological information, with lexical access modelled as a direct mapping onto this single distributed representation. We first examine the integration of partial cues to phonological identity, showing that the model provides a sound basis for simulating phonetic and lexical decision data from Marslen-Wilson and Warren (1994). We then investigate the time course of lexical access, and argue that the process of competition between word candidates during lexical access can be interpreted in terms of interference between distributed lexical representations. The relation between our model and other models of spoken word recognition is discussed.},
author = {Gaskell, M. Gareth and Marslen-Wilson, William D.},
doi = {10.1080/016909697386646},
issn = {0169-0965},
journal = {Language and Cognitive Processes},
month = {oct},
number = {5-6},
pages = {613--656},
publisher = { Taylor {\&} Francis Group },
title = {{Integrating Form and Meaning: A Distributed Model of Speech Perception}},
url = {http://www.tandfonline.com/doi/abs/10.1080/016909697386646},
volume = {12},
year = {1997}
}
@article{Hefner1986,
author = {Hefner, H. E. and Heffner, R. S.},
journal = {Journal of Neurophysiology},
number = {3},
title = {{Effect of unilateral and bilateral auditory cortex lesions on the discrimination of vocalizations by Japanese macaques}},
url = {http://jn.physiology.org/content/56/3/683.short},
volume = {56},
year = {1986}
}
@article{Ohl1999,
abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
doi = {10.1101/LM.6.4.347},
file = {::},
issn = {1072-0502},
journal = {Learning {\&} memory (Cold Spring Harbor, N.Y.)},
number = {4},
pages = {347--62},
pmid = {10509706},
publisher = {Cold Spring Harbor Laboratory Press},
title = {{Bilateral ablation of auditory cortex in Mongolian gerbil affects discrimination of frequency modulated tones but not of pure tones.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC311295},
volume = {6},
year = {1999}
}
@article{Engineer2015,
abstract = {Speech sounds evoke unique neural activity patterns in primary auditory cortex (A1). Extensive speech sound discrimination training alters A1 responses. While the neighboring auditory cortical fields each contain information about speech sound identity, each field processes speech sounds differently. We hypothesized that while all fields would exhibit training-induced plasticity following speech training, there would be unique differences in how each field changes. In this study, rats were trained to discriminate speech sounds by consonant or vowel in quiet and in varying levels of background speech-shaped noise. Local field potential and multiunit responses were recorded from four auditory cortex fields in rats that had received 10 weeks of speech discrimination training. Our results reveal that training alters speech evoked responses in each of the auditory fields tested. The neural response to consonants was significantly stronger in anterior auditory field (AAF) and A1 following speech training. The neural response to vowels following speech training was significantly weaker in ventral auditory field (VAF) and posterior auditory field (PAF). This differential plasticity of consonant and vowel sound responses may result from the greater paired pulse depression, expanded low frequency tuning, reduced frequency selectivity, and lower tone thresholds, which occurred across the four auditory fields. These findings suggest that alterations in the distributed processing of behaviorally relevant sounds may contribute to robust speech discrimination.},
author = {Engineer, Crystal T. and Rahebi, Kimiya C. and Buell, Elizabeth P. and Fink, Melyssa K. and Kilgard, Michael P.},
doi = {10.1016/j.bbr.2015.03.044},
file = {::},
isbn = {1872-7549 (Electronic)
0166-4328 (Linking)},
issn = {18727549},
journal = {Behavioural Brain Research},
keywords = {Auditory processing,Map reorganization,Receptive field plasticity,Speech therapy},
pages = {256--264},
pmid = {25827927},
publisher = {Elsevier B.V.},
title = {{Speech training alters consonant and vowel responses in multiple auditory cortex fields}},
url = {http://dx.doi.org/10.1016/j.bbr.2015.03.044},
volume = {287},
year = {2015}
}
@article{Ohl2001,
author = {Ohl, F. W. and Scheich, H. and Freeman, W. J.},
doi = {10.1038/35089076},
issn = {0028-0836},
journal = {Nature},
month = {aug},
number = {6848},
pages = {733--736},
publisher = {Nature Publishing Group},
title = {{Change in pattern of ongoing cortical activity with auditory category learning}},
url = {http://www.nature.com/doifinder/10.1038/35089076},
volume = {412},
year = {2001}
}
@article{Mesgarani2014,
abstract = {During speech perception, linguistic elements such as consonants and vowels are extracted from a complex acoustic speech signal. The superior temporal gyrus (STG) participates in high-order auditory processing of speech, but how it encodes phonetic information is poorly understood. We used high-density direct cortical surface recordings in humans while they listened to natural, continuous speech to reveal the STG representation of the entire English phonetic inventory. At single electrodes, we found response selectivity to distinct phonetic features. Encoding of acoustic properties was mediated by a distributed population response. Phonetic features could be directly related to tuning for spectrotemporal acoustic cues, some of which were encoded in a nonlinear fashion or by integration of multiple cues. These findings demonstrate the acoustic-phonetic representation of speech in human STG.},
author = {Mesgarani, Nima and Cheung, Connie and Johnson, Keith and Chang, Edward F},
doi = {10.1126/science.1245994},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Mesgarani et al. - 2014 - Phonetic feature encoding in human superior temporal gyrus(2).pdf:pdf},
isbn = {0036-8075},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Auditory Cortex,Auditory Cortex: anatomy {\&} histology,Auditory Cortex: physiology,Female,Humans,Magnetic Resonance Imaging,Male,Phonetics,Speech Acoustics,Speech Perception},
number = {6174},
pages = {1006--10},
pmid = {24482117},
title = {{Phonetic feature encoding in human superior temporal gyrus.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4350233{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {343},
year = {2014}
}
@article{Rutishauser2015,
abstract = {Previous explanations of computations performed by recurrent networks have focused on symmetrically connected saturating neurons and their convergence toward attractors. Here we analyze the behavior of asymmetrical connected networks of linear threshold neurons, whose positive response is unbounded. We show that, for a wide range of parameters, this asymmetry brings interesting and computationally useful dynamical properties. When driven by input, the network explores potential solutions through highly unstable 'expansion' dynamics. This expansion is steered and constrained by negative divergence of the dynamics, which ensures that the dimensionality of the solution space continues to reduce until an acceptable solution manifold is reached. Then the system contracts stably on this manifold towards its final solution trajectory. The unstable positive feedback and cross inhibition that underlie expansion and divergence are common motifs in molecular and neuronal networks. Therefore we propose that very simple organizational constraints that combine these motifs can lead to spontaneous computation and so to the spontaneous modification of entropy that is characteristic of living systems.},
author = {Rutishauser, Ueli and Slotine, Jean Jacques and Douglas, Rodney},
doi = {10.1371/journal.pcbi.1004039},
file = {::},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {1},
pages = {e1004039},
pmid = {25617645},
title = {{Computation in Dynamically Bounded Asymmetric Systems}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1004039},
volume = {11},
year = {2015}
}
@article{Bailey1980,
abstract = {A series of experiments is reported that investigated the pattern of acoustic information specifying place and manner of stop consonants in medial position after [s]. In both production and perception, information for stop place includes the spectrum of the fricative at offset, the duration of the silent closure interval, the spectral relationship between the frequency of the stop release burst and the following periodically excited formants, and the spectral and temporal characteristics of the first formant transition. Similarly, the information for stop manner includes the duration of silent closure, the frequency of the first formant at the release, the magnitude of the first formant transition, and the proximity of the second and third formants at release. A relationship was shown to exist in perception between the spectral characteristics of the first formant and the duration of the silent closure required to hear a stop. This appears to reciprocate the covariation of these parameters in production across different places of articulation and different vocalic contexts. The existence of perceptual sensitivity to a wide range of the acoustic consequences of production questions the efficacy of accounts of speech perception in terms of the fractionation of the signal into elemental acoustic cues, which are then integrated to yield a phonetic percept. It is argued that it is inappropriate to ascribe a psychological status to cues whose only reality is their operational role as physical parameters whose manipulation can change the phenotic interpretation of a signal. It is suggested that the metric of the information for phonetic perception cannot be that of the cues; rather, a metric should be sought in which acoustic and articulatory dynamics are isomorphic.},
author = {Bailey, P J and Summerfield, Q},
doi = {10.1037/0096-1523.6.3.536},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Bailey, Summerfield - 1980 - Information in speech observations on the perception of s-stop clusters(2).pdf:pdf},
isbn = {0096-1523},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
keywords = {Humans,Phonetics,Psychoacoustics,Speech Percept},
month = {aug},
number = {3},
pages = {536--563},
pmid = {6447767},
title = {{Information in speech: observations on the perception of [s]-stop clusters}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6447767},
volume = {6},
year = {1980}
}
@article{Liberman1985,
abstract = {A motor theory of speech perception, initially proposed to account for results of early experiments with synthetic speech, is now extensively revised to accommodate recent findings, and to relate the assumptions of the theory to those that might be made about other perceptual modes. According to the revised theory, phonetic information is perceived in a biologically distinct system, a 'module' specialized to detect the intended gestures of the speaker that are the basis for phonetic categories. Built into the structure of this module is the unique but lawful relationship between the gestures and the acoustic patterns in which they are variously overlapped. In consequence, the module causes perception of phonetic structure without translation from preliminary auditory impressions. Thus, it is comparable to such other modules as the one that enables an animal to localize sound. Peculiar to the phonetic module are the relation between perception and production it incorporates and the fact that it must compete with other modules for the same stimulus variations. ?? 1985.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Liberman, Alvin M. and Mattingly, Ignatius G.},
doi = {10.1016/0010-0277(85)90021-6},
eprint = {NIHMS150003},
isbn = {0010-0277, 0010-0277},
issn = {00100277},
journal = {Cognition},
month = {oct},
number = {1},
pages = {1--36},
pmid = {4075760},
title = {{The motor theory of speech perception revised}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/4075760},
volume = {21},
year = {1985}
}
@article{Kosem2016,
author = {K{\"{o}}sem, Anne and Basirat, Anahita and Azizi, Leila and van Wassenhove, Virginie},
doi = {10.1152/jn.00074.2016},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
number = {6},
pages = {2497--2512},
title = {{High-frequency neural activity predicts word parsing in ambiguous speech streams}},
url = {http://jn.physiology.org/lookup/doi/10.1152/jn.00074.2016},
volume = {116},
year = {2016}
}
@article{Ohala1996,
abstract = {Three types of evidence are reviewed which cast doubt on claims that recovery of the speaker's articulations is an inherent part of speech perception: (a) Phonological data (e.g., universal tendencies of languages' segment inventories, phonotactic patterns, sound changes, etc.) show unmistakably that the acoustic-auditory properties of speech sounds, not their articulations, are the primary determinant of their behavior. (b) Infants and various nonhuman species can differentiate certain sound contrasts in human speech even though it is highly unlikely that they can deduce the vocal tract movements generating the sounds. (c) Humans can differentiate many nonspeech sounds almost as complex as speech, e.g., music, machine noises, as well as bird and monkey vocalizations, where there is little or no possibility of recovering the mechanisms producing the sounds.},
author = {Ohala, J J},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {mar},
number = {3},
pages = {1718--25},
pmid = {8819861},
title = {{Speech perception is hearing sounds, not tongues.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8819861},
volume = {99},
year = {1996}
}
@article{Bub2001,
abstract = {It is generally accepted, following Landauer and Bennett, that the process of measurement involves no minimum entropy cost, but the erasure of information in resetting the memory register of a computer to zero requires dissipating heat into the environment. This thesis has been challenged recently in a two-part article by Earman and Norton. I review some relevant observations in the thermodynamics of computation and argue that Earman and Norton are mistaken: there is in principle no entropy cost to the acquisition of information, but the destruction of information does involve an irreducible entropy cost. ?? 2001 Elsevier Science Ltd.},
archivePrefix = {arXiv},
arxivId = {quant-ph/0203017},
author = {Bub, Jeffrey},
doi = {10.1016/S1355-2198(01)00023-5},
eprint = {0203017},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Bub - 2001 - Maxwell's Demon and the Thermodynamics of Computation(2).pdf:pdf},
isbn = {1355-2198},
issn = {13552198},
journal = {Studies in History and Philosophy of Science Part B - Studies in History and Philosophy of Modern Physics},
keywords = {Information,Maxwell's Demon,Measurement,Thermodynamics of Computation},
number = {4},
pages = {569--579},
primaryClass = {quant-ph},
title = {{Maxwell's Demon and the Thermodynamics of Computation}},
volume = {32},
year = {2001}
}
@article{Idemaru2011,
author = {Idemaru, Kaori and Holt, Lori L.},
doi = {10.1037/a0025641},
file = {::},
issn = {1939-1277},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
keywords = {dimension-based learning,perceptual learning,speech perception,statistical learning,talker adaptation,word recognition},
number = {6},
pages = {1939--1956},
publisher = {American Psychological Association},
title = {{Word recognition reflects dimension-based statistical learning.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0025641},
volume = {37},
year = {2011}
}
@incollection{Kluender2013a,
abstract = {Predicated upon principles of information theory, efficient coding has proven valuable for understanding visual perception. Here, we illustrate how efficient coding provides a powerful explanatory framework for understanding speech perception. This framework dissolves debates about objects of perception, instead focusing on the objective of perception: optimizing information transmis-sion between the environment and perceivers. A simple measure of physiologically significant information is shown to predict intelligibility of variable-rate speech and discriminability of vowel sounds. Reliable covariance between acoustic attributes in complex sounds, both speech and nonspeech, is demonstrated to be amply available in natural sounds and efficiently coded by listeners. An efficient coding framework provides a productive approach to answer questions concerning perception of vowel sounds (including vowel inherent spectral change), perception of speech, and perception most broadly.},
annote = {From Duplicate 1 (The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception - Bartlett, Edward L.)

From Duplicate 1 (The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception. - Bartlett, Edward L.)

10.1016/j.bandl.2013.03.003},
author = {Kluender, Keith R and Stilp, Christian E and Kiefte, Michael and Kluender, K R and Stilp, C E and Kiefte, M},
booktitle = {Vowel Inherent Spectral Change},
doi = {10.1007/978-3-642-14209-3_6},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Kluender et al. - 2013 - Perception of Vowel Sounds Within a Biologically Realistic Model of Efficient Coding(2).pdf:pdf},
isbn = {9783642142093},
pages = {117--151},
title = {{Perception of Vowel Sounds Within a Biologically Realistic Model of Efficient Coding}},
year = {2013}
}
@inproceedings{JiaDeng2009,
author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206848},
file = {::},
isbn = {978-1-4244-3992-8},
month = {jun},
pages = {248--255},
publisher = {IEEE},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://ieeexplore.ieee.org/document/5206848/},
year = {2009}
}
@misc{Maechler2017,
author = {Maechler, Martin and Rousseeuw, Peter and Struyf, Anja and Hubert, Mia and Hornik, Kurt},
title = {{cluster: Cluster Analysis Basics and Extensions}},
year = {2017}
}
@article{Stevens,
abstract = {The consonantal segments that underlie an utterance are manifested in the acoustic signal by abrupt discontinuities or dislocations in the spectral pattern. There are potentially two such discontinuities for each consonant, corresponding to the formation and release of a constriction in the oral cavity by the lips, the tongue blade, or the tongue body. Acoustic cues for the various consonant features of place, voicing and nasality reside in the signal in quite different forms on the two sides of each acoustic discontinuity. Examples of these diverse cues and their origin in acoustic theory are reviewed, with special attention to place features and features related to the laryngeal state and to nasalization. A listener appears to have the ability to integrate these diverse, brief acoustic cues for the features of consonants, although the mechanism for this integration process is unclear.},
author = {Stevens, K N},
doi = {28468},
issn = {0031-8388},
journal = {Phonetica},
number = {2-4},
pages = {139--51},
pmid = {10992135},
title = {{Diverse acoustic cues at consonantal landmarks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10992135},
volume = {57}
}
@article{Clevert2015,
abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10{\%} classification error for a single crop, single model network.},
archivePrefix = {arXiv},
arxivId = {1511.07289},
author = {Clevert, Djork-Arn{\'{e}} and Unterthiner, Thomas and Hochreiter, Sepp},
eprint = {1511.07289},
file = {::},
month = {nov},
title = {{Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}},
url = {http://arxiv.org/abs/1511.07289},
year = {2015}
}
@article{Kleinschmidt2016,
abstract = {When a listener hears many good examples of a /b/ in a row, they are less likely to classify other sounds on, e.g., a /b/-to-/d/ continuum as /b/. This phenomenon is known as selective adaptation and is a well-studied property of speech perception. Traditionally, selective adaptation is seen as a mechanistic property of the speech perception system, and attributed to fatigue in acoustic-phonetic feature detectors. However, recent developments in our understanding of non-linguistic sensory adaptation and higher-level adaptive plasticity in speech perception and language comprehension suggest that it is time to re-visit the phenomenon of selective adaptation. We argue that selective adaptation is better thought of as a computational property of the speech perception system. Drawing on a common thread in recent work on both non-linguistic sensory adaptation and plasticity in language comprehension, we furthermore propose that selective adaptation can be seen as a consequence of distributional learning across multiple levels of representation. This proposal opens up new questions for research on selective adaptation itself, and also suggests that selective adaptation can be an important bridge between work on adaptation in low-level sensory systems and the complicated plasticity of the adult language comprehension system.},
author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
doi = {10.3758/s13423-015-0943-z},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
keywords = {Computational models,Perceptual learning,Speech perception,Statistical inference},
month = {jun},
number = {3},
pages = {678--691},
pmid = {26438255},
title = {{Re-examining selective adaptation: Fatiguing feature detectors, or distributional learning?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26438255 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4821823 http://link.springer.com/10.3758/s13423-015-0943-z},
volume = {23},
year = {2016}
}
@article{Moczulska2013,
abstract = {Long-lasting changes in synaptic connections induced by relevant experiences are believed to represent the physical correlate of memories. Here, we combined chronic in vivo two-photon imaging of dendritic spines with auditory-cued classical conditioning to test if the formation of a fear memory is associated with structural changes of synapses in the mouse auditory cortex. We find that paired conditioning and unpaired conditioning induce a transient increase in spine formation or spine elimination, respectively. A fraction of spines formed during paired conditioning persists and leaves a long-lasting trace in the network. Memory recall triggered by the reexposure of mice to the sound cue did not lead to changes in spine dynamics. Our findings provide a synaptic mechanism for plasticity in sound responses of auditory cortex neurons induced by auditory-cued fear conditioning; they also show that retrieval of an auditory fear memory does not lead to a recapitulation of structural plasticity in the auditory cortex as observed during initial memory consolidation.},
author = {Moczulska, Kaja Ewa and Tinter-Thiede, Juliane and Peter, Manuel and Ushakova, Lyubov and Wernle, Tanja and Bathellier, Brice and Rumpel, Simon},
doi = {10.1073/pnas.1312508110},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Moczulska et al. - 2013 - Dynamics of dendritic spines in the mouse auditory cortex during memory formation and memory recall(3).pdf:pdf;:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Moczulska et al. - 2013 - Dynamics of dendritic spines in the mouse auditory cortex during memory formation and memory recall(4).pdf:pdf},
isbn = {1091-6490 (Electronic)$\backslash$r0027-8424 (Linking)},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {auditory fear conditioning,learning,reconsolidation},
number = {45},
pages = {18315--20},
pmid = {24151334},
title = {{Dynamics of dendritic spines in the mouse auditory cortex during memory formation and memory recall.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24151334{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3831433},
volume = {110},
year = {2013}
}
@misc{Lisker1977,
abstract = {InAmerican English, initial /bdg/ often lack the acoustic feature takenas the defining feature of voiced stops; intervocalically before unstressedvowel /ptk/ lack aspiration, without which initial stops are notlabeled /ptk/. Initially, the two categories differ in the timingof vocal fold adduction and onset of fold vibration, andseveral acoustic cues, all tied to the VOT difference, havebeen studied. Medially there is also a difference in themanagement of the larynx, though it results in a phoneticallysimpler contrast, one of voicing with no accompanying aspiration difference.Acoustically, however, the list of features that play, or mightplausibly play a role is quite large. The word pairrapid-rabid, for example, might be affected by the following: (1)presence/absence of low-frequency buzz during the closure interval, (2) durationof closure, (3) F1 offset frequency before closure, (4) F1offset transition duration, (5) F1 onset frequency following closure, (6)F1 onset transition duration, (7) {\ae} duration, (8) F1 cut-backbefore closure, (9) F1 cutback following closure, (10) VOT cutbackbefore closure, (11) VOT delay after closure, (12) F0 contourbefore closure, (13) F0 contour after closure, (14) amplitude ofi relative to {\ae}, (15) decay time of glottal signalpreceding closure, (16) intensity of burst following closure. Even ifsome of these should turn out to be perceptually negligible,enough of them surely have cue value to make ita formidable task to justify preferring an acoustic to anarticulatory account of the distinction between the two English words.The support of the National Institute of Child Health andHuman Development is gratefully acknowledged. 1977 Acoustical Society of America},
author = {Lisker, Leigh},
booktitle = {The Journal of the Acoustical Society of America},
doi = {10.1121/1.2016377},
isbn = {0001-4966},
issn = {00014966},
number = {S1},
pages = {S77},
title = {{Rapid versus rabid: A catalogue of acoustic features that may cue the distinction}},
volume = {62},
year = {1977}
}
@article{Centanni2013,
abstract = {We have developed a classifier capable of locating and identifying speech sounds using activity from rat auditory cortex with an accuracy equivalent to behavioral performance and without the need to specify the onset time of the speech sounds. This classifier can identify speech sounds from a large speech set within 40. ms of stimulus presentation. To compare the temporal limits of the classifier to behavior, we developed a novel task that requires rats to identify individual consonant sounds from a stream of distracter consonants. The classifier successfully predicted the ability of rats to accurately identify speech sounds for syllable presentation rates up to 10 syllables per second (up to 17.9 ± 1.5 bits/s), which is comparable to human performance. Our results demonstrate that the spatiotemporal patterns generated in primary auditory cortex can be used to quickly and accurately identify consonant sounds from a continuous speech stream without prior knowledge of the stimulus onset times. Improved understanding of the neural mechanisms that support robust speech processing in difficult listening conditions could improve the identification and treatment of a variety of speech-processing disorders. {\textcopyright} 2013 IBRO.},
author = {Centanni, T. M. and Sloan, A. M. and Reed, A. C. and Engineer, C. T. and Rennaker, R. L. and Kilgard, M. P.},
doi = {10.1016/j.neuroscience.2013.11.030},
file = {::},
isbn = {0306-4522},
issn = {03064522},
journal = {Neuroscience},
keywords = {Auditory cortex,Classifier,Coding,Rat,Temporal patterns},
pages = {292--306},
pmid = {24286757},
publisher = {IBRO},
title = {{Detection and identification of speech sounds using cortical activity patterns}},
url = {http://dx.doi.org/10.1016/j.neuroscience.2013.11.030},
volume = {258},
year = {2013}
}
@article{Theunissen2014,
abstract = {We might be forced to listen to a high-frequency tone at our audiologist's office or we might enjoy falling asleep with a white-noise machine, but the sounds that really matter to us are the voices of our companions or music from our favourite radio station. The auditory system has evolved to process behaviourally relevant natural sounds. Research has shown not only that our brain is optimized for natural hearing tasks but also that using natural sounds to probe the auditory system is the best way to understand the neural computations that enable us to comprehend speech or appreciate music.},
author = {Theunissen, Fr{\'{e}}d{\'{e}}ric E and Elie, Julie E},
doi = {10.1038/nrn3731},
isbn = {1471-0048 (Electronic)$\backslash$r1471-003X (Linking)},
issn = {1471-0048},
journal = {Nature reviews. Neuroscience},
keywords = {Acoustic Stimulation,Animals,Auditory Pathways,Auditory Pathways: physiology,Auditory Perception,Auditory Perception: physiology,Brain Mapping,Hearing,Hearing: physiology,Humans,Music,Sound},
number = {6},
pages = {355--66},
pmid = {24840800},
title = {{Neural processing of natural sounds.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24840800},
volume = {15},
year = {2014}
}
@article{Carbonell2014,
author = {Carbonell, Kathy M. and Lotto, Andrew J.},
doi = {10.3389/fpsyg.2014.00427},
file = {::},
issn = {1664-1078},
journal = {Frontiers in psychology},
keywords = {as is apparent from,auditory processing,motor theory,multisensory i,multisensory integration,of nearly any research,of speech,or review article,reading the first line,sensorimotor effects on perception,specialness,speech perception,the},
month = {jun},
number = {June},
pages = {427},
pmid = {24917830},
publisher = {Frontiers},
title = {{Speech is not special{\ldots} again.}},
url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00427/abstract http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4042079{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {5},
year = {2014}
}
@misc{Brenowitz1997,
abstract = {This special issue of the Journal of Neurobiology is devoted to a consideration of the avian song control system. In the 20 years that have passed since Nottebohm et al. 1976) first identified forebrain circuits that control song in birds, the song system has emerged as a leading model in behavioral neuroscience. To mark the beginning of the third decade of study of this model, we invited several leading investigators to contribute to this volume. We set two goals for the authors: to review progress in their area of study and, more important, to identify critical directions for future research. The modern study of birdsong began with the work of William Thorpe (1958, 1961). He showed that chaffinches (Fringilla coelebs) collected as nestlings and reared in the laboratory in isolation from conspecific adult males produced very abnormal songs. If these young birds were exposed to tape recordings of wild chaffinch songs, however, they eventually produced normal songs that matched those heard on the recordings. These studies demonstrated for the first time that young birds must learn the song of their species by listening to adult conspecifics. Thorpe's student Peter Marler greatly expanded upon this early work. Marler and his colleagues demonstrated the existence of local geographic song "dialects," that song learning is characterized by early sensitive periods, and that birds have innate predispositions to learn the song of their species. Masakazu Konishi, while a student with Marler, showed that birds must be able to hear themselves sing to develop song normally. Fernando Nottebohm, also while a student with Marler, showed that the peripheral control of song production is lateralized. Nottebohm and his colleagues subsequently identified neural circuits in the avian forebrain that control song behavior. This important discovery paved the way for many investigators who have subsequently contributed to our understanding of song behavior and its neural control. The birdsong system offers several advantages as a model for identifying neural mechanisms that underlie biologically relevant behavior: 1. Song is a learned behavior that is controlled by discrete neural circuits. 2. There are distinct phases in the development of song, with well-defined sensitive periods. One can relate the ontogeny of song behavior to the development of the underlying neural circuits. 3. Song is the product of stereotyped motor programs, with hierarchical organization of the premotor and motor nuclei. 4. Song behavior and the associated neural circuits are sexually dimorphic in most species. 5. Gonadal steroid hormones have pronounced effects on the development and adult function of the song control circuits, as well as on song behavior. 6. There is extensive plasticity of the adult song system, including ongoing neurogenesis and seasonal changes in morphology. 7. There is pronounced species diversity in different aspects of song behavior, including the timing of vocal learning, sex patterns of song production, number of songs that are learned, and seasonality of song behavior. This diversity provides opportunities for comparative studies of the song control system.},
author = {Brenowitz, E. A. and Margoliash, D. and Nordeen, K. W.},
booktitle = {Journal of Neurobiology},
isbn = {0022-3034},
issn = {00223034},
number = {5},
pages = {495--500},
pmid = {9369455},
title = {{An introduction to birdsong and the avian song system}},
volume = {33},
year = {1997}
}
@article{Fritz2003,
abstract = {Listening is an active process in which attentive focus on salient acoustic features in auditory tasks can influence receptive field properties of cortical neurons. Recent studies showing rapid task-related changes in neuronal spectrotemporal receptive fields (STRFs) in primary auditory cortex of the behaving ferret are reviewed in the context of current research on cortical plasticity. Ferrets were trained on spectral tasks, including tone detection and two-tone discrimination, and on temporal tasks, including gap detection and click-rate discrimination. STRF changes could be measured on-line during task performance and occurred within minutes of task onset. During spectral tasks, there were specific spectral changes (enhanced response to tonal target frequency in tone detection and discrimination, suppressed response to tonal reference frequency in tone discrimination). However, only in the temporal tasks, the STRF was changed along the temporal dimension by sharpening temporal dynamics. In ferrets trained on multiple tasks, distinctive and task-specific STRF changes could be observed in the same cortical neurons in successive behavioral sessions. These results suggest that rapid task-related plasticity is an ongoing process that occurs at a network and single unit level as the animal switches between different tasks and dynamically adapts cortical STRFs in response to changing acoustic demands. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Fritz, Jonathan and Elhilali, Mounya and Shamma, Shihab},
doi = {10.1016/j.heares.2005.01.015},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Fritz, Elhilali, Shamma - 2005 - Active listening Task-dependent plasticity of spectrotemporal receptive fields in primary auditory c(2).pdf:pdf},
isbn = {0378-5955 (Print)$\backslash$r0378-5955 (Linking)},
issn = {03785955},
journal = {Hearing Research},
keywords = {Adaptive,Attention,Auditory,Behavior,Cortex,Plasticity},
number = {1-2},
pages = {159--176},
pmid = {16081006},
title = {{Active listening: Task-dependent plasticity of spectrotemporal receptive fields in primary auditory cortex}},
url = {http://www.nature.com/doifinder/10.1038/nn1141},
volume = {206},
year = {2005}
}
@inproceedings{Schouten2003,
abstract = {Comparing phoneme classification and discrimination (or "categorical perception") of a stimulus continuum has for a long time been regarded as a useful method for investigating the storage and retrieval of phoneme categories in long-term memory. The closeness of the relationship between the two tasks, i.e. the degree of categorical perception, depends on a number of factors, some of which are unknown or random. One very important factor, however, seems to be the degree of bias (in the signal-detection sense of the term) in the discrimination task. When the task is such (as it is in 2IFC, for example) that the listener has to rely heavily on an internal, subjective, criterion, discrimination can seem to be almost perfectly categorical, if the stimuli are natural enough. Presenting the same stimuli in a much less biasing task, however, leads to discrimination results that are completely unrelated to phoneme classification. Even the otherwise ubiquitous peak at the phoneme boundary has disappeared. The traditional categorical-perception experiment measures the bias inherent in the discrimination task; if we want to know how speech sounds are categorized, we will have to look elsewhere. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Schouten, Bert and Gerrits, Ellen and {Van Hessen}, Arjan},
booktitle = {Speech Communication},
doi = {10.1016/S0167-6393(02)00094-8},
isbn = {0167-6393},
issn = {01676393},
keywords = {Categorical perception},
number = {1},
pages = {71--80},
title = {{The end of categorical perception as we know it}},
volume = {41},
year = {2003}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org/},
year = {2016}
}
@article{Hillenbrand1994,
abstract = {This study was designed as a replication and extension of the classic study of vowel acoustics by Peterson and Barney (PB) [J. Acoust. Soc. Am. 24, 175–184 (1952)]. Recordings were made of 50 men, 50 women, and 50 children producing the vowels /i, i, eh, {\ae}, hooked backward eh, inverted vee), a, open oh, u, u/ in h–V–d syllables. Formant contours for F1–F4 were measured from LPC spectra using a custom interactive editing tool. For comparison with the PB data, formant patterns were sampled at a time that was judged by visual inspection to be maximally steady. Preliminary analysis shows numerous differences between the present data and those of PB, both in terms of average formant frequencies for vowels, and the degree of overlap among adjacent vowels. As with the original study, listening tests showed that the signals were nearly always identified as the vowel intended by the talker.},
author = {Hillenbrand, James and Getty, Laura A. and Wheeler, Kimberlee and Clark, Michael J.},
doi = {10.1121/1.409456},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {COMPARATIVE EVALUATIONS,PERFORMANCE TESTING,SPEECH RECOGNITION,VOWELS},
month = {may},
number = {5},
pages = {2875--2875},
publisher = {Acoustical Society of America},
title = {{Acoustic characteristics of American English vowels}},
url = {http://asa.scitation.org/doi/10.1121/1.409456},
volume = {95},
year = {1994}
}
@article{Lotto1997,
author = {Lotto, AJ and Kluender, KR and Holt, LL},
journal = {Chicago Linguistic Society},
title = {{Animal models of speech perception phenomena}},
url = {https://www.researchgate.net/profile/Keith{\_}Kluender/publication/237280984{\_}(from{\_}K.{\_}Singer{\_}R.{\_}Eggert{\_}{\_}G.{\_}Anderson{\_}(Eds.){\_}Chicago{\_}Linguistic{\_}Society{\_}Volume{\_}33{\_}(Chicago{\_}Linguistic{\_}Society{\_}Chicago).{\_}pp.{\_}357-367{\_}(1997).){\_}Animal{\_}models{\_}of{\_}speech{\_}perception{\_}phen},
year = {1997}
}
@article{Weiss2001,
abstract = {In this article we analyze the effect of class distribution on classifier learning. We begin by describing the different ways in which class distribution affects learning and how it affects the evaluation of learned classifiers. We then present the results of two comprehensive experimental studies. The first study compares the performance of classifiers generated from unbalanced data sets with the performance of classifiers generated from balanced versions of the same data sets. This...},
author = {Weiss, Gm and Provost, Foster},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Weiss, Provost - 2001 - The effect of class distribution on classifier learning an empirical study(2).pdf:pdf},
journal = {Rutgers Univ},
title = {{The effect of class distribution on classifier learning: an empirical study}},
url = {ftp://ftp.cs.rutgers.edu/http/cs/cs/pub/technical-reports/work/ml-tr-44.pdf},
year = {2001}
}
@article{Kleinschmidt2016a,
author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
doi = {10.3758/s13423-015-0943-z},
file = {::},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
month = {jun},
number = {3},
pages = {678--691},
publisher = {Springer US},
title = {{Re-examining selective adaptation: Fatiguing feature detectors, or distributional learning?}},
url = {http://link.springer.com/10.3758/s13423-015-0943-z},
volume = {23},
year = {2016}
}
@article{Estes2015,
abstract = {To learn from their environments, infants must detect structure behind pervasive variation. This presents substantial and largely untested learning challenges in early language acquisition. The current experiments address whether infants can use statistical learning mechanisms to segment words when the speech signal contains acoustic variation produced by changes in speakers' voices. In Experiment 1, 8- and 10-month-old infants listened to a continuous stream of novel words produced by 8 different female voices. The voices alternated frequently, potentially interrupting infants' detection of transitional probability patterns that mark word boundaries. Infants at both ages successfully segmented words in the speech stream. In Experiment 2, 8-month-olds demonstrated the ability to generalize their learning about the speech stream when presented with a new, acoustically distinct voice during testing. However, in Experiments 3 and 4, when the same speech stream was produced by only 2 female voices, infants failed to segment the words. The results of these experiments indicate that low acoustic variation may interfere with infants' efficiency in segmenting words from continuous speech, but that infants successfully use statistical cues to segment words in conditions of high acoustic variation. These findings contribute to our understanding of whether statistical learning mechanisms can scale up to meet the demands of natural learning environments.},
author = {Estes, Katharine Graf and Lew-Williams, Casey},
doi = {10.1037/a0039725},
issn = {1939-0599},
journal = {Developmental Psychology},
month = {nov},
number = {11},
pages = {1517--1528},
pmid = {26389607},
title = {{Listening through voices: Infant statistical word segmentation across multiple speakers.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26389607 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4631842 http://doi.apa.org/getdoi.cfm?doi=10.1037/a0039725},
volume = {51},
year = {2015}
}
@article{Kluender2000,
abstract = {Broadly speaking, nonhuman animal models contribute to understanding speech perception by humans in two ways—by analogy and by homology. The former is generally easier and examples are more abundant. Because demonstrating homology requires deeper explication of underlying mechanisms, claims can be more precarious but carry potentially greater explanatory payoff. When studying nonhuman organisms as an analogy, the emphasis is typically upon how animal physiological or behavioral processes have adapted to fulfill requirements of particular ecological niches. By contrast, study of animals as homology often violates ecology in search of common underlying processes, and the animal becomes a method more than an object of study. Examples of findings for animal analogies and homologies will be reviewed. Data will be presented from experiments in which nonhuman subjects play the role of homology in revealing both foundational sensory processes and more plastic processes of perceptual development. Animal models pro...},
author = {Kluender, Keith R.},
doi = {10.1121/1.429153},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
month = {may},
number = {5},
pages = {2835--2835},
publisher = {Acoustical Society of AmericaASA},
title = {{Contributions of nonhuman animal models to understanding human speech perception}},
url = {http://asa.scitation.org/doi/10.1121/1.429153},
volume = {107},
year = {2000}
}
@misc{Sundar2014,
abstract = {Constructs confidence intervals on the probability of success in a binomial experiment via several parameterizations},
author = {Sundar, Dorai-Raj},
pages = {R package version 1.1--1},
title = {{binom: Binomial Confidence Intervals For Several Parameterizations}},
year = {2014}
}
@article{Eimas1973,
abstract = {Using a selective adaptation procedure, evidence was obtained for the existence of linguistic feature detectors, analogous to visual feature detectors. These detectors are each sensitive to a restricted range of voice onset times, the physical continuum underlying the perceived phonetic distinctions between voiced and voiceless stop consonants. The sensitivity of a particular detector can be reduced selectively by repetitive presentation of its adequate stimulus. This results in a shift in the locus of the phonetic boundary separating the voiced and voiceless stops.},
author = {Eimas, Peter D. and Corbit, John D.},
doi = {10.1016/0010-0285(73)90006-6},
issn = {00100285},
journal = {Cognitive Psychology},
number = {1},
pages = {99--109},
title = {{Selective adaptation of linguistic feature detectors}},
url = {http://www.sciencedirect.com/science/article/pii/0010028573900066},
volume = {4},
year = {1973}
}
@article{Pedregosa2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Froemke2007,
abstract = {Receptive fields of sensory cortical neurons are plastic, changing in response to alterations of neural activity or sensory experience. In this way, cortical representations of the sensory environment can incorporate new information about the world, depending on the relevance or value of particular stimuli. Neuromodulation is required for cortical plasticity, but it is uncertain how subcortical neuromodulatory systems, such as the cholinergic nucleus basalis, interact with and refine cortical circuits. Here we determine the dynamics of synaptic receptive field plasticity in the adult primary auditory cortex (also known as AI) using in vivo whole-cell recording. Pairing sensory stimulation with nucleus basalis activation shifted the preferred stimuli of cortical neurons by inducing a rapid reduction of synaptic inhibition within seconds, which was followed by a large increase in excitation, both specific to the paired stimulus. Although nucleus basalis was stimulated only for a few minutes, reorganization of synaptic tuning curves progressed for hours thereafter: inhibition slowly increased in an activity-dependent manner to rebalance the persistent enhancement of excitation, leading to a retuned receptive field with new preference for the paired stimulus. This restricted period of disinhibition may be a fundamental mechanism for receptive field plasticity, and could serve as a memory trace for stimuli or episodes that have acquired new behavioural significance.},
author = {Froemke, Robert C and Merzenich, Michael M and Schreiner, Christoph E},
doi = {10.1038/nature06289},
file = {::},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
keywords = {Afferent,Animals,Auditory Cortex,Excitatory Postsynaptic Pote,Female,Memory,Neuronal Plasticity,Neurons,Rats,Sprague-Dawley,Synapses,Time Factors,cytology/physiology,metabolism,physiology},
number = {7168},
pages = {425--429},
pmid = {18004384},
title = {{A synaptic memory trace for cortical receptive field plasticity.}},
url = {http://dx.doi.org/10.1038/nature06289},
volume = {450},
year = {2007}
}
@article{Holt2001,
abstract = {For stimuli modeling stop consonants varying in the acoustic correlates of voice onset time (VOT), human listeners are more likely to perceive stimuli with lower f0's as voiced consonants—a pattern of perception that follows regularities in English speech production. The present study examines the basis of this observation. One hypothesis is that lower f0's enhance perception of voiced stops by virtue of perceptual interactions that arise from the operating characteristics of the auditory system. A second hypothesis is that this perceptual pattern develops as a result of experience with f0-voicing covariation. In a test of these hypotheses, Japanese quail learned to respond to stimuli drawn from a series varying in VOT through training with one of three patterns of f0-voicing covariation. Voicing and f0 varied in the natural pattern (shorter VOT, lower f0), in an inverse pattern (shorter VOT, higher f0), or in a random pattern (no f0-voicing covariation). Birds trained with stimuli that had no f0-voicing ...},
author = {Holt, Lori L. and Lotto, Andrew J. and Kluender, Keith R.},
doi = {10.1121/1.1339825},
file = {::},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {speech intelligibility},
month = {feb},
number = {2},
pages = {764--774},
publisher = {Acoustical Society of AmericaASA},
title = {{Influence of fundamental frequency on stop-consonant voicing perception: A case of learned covariation or auditory enhancement?}},
url = {http://asa.scitation.org/doi/10.1121/1.1339825},
volume = {109},
year = {2001}
}
@article{Fitch2000,
abstract = {The evolution of speech can be studied independently of the evolution of language, with the advantage that most aspects of speech acoustics, physiology and neural control are shared with animals, and thus open to empirical investigation. At least two changes were necessary prerequisites for modern human speech abilities: (1) modification of vocal tract morphology, and (2) development of vocal imitative ability. Despite an extensive literature, attempts to pinpoint the timing of these changes using fossil data have proven inconclusive. However, recent comparative data from nonhuman primates have shed light on the ancestral use of formants (a crucial cue in human speech) to identify individuals and gauge body size. Second, comparative analysis of the diverse vertebrates that have evolved vocal imitation (humans, cetaceans, seals and birds) provides several distinct, testable hypotheses about the adaptive function of vocal mimicry. These developments suggest that, for understanding the evolution of speech, comparative analysis of living species provides a viable alternative to fossil data. However, the neural basis for vocal mimicry and for mimesis in general remains unknown.},
author = {Fitch, W.Tecumseh},
doi = {10.1016/S1364-6613(00)01494-7},
file = {::},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {7},
pages = {258--267},
title = {{The evolution of speech: a comparative review}},
volume = {4},
year = {2000}
}
@article{Blank2016,
abstract = {Successful perception depends on combining sensory input with prior knowledge. However, the underlying mechanism by which these two sources of information are combined is unknown. In speech perception, as in other domains, two functionally distinct coding schemes have been proposed for how expectations influence representation of sensory evidence. Traditional models suggest that expected features of the speech input are enhanced or sharpened via interactive activation (Sharpened Signals). Conversely, Predictive Coding suggests that expected features are suppressed so that unexpected features of the speech input (Prediction Errors) are processed further. The present work is aimed at distinguishing between these two accounts of how prior knowledge influences speech perception. By combining behavioural, univariate, and multivariate fMRI measures of how sensory detail and prior expectations influence speech perception with computational modelling, we provide evidence in favour of Prediction Error computations. Increased sensory detail and informative expectations have additive behavioural and univariate neural effects because they both improve the accuracy of word report and reduce the BOLD signal in lateral temporal lobe regions. However, sensory detail and informative expectations have interacting effects on speech representations shown by multivariate fMRI in the posterior superior temporal sulcus. When prior knowledge was absent, increased sensory detail enhanced the amount of speech information measured in superior temporal multivoxel patterns, but with informative expectations, increased sensory detail reduced the amount of measured information. Computational simulations of Sharpened Signals and Prediction Errors during speech perception could both explain these behavioural and univariate fMRI observations. However, the multivariate fMRI observations were uniquely simulated by a Prediction Error and not a Sharpened Signal model. The interaction between prior expectation and sensory detail provides evidence for a Predictive Coding account of speech perception. Our work establishes methods that can be used to distinguish representations of Prediction Error and Sharpened Signals in other perceptual domains.},
author = {Blank, Helen and Davis, Matthew H},
doi = {10.1371/journal.pbio.1002577},
editor = {Zatorre, Robert},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Blank, Davis - 2016 - Prediction Errors but Not Sharpened Signals Simulate Multivoxel fMRI Patterns during Speech Perception(2).pdf:pdf},
issn = {1545-7885},
journal = {PLoS Biology},
month = {nov},
number = {11},
pages = {e1002577},
pmid = {27846209},
publisher = {Public Library of Science},
title = {{Prediction Errors but Not Sharpened Signals Simulate Multivoxel fMRI Patterns during Speech Perception.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27846209},
volume = {14},
year = {2016}
}
@article{Peterson1952,
abstract = {Relationships between a listener's identification of a spoken vowel and its properties as revealed from acoustic measurement of its sound wave have been a subject of study by many investigators.Both the utterance and the identification of a vowel depend upon the language and dialectal backgrounds and the vocal and auditory characteristics of the individuals concerned.The purpose of this paper is to discuss some of the control methods that have been used in the evaluation of these effects in a vowel study program at Bell Telephone Laboratories.The plan of the study, calibration of recording and measureing equipment, and methods for checking the performance of both speakers and listeners are described.The methods are illustrated from results of tests involving some 76 speakers and 70 listerners.},
author = {Peterson, Gordon E and Barney, Harold L},
doi = {10.1121/1.1906875},
isbn = {0001-4966},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {2},
pages = {175--184},
title = {{Control methods used in a study of the vowels}},
volume = {24},
year = {1952}
}
@article{Gourevitch2009,
abstract = {In order to investigate how the auditory scene is analyzed and perceived, auditory spectrotemporal receptive fields (STRFs) are generally used as a convenient way to describe how frequency and temporal sound information is encoded. However, using broadband sounds to estimate STRFs imperfectly reflects the way neurons process complex stimuli like conspecific vocalizations insofar as natural sounds often show limited bandwidth. Using recordings in the primary auditory cortex of anesthetized cats, we show that presentation of narrowband stimuli not including the best frequency of neurons provokes the appearance of residual peaks and increased firing rate at some specific spectral edges of stimuli compared with classical STRFs obtained from broadband stimuli. This result is the same for STRFs obtained from both spikes and local field potentials. Potential mechanisms likely involve release from inhibition. We thus emphasize some aspects of context dependency of STRFs, that is, how the balance of inhibitory and excitatory inputs is able to shape the neural response from the spectral content of stimuli.},
author = {Gour{\'{e}}vitch, Boris and Nore{\~{n}}a, Arnaud and Shaw, Gregory and Eggermont, Jos J.},
doi = {10.1093/cercor/bhn184},
issn = {1460-2199},
journal = {Cerebral Cortex},
month = {jun},
number = {6},
pages = {1448--1461},
pmid = {18854580},
title = {{Spectrotemporal Receptive Fields in Anesthetized Cat Primary Auditory Cortex Are Context Dependent}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18854580 https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhn184},
volume = {19},
year = {2009}
}
@article{Erickson1998,
abstract = {Psychological theories of categorization generally focus on either rule- or exemplar-based$\backslash$r$\backslash$nexplanations. We present 2 experiments that show evidence of both rule induction and$\backslash$r$\backslash$nexemplar encoding as well as a connectionist model, ATRn.rM, that specifies a mechanism for$\backslash$r$\backslash$ncombining rule- and exemplar-based representation. In 2 experiments participants learned to$\backslash$r$\backslash$nclassify items, most of which followed a simple rule, although there were a few frequently$\backslash$r$\backslash$noccurring exceptions. Experiment 1 examined how people extrapolate beyond the range of$\backslash$r$\backslash$ntraining. Experiment 2 examined the effect of instance frequency on generalization.$\backslash$r$\backslash$nCategorization behavior was well described by the model, in which exemplar representation is$\backslash$r$\backslash$nused for both rule and exception processing. A key element in correctly modeling these results$\backslash$r$\backslash$nwas capturing the interaction between the rule- and exemplar-based representations by using$\backslash$r$\backslash$nshifts of attention between rules and exemplars.},
author = {Erickson, Michael A. and Kruschke, John K.},
doi = {10.1037/0096-3445.127.2.107},
isbn = {0096-3445; 1939-2222},
issn = {0096-3445},
journal = {Journal of Experimental Psychology: General},
number = {2},
pages = {107--140},
pmid = {9622910},
title = {{Rules and Exemplars in Category Learning}},
url = {http://www.indiana.edu/{~}kruschke/articles/EricksonK1998.pdf},
volume = {127},
year = {1998}
}
@inproceedings{Chetouani2002,
author = {Chetouani, M. and Gas, B. and Zarader, J.L. and Chavy, C.},
booktitle = {Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)},
doi = {10.1109/IJCNN.2002.1005585},
isbn = {0-7803-7278-6},
pages = {852--857 vol.1},
publisher = {IEEE},
title = {{Discriminative training for neural predictive coding applied to speech features extraction}},
url = {http://ieeexplore.ieee.org/document/1005585/},
year = {2002}
}
@article{Ranasinghe2013,
abstract = {Neurons at higher stations of each sensory system are responsive to feature combinations not present at lower levels. As a result, the activity of these neurons becomes less redundant than lower levels. We recorded responses to speech sounds from the inferior colliculus and the primary auditory cortex neurons of rats, and tested the hypothesis that primary auditory cortex neurons are more sensitive to combinations of multiple acoustic parameters compared to inferior colliculus neurons. We independently eliminated periodicity information, spectral information and temporal information in each consonant and vowel sound using a noise vocoder. This technique made it possible to test several key hypotheses about speech sound processing. Our results demonstrate that inferior colliculus responses are spatially arranged and primarily determined by the spectral energy and the fundamental frequency of speech, whereas primary auditory cortex neurons generate widely distributed responses to multiple acoustic parameters, and are not strongly influenced by the fundamental frequency of speech. We found no evidence that inferior colliculus or primary auditory cortex was specialized for speech features such as voice onset time or formants. The greater diversity of responses in primary auditory cortex compared to inferior colliculus may help explain how the auditory system can identify a wide range of speech sounds across a wide range of conditions without relying on any single acoustic cue. ?? 2013 IBRO.},
author = {Ranasinghe, K. G. and Vrana, W. A. and Matney, C. J. and Kilgard, M. P.},
doi = {10.1016/j.neuroscience.2013.08.005},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Ranasinghe et al. - 2013 - Increasing diversity of neural responses to speech sounds across the central auditory pathway(2).pdf:pdf},
isbn = {1873-7544 (Electronic)
0306-4522 (Linking)},
issn = {03064522},
journal = {Neuroscience},
keywords = {Multiple acoustic parameters,Neural response diversity,Noise-vocoded speech,Rat auditory system,Redundancy reduction},
pages = {80--97},
pmid = {23954862},
publisher = {IBRO},
title = {{Increasing diversity of neural responses to speech sounds across the central auditory pathway}},
url = {http://dx.doi.org/10.1016/j.neuroscience.2013.08.005},
volume = {252},
year = {2013}
}
@book{Wittgenstein1958,
address = {Oxford, UK},
author = {Wittgenstein, Ludwig},
editor = {Blackwell, Basil},
pages = {30--36},
publisher = {Basil Blackwell Ltd.},
title = {{Philosophical Investigations}},
year = {1958}
}
@article{Jayaraman2015,
author = {Jayaraman, Swapnaa and Fausey, Caitlin M. and Smith, Linda B.},
doi = {10.1371/journal.pone.0123780},
editor = {Nardini, Marko},
file = {::},
issn = {1932-6203},
journal = {PLOS ONE},
month = {may},
number = {5},
pages = {e0123780},
publisher = {Public Library of Science},
title = {{The Faces in Infant-Perspective Scenes Change over the First Year of Life}},
url = {http://dx.plos.org/10.1371/journal.pone.0123780},
volume = {10},
year = {2015}
}
@article{Norris2016,
abstract = {Speech perception involves prediction, but how is that prediction implemented? In cognitive models prediction has often been taken to imply that there is feedback of activation from lexical to pre-lexical processes as implemented in interactive-activation models (IAMs). We show that simple activation feedback does not actually improve speech recognition. However, other forms of feedback can be beneficial. In particular, feedback can enable the listener to adapt to changing input, and can potentially help the listener to recognise unusual input, or recognise speech in the presence of competing sounds. The common feature of these helpful forms of feedback is that they are all ways of optimising the performance of speech recognition using Bayesian inference. That is, listeners make predictions about speech because speech recognition is optimal in the sense captured in Bayesian models.},
author = {Norris, Dennis and McQueen, James M and Cutler, Anne},
doi = {10.1080/23273798.2015.1081703},
file = {::},
issn = {2327-3798},
journal = {Language, cognition and neuroscience},
keywords = {Bayesian inference,Speech recognition,feedback,prediction},
month = {jan},
number = {1},
pages = {4--18},
pmid = {26740960},
publisher = {Taylor {\&} Francis},
title = {{Prediction, Bayesian inference and feedback in speech recognition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26740960 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4685608},
volume = {31},
year = {2016}
}
@article{Ison2007a,
abstract = {Auditory brainstem-evoked response (ABR) thresholds were obtained in a longitudinal study of C57BL/6J mice between 10 and 53 weeks old, with repeated testing every 2 weeks. On alternate weeks, acoustic startle reflex (ASR) amplitudes were measured, elicited by tone pips with stimulus frequencies of 3, 6, 12, and 24 kHz, and intensities from subthreshold up to 110 dB sound pressure level. The increase in ABR thresholds for 3 and 6 kHz test stimuli followed a linear time course with increasing age from 10 to 53 weeks, with a slope of about 0.7 dB/week, and for 48 kHz a second linear time course, but beginning at 10 weeks with a slope of about 2.3 dB/week. ABR thresholds for 12, 24, and 32 kHz increased after one linear segment with a 0.7 dB slope, then after a variable delay related to the test frequency, shifted to a second segment having slopes of 3-5 dB/week. Hearing loss initially reduced the ASR for all eliciting stimuli, but at about 6 months of age, the response elicited by intense 3 and 6 kHz stimuli began to increase to reach values about three times above normal, and previously subthreshold stimuli came to elicit vigorous responses seen at first only for the intense stimuli. This hyperacusis-like effect appeared in all mice but was especially pronounced in mice with more serious hearing loss. These ABR data, together with a review of histopathological data in the C57BL/6 literature, suggest that the non-frequency-specific slow time course of hearing loss results from pathology in the lateral wall of the cochlea, whereas the stimulus-specific hearing loss with a rapid time course results from hair cell loss. Delayed exaggeration of the ASR with hearing loss reveals a deficit in centrifugal inhibitory control over the afferent reflex pathways after central neural reorganization, suggesting that this mouse may provide a useful model of age-related tinnitus and associated hyperacusis.},
annote = {From Duplicate 2 (Age-related hearing loss in C57BL/6J mice has both frequency-specific and non-frequency-specific components that produce a hyperacusis-like exaggeration of the acoustic startle reflex - Ison, James R.; Allen, Paul D.; O'Neill, William E.)

10.1007/s10162-007-0098-3},
author = {Ison, James R. and Allen, Paul D. and O'Neill, William E.},
doi = {10.1007/s10162-007-0098-3},
isbn = {1525-3961 (Print)$\backslash$n1438-7573 (Linking)},
issn = {15253961},
journal = {JARO - Journal of the Association for Research in Otolaryngology},
keywords = {Aging,Hearing loss,Mixed strial/sensory presbycusis,Plasticity,Startle,Tinnitus/hyperacusis},
month = {nov},
number = {4},
pages = {539--550},
pmid = {17952509},
publisher = {Springer-Verlag},
title = {{Age-related hearing loss in C57BL/6J mice has both frequency-specific and non-frequency-specific components that produce a hyperacusis-like exaggeration of the acoustic startle reflex}},
url = {http://link.springer.com/10.1007/s10162-007-0098-3 http://dx.doi.org/10.1007/s10162-007-0098-3},
volume = {8},
year = {2007}
}
@article{Nosofsky1988,
author = {Nosofsky, Robert M. and M., Robert},
doi = {10.1037/0278-7393.14.1.54},
file = {::},
issn = {0278-7393},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
keywords = {adults,classification {\&} typicality ratings,stimulus similarity {\&} frequency},
number = {1},
pages = {54--65},
publisher = {American Psychological Association},
title = {{Similarity, frequency, and category representations.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.14.1.54},
volume = {14},
year = {1988}
}
@article{Malone2015,
author = {Malone, Brian J. and Scott, Brian H. and Semple, Malcolm N.},
journal = {Journal of Neurophysiology},
number = {7},
title = {{Diverse cortical codes for scene segmentation in primate auditory cortex}},
url = {http://jn.physiology.org/content/113/7/2934},
volume = {113},
year = {2015}
}
@article{Clerkin2016,
author = {Clerkin, Elizabeth M. and Hart, Elizabeth and Rehg, James M. and Yu, Chen and Smith, Linda B.},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
number = {1711},
title = {{Real-world visual statistics and infants' first-learned object names}},
url = {http://rstb.royalsocietypublishing.org/content/372/1711/20160055.article-info},
volume = {372},
year = {2016}
}
@incollection{Flemming2005,
address = {Oxford, UK},
author = {Flemming, Edward},
booktitle = {The Handbook of Speech Perception},
doi = {10.1002/9780470757024.ch7},
file = {::},
isbn = {9780470757024},
keywords = {optimality theory,phonetic descriptions,phonological contrast,phonological patterns,speech perception},
pages = {156--181},
publisher = {Blackwell Publishing Ltd},
title = {{Speech Perception and Phonological Contrast}},
url = {http://doi.wiley.com/10.1002/9780470757024.ch7},
year = {2005}
}
@article{Kobler1985,
author = {Kobler, J.B. and Wilson, B.S. and Henson, O.W. and Bishop, A.L.},
doi = {10.1016/0378-5955(85)90161-3},
issn = {03785955},
journal = {Hearing Research},
month = {jan},
number = {2},
pages = {99--108},
title = {{Echo intensity compensation by echolocating bats}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0378595585901613},
volume = {20},
year = {1985}
}
@article{Arnal2012,
abstract = {Many theories of perception are anchored in the central notion that the brain continuously updates an internal model of the world to infer the probable causes of sensory events. In this framework, the brain needs not only to predict the causes of sensory input, but also when they are most likely to happen. In this article, we review the neurophysiological bases of sensory predictions of “what' (predictive coding) and ‘when' (predictive timing), with an emphasis on low-level oscillatory mechanisms. We argue that neural rhythms offer distinct and adapted computational solutions to predicting ‘what' is going to happen in the sensory environment and ‘when'.},
author = {Arnal, Luc H. and Giraud, Anne-Lise},
doi = {10.1016/j.tics.2012.05.003},
file = {::},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {7},
pages = {390--398},
title = {{Cortical oscillations and sensory predictions}},
volume = {16},
year = {2012}
}
@article{Sadagopan2009,
abstract = {In the auditory cortex of awake animals, a substantial number of neurons do not respond to pure tones. These neurons have historically been classified as "unresponsive" and even been speculated as being nonauditory. We discovered, however, that many of these neurons in the primary auditory cortex (A1) of awake marmoset monkeys were in fact highly selective for complex sound features. We then investigated how such selectivity might arise from the tone-tuned inputs that these neurons likely receive. We found that these non-tone responsive neurons exhibited nonlinear combination-sensitive responses that require precise spectral and temporal combinations of two tone pips. The nonlinear spectrotemporal maps derived from these neurons were correlated with their selectivity for complex acoustic features. These non-tone responsive and nonlinear neurons were commonly encountered at superficial cortical depths in A1. Our findings demonstrate how temporally and spectrally specific nonlinear integration of putative tone-tuned inputs might underlie a diverse range of high selectivity of A1 neurons in awake animals. We propose that describing A1 neurons with complex response properties in terms of tone-tuned input channels can conceptually unify a wide variety of observed neural selectivity to complex sounds into a lower dimensional description.},
author = {Sadagopan, S. and Wang, X.},
doi = {10.1523/JNEUROSCI.1286-09.2009},
issn = {0270-6474},
journal = {Journal of Neuroscience},
month = {sep},
number = {36},
pages = {11192--11202},
pmid = {19741126},
title = {{Nonlinear Spectrotemporal Interactions Underlying Selectivity for Complex Sounds in Auditory Cortex}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19741126 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2757444 http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1286-09.2009},
volume = {29},
year = {2009}
}
@article{Shepard1987,
abstract = {A psychological space is established for any set of stimuli by determining metric distances between the stimuli such that the probability that a response learned to any stimulus will generalize to any other is an invariant monotonic function of the distance between them. To a good approximation, this probability of generalization (i) decays exponentially with this distance, and (ii) does so in accordance with one of two metrics, depending on the relation between the dimensions along which the stimuli vary. These empirical regularities are mathematically derivable from universal principles of natural kinds and probabilistic geometry that may, through evolutionary internalization, tend to govern the behaviors of all sentient organisms.},
author = {Shepard, R.},
doi = {10.1126/science.3629243},
isbn = {0036-8075 (Print)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science},
number = {4820},
pages = {1317--1323},
pmid = {3629243},
title = {{Toward a universal law of generalization for psychological science}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.3629243},
volume = {237},
year = {1987}
}
@article{Bartlett2013,
abstract = {The auditory thalamus, or medial geniculate body (MGB), is the primary sensory input to auditory cortex. Therefore, it plays a critical role in the complex auditory processing necessary for robust speech perception. This review will describe the functional organization of the thalamus as it relates to processing acoustic features important for speech perception, focusing on thalamic nuclei that relate to auditory representations of language sounds. The MGB can be divided into three main subdivisions, the ventral, dorsal, and medial subdivisions, each with different connectivity, auditory response properties, neuronal properties, and synaptic properties. Together, the MGB subdivisions actively and dynamically shape complex auditory processing and form ongoing communication loops with auditory cortex and subcortical structures. Copyright {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
annote = {From Duplicate 1 (The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception - Bartlett, Edward L.)

From Duplicate 1 (The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception. - Bartlett, Edward L.)

10.1016/j.bandl.2013.03.003},
author = {Bartlett, Edward L.},
doi = {10.1016/j.bandl.2013.03.003},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Bartlett - 2013 - The organization and physiology of the auditory thalamus and its role in processing acoustic features important for(2).pdf:pdf},
issn = {10902155},
journal = {Brain and language},
month = {jul},
number = {1},
pages = {29--48},
pmid = {23725661},
title = {{The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception.}},
url = {http://dx.doi.org/10.1016/j.bandl.2013.03.003 http://linkinghub.elsevier.com/retrieve/pii/S0093934X13000722},
volume = {126},
year = {2013}
}
@article{Clauset2007a,
abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.},
archivePrefix = {arXiv},
arxivId = {0706.1062},
author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, M. E. J.},
doi = {10.1137/070710111},
eprint = {0706.1062},
file = {::},
month = {jun},
title = {{Power-law distributions in empirical data}},
url = {http://arxiv.org/abs/0706.1062 http://dx.doi.org/10.1137/070710111},
year = {2007}
}
