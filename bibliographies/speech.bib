
@article{abbottCrossmodalPhoneticEncoding2018,
  title = {Cross-Modal Phonetic Encoding Facilitates the {{McGurk}} Illusion and Phonemic Restoration},
  author = {Abbott, Noelle M and Shahin, Antoine J},
  year = {2018},
  month = oct,
  journal = {Journal of Neurophysiology},
  pages = {jn.00262.2018},
  publisher = {{American Physiological Society Bethesda, MD}},
  issn = {0022-3077},
  doi = {10.1152/jn.00262.2018},
  url = {https://www.physiology.org/doi/10.1152/jn.00262.2018},
  urldate = {2018-10-19},
  abstract = {In spoken language, audiovisual (AV) perception occurs when the visual modality influences encoding of acoustic features (e.g., phonetic representations) at the auditory cortex. We examined how visual speech (lip-movements) transforms phonetic representations, indexed by changes to the N1 auditory evoked potential (AEP). EEG was acquired while human subjects watched and listened to videos of a speaker uttering consonant vowel (CV) syllables, /ba/ and /wa/, presented in auditory-only, AV congruent or incongruent contexts, or in a context in which the consonants were replaced by white noise (noise-replaced). Subjects reported whether they heard 'ba' or 'wa'. We hypothesized that the auditory N1 amplitude during illusory perception (caused by incongruent AV input, like in the McGurk illusion, or white noise-replaced consonants in CV utterances) should shift to reflect the auditory N1 characteristics of the phonemes conveyed visually (by mouth movements) as opposed to acoustically. Indeed, the N1 AEP became l...},
  keywords = {\#nosource,Audiovisual integration,Auditory evoked potentials,cross-modal encoding,McGurk Illusion,Phonemic restoration}
}

@misc{AberrantCorticalActivity,
  title = {Aberrant {{Cortical Activity}} in {{Multiple GCaMP6-Expressing Transgenic Mouse Lines}} | {{eNeuro}}},
  url = {http://www.eneuro.org/content/4/5/ENEURO.0207-17.2017/tab-figures-data},
  urldate = {2019-01-12},
  file = {/Users/jonny/Zotero/storage/2SQZ9Z5E/tab-figures-data.html}
}

@article{anumanchipalliSpeechSynthesisNeural2019,
  title = {Speech Synthesis from Neural Decoding of Spoken Sentences},
  author = {Anumanchipalli, Gopala K. and Chartier, Josh and Chang, Edward F.},
  year = {2019},
  month = apr,
  journal = {Nature},
  volume = {568},
  number = {7753},
  pages = {493},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1119-1},
  url = {https://www.nature.com/articles/s41586-019-1119-1},
  urldate = {2019-04-29},
  abstract = {A neural decoder uses kinematic and sound representations encoded in human cortical activity to synthesize audible sentences, which are readily identified and transcribed by listeners.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/jonny/Papers/AnumanchipalliG/2019/Anumanchipalli_2019_Speech synthesis from neural decoding of spoken sentences.pdf;/Users/jonny/Zotero/storage/ZBFXXJJC/s41586-019-1119-1.html}
}

@article{Arnal2012,
  title = {Cortical Oscillations and Sensory Predictions},
  author = {Arnal, Luc H. and Giraud, Anne-Lise},
  year = {2012},
  journal = {Trends in Cognitive Sciences},
  volume = {16},
  number = {7},
  pages = {390--398},
  issn = {13646613},
  doi = {10.1016/j.tics.2012.05.003},
  abstract = {Many theories of perception are anchored in the central notion that the brain continuously updates an internal model of the world to infer the probable causes of sensory events. In this framework, the brain needs not only to predict the causes of sensory input, but also when they are most likely to happen. In this article, we review the neurophysiological bases of sensory predictions of ``what' (predictive coding) and `when' (predictive timing), with an emphasis on low-level oscillatory mechanisms. We argue that neural rhythms offer distinct and adapted computational solutions to predicting `what' is going to happen in the sensory environment and `when'.},
  file = {/Users/jonny/Zotero/storage/YJQ585LJ/Arnal, Giraud - 2012 - Cortical oscillations and sensory predictions(3).pdf}
}

@article{Ashby1992,
  title = {Complex Decision Rules in Categorization: {{Contrasting}} Novice and Experienced Performance.},
  author = {Ashby, F. Gregory and Maddox, W. Todd},
  year = {1992},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {18},
  number = {1},
  pages = {50--71},
  publisher = {{American Psychological Association}},
  issn = {1939-1277},
  doi = {10.1037/0096-1523.18.1.50},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.18.1.50},
  urldate = {2017-02-15},
  keywords = {experience level,naive vs experienced adults},
  file = {/Users/jonny/Papers/AshbyF/1992/Ashby_1992_Complex decision rules in categorization.pdf}
}

@article{Ashby2005,
  title = {Human {{Category Learning}}},
  author = {Ashby, F. Gregory and Maddox, W. Todd},
  year = {2005},
  month = feb,
  journal = {Annual Review of Psychology},
  volume = {56},
  number = {1},
  pages = {149--178},
  publisher = {{Annual Reviews}},
  issn = {0066-4308},
  doi = {10.1146/annurev.psych.56.091103.070217},
  url = {http://www.annualreviews.org/doi/10.1146/annurev.psych.56.091103.070217},
  urldate = {2017-02-15},
  abstract = {Much recent evidence suggests some dramatic differences in the way people learn perceptual categories, depending on exactly how the categories were constructed. Four different kinds of category-learning tasks are currently popular\textemdash rule-based tasks, information-integration tasks, prototype distortion tasks, and the weather prediction task. The cognitive, neuropsychological, and neuroimaging results obtained using these four tasks are qualitatively different. Success in rule-based (explicit reasoning) tasks depends on frontal-striatal circuits and requires working memory and executive attention. Success in information-integration tasks requires a form of procedural learning and is sensitive to the nature and timing of feedback. Prototype distortion tasks induce perceptual (visual cortical) learning. A variety of different strategies can lead to success in the weather prediction task. Collectively, results from these four tasks provide strong evidence that human category learning is mediated by multiple, qua...},
  keywords = {decision bound,exemplar,multiple systems,prototype,striatum},
  file = {/Users/jonny/Papers/AshbyF/2005/Ashby_2005_Human Category Learning2.pdf}
}

@article{Bailey1980,
  title = {Information in Speech: Observations on the Perception of [s]-Stop Clusters},
  author = {Bailey, P J and Summerfield, Q},
  year = {1980},
  month = aug,
  journal = {Journal of experimental psychology. Human perception and performance},
  volume = {6},
  number = {3},
  pages = {536--563},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.6.3.536},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/6447767},
  urldate = {2016-12-13},
  abstract = {A series of experiments is reported that investigated the pattern of acoustic information specifying place and manner of stop consonants in medial position after [s]. In both production and perception, information for stop place includes the spectrum of the fricative at offset, the duration of the silent closure interval, the spectral relationship between the frequency of the stop release burst and the following periodically excited formants, and the spectral and temporal characteristics of the first formant transition. Similarly, the information for stop manner includes the duration of silent closure, the frequency of the first formant at the release, the magnitude of the first formant transition, and the proximity of the second and third formants at release. A relationship was shown to exist in perception between the spectral characteristics of the first formant and the duration of the silent closure required to hear a stop. This appears to reciprocate the covariation of these parameters in production across different places of articulation and different vocalic contexts. The existence of perceptual sensitivity to a wide range of the acoustic consequences of production questions the efficacy of accounts of speech perception in terms of the fractionation of the signal into elemental acoustic cues, which are then integrated to yield a phonetic percept. It is argued that it is inappropriate to ascribe a psychological status to cues whose only reality is their operational role as physical parameters whose manipulation can change the phenotic interpretation of a signal. It is suggested that the metric of the information for phonetic perception cannot be that of the cues; rather, a metric should be sought in which acoustic and articulatory dynamics are isomorphic.},
  isbn = {0096-1523},
  pmid = {6447767},
  keywords = {_tablet,Humans,Phonetics,Psychoacoustics,Speech Percept},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BaileyP/bailey_1980_information_in_speech.pdf;/Users/jonny/Dropbox/papers/zotero/B/BaileyP/false}
}

@article{Bartlett2013,
  title = {The Organization and Physiology of the Auditory Thalamus and Its Role in Processing Acoustic Features Important for Speech Perception.},
  author = {Bartlett, Edward L.},
  year = {2013},
  month = jul,
  journal = {Brain and language},
  volume = {126},
  number = {1},
  pages = {29--48},
  issn = {10902155},
  doi = {10.1016/j.bandl.2013.03.003},
  url = {http://dx.doi.org/10.1016/j.bandl.2013.03.003},
  abstract = {The auditory thalamus, or medial geniculate body (MGB), is the primary sensory input to auditory cortex. Therefore, it plays a critical role in the complex auditory processing necessary for robust speech perception. This review will describe the functional organization of the thalamus as it relates to processing acoustic features important for speech perception, focusing on thalamic nuclei that relate to auditory representations of language sounds. The MGB can be divided into three main subdivisions, the ventral, dorsal, and medial subdivisions, each with different connectivity, auditory response properties, neuronal properties, and synaptic properties. Together, the MGB subdivisions actively and dynamically shape complex auditory processing and form ongoing communication loops with auditory cortex and subcortical structures. Copyright \textcopyright{} 2013 Elsevier Inc. All rights reserved.},
  pmid = {23725661},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BartlettE/bartlett_2013_the_organization_and_physiology_of_the_auditory_thalamus_and_its_role_in.pdf}
}

@article{Bates2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} Lme4},
  author = {Bates, Douglas and Machler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01},
  keywords = {\#nosource}
}

@article{Beale1995,
  title = {Categorical Effects in the Perception of Faces},
  author = {Beale, James M. and Keil, Frank C.},
  year = {1995},
  journal = {Cognition},
  volume = {57},
  number = {3},
  pages = {217--239},
  issn = {00100277},
  doi = {10.1016/0010-0277(95)00669-X},
  abstract = {These studies suggest categorical perception effects may be much more general than has commonly been believed and can occur in apparently similar ways at dramatically different levels of processing. To test the nature of individual face representations, a linear continuum of ``morphed'' faces was generated between individual exemplars of familiar faces. In separate categorization, discrimination and ``better-likeness'' tasks, subjects viewed pairs of faces from these continua. Subjects discriminate most accurately when face-pairs straddle apparent category boundaries; thus individual faces are perceived categorically. A high correlation is found between the familiarity of a face-pair and the magnitude of the categorization effect. Categorical perception therefore is not limited to low-level perceptual continua, but can occur at higher levels and may be acquired through experience as well.},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BealeJ/beale_1995_categorical_effects_in_the_perception_of_faces.pdf}
}

@article{Belin2000a,
  title = {Voice-Selective Areas in Human Auditory Cortex.},
  author = {Belin, P and Zatorre, R J and Lafaille, P and Ahad, P and Pike, B},
  year = {2000},
  journal = {Nature},
  volume = {403},
  number = {6767},
  pages = {309--312},
  issn = {0028-0836},
  doi = {10.1038/35002078},
  abstract = {The human voice contains in its acoustic structure a wealth of information on the speaker's identity and emotional state which we perceive with remarkable ease and accuracy. Although the perception of speaker-related features of voice plays a major role in human communication, little is known about its neural basis. Here we show, using functional magnetic resonance imaging in human volunteers, that voice-selective regions can be found bilaterally along the upper bank of the superior temporal sulcus (STS). These regions showed greater neuronal activity when subjects listened passively to vocal sounds, whether speech or non-speech, than to non-vocal environmental sounds. Central STS regions also displayed a high degree of selectivity by responding significantly more to vocal sounds than to matched control stimuli, including scrambled voices and amplitude-modulated noise. Moreover, their response to stimuli degraded by frequency filtering paralleled the subjects' behavioural performance in voice-perception tasks that used these stimuli. The voice-selective areas in the STS may represent the counterpart of the face-selective areas in human visual cortex; their existence sheds new light on the functional architecture of the human auditory cortex.},
  isbn = {0028-0836 (Print)},
  pmid = {10659849},
  file = {/Users/jonny/Zotero/storage/K99RUWSW/Belin et al. - 2000 - Voice-selective areas in human auditory cortex(5).pdf;/Users/jonny/Zotero/storage/PR8HBXVJ/Belin et al. - 2000 - Voice-selective areas in human auditory cortex(6).pdf}
}

@article{Bidelman2013,
  title = {Tracing the Emergence of Categorical Speech Perception in the Human Auditory System},
  author = {Bidelman, Gavin M. and Moreno, Sylvain and Alain, Claude},
  year = {2013},
  journal = {NeuroImage},
  volume = {79},
  pages = {201--212},
  publisher = {{Elsevier Inc.}},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2013.04.093},
  url = {http://dx.doi.org/10.1016/j.neuroimage.2013.04.093},
  abstract = {Speech perception requires the effortless mapping from smooth, seemingly continuous changes in sound features into discrete perceptual units, a conversion exemplified in the phenomenon of categorical perception. Explaining how/when the human brain performs this acoustic-phonetic transformation remains an elusive problem in current models and theories of speech perception. In previous attempts to decipher the neural basis of speech perception, it is often unclear whether the alleged brain correlates reflect an underlying percept or merely changes in neural activity that covary with parameters of the stimulus. Here, we recorded neuroelectric activity generated at both cortical and subcortical levels of the auditory pathway elicited by a speech vowel continuum whose percept varied categorically from /u/ to /a/. This integrative approach allows us to characterize how various auditory structures code, transform, and ultimately render the perception of speech material as well as dissociate brain responses reflecting changes in stimulus acoustics from those that index true internalized percepts. We find that activity from the brainstem mirrors properties of the speech waveform with remarkable fidelity, reflecting progressive changes in speech acoustics but not the discrete phonetic classes reported behaviorally. In comparison, patterns of late cortical evoked activity contain information reflecting distinct perceptual categories and predict the abstract phonetic speech boundaries heard by listeners. Our findings demonstrate a critical transformation in neural speech representations between brainstem and early auditory cortex analogous to an acoustic-phonetic mapping necessary to generate categorical speech percepts. Analytic modeling demonstrates that a simple nonlinearity accounts for the transformation between early (subcortical) brain activity and subsequent cortical/behavioral responses to speech ({$>$}. 150-200. ms) thereby describing a plausible mechanism by which the brain achieves its acoustic-to-phonetic mapping. Results provide evidence that the neurophysiological underpinnings of categorical speech are present cortically by \textasciitilde. 175. ms after sound enters the ear. ?? 2013 Elsevier Inc..},
  isbn = {1053-8119},
  pmid = {23648960},
  keywords = {Auditory event-related potentials (ERP),Brainstem response,Categorical perception,Neural computation,Speech perception},
  file = {/Users/jonny/Zotero/storage/FH82KE9G/Bidelman, Moreno, Alain - 2013 - Tracing the emergence of categorical speech perception in the human auditory system(3).pdf}
}

@article{Bird2014,
  title = {Categorical Encoding of Color in the Brain.},
  author = {Bird, Chris M and Berens, Samuel C and Horner, Aidan J and Franklin, Anna},
  year = {2014},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {111},
  number = {12},
  pages = {4590--5},
  publisher = {{National Academy of Sciences}},
  issn = {1091-6490},
  doi = {10.1073/pnas.1315275111},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/24591602},
  urldate = {2017-01-20},
  abstract = {The areas of the brain that encode color categorically have not yet been reliably identified. Here, we used functional MRI adaptation to identify neuronal populations that represent color categories irrespective of metric differences in color. Two colors were successively presented within a block of trials. The two colors were either from the same or different categories (e.g., "blue 1 and blue 2" or "blue 1 and green 1"), and the size of the hue difference was varied. Participants performed a target detection task unrelated to the difference in color. In the middle frontal gyrus of both hemispheres and to a lesser extent, the cerebellum, blood-oxygen level-dependent response was greater for colors from different categories relative to colors from the same category. Importantly, activation in these regions was not modulated by the size of the hue difference, suggesting that neurons in these regions represent color categorically, regardless of metric color difference. Representational similarity analyses, which investigated the similarity of the pattern of activity across local groups of voxels, identified other regions of the brain (including the visual cortex), which responded to metric but not categorical color differences. Therefore, categorical and metric hue differences appear to be coded in qualitatively different ways and in different brain regions. These findings have implications for the long-standing debate on the origin and nature of color categories, and also further our understanding of how color is processed by the brain.},
  pmid = {24591602},
  keywords = {categorization,chromatic,functional magnetic resonance imaging},
  file = {/Users/jonny/Zotero/storage/PN5G2FMN/Bird et al. - 2014 - Categorical encoding of color in the brain(3).pdf}
}

@article{Bizley2013,
  title = {The What, Where and How of Auditory-Object Perception.},
  author = {Bizley, Jennifer K and Cohen, Yale E},
  year = {2013},
  month = oct,
  journal = {Nature reviews. Neuroscience},
  volume = {14},
  number = {10},
  pages = {693--707},
  publisher = {{NIH Public Access}},
  issn = {1471-0048},
  doi = {10.1038/nrn3565},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/24052177},
  urldate = {2017-02-15},
  abstract = {The fundamental perceptual unit in hearing is the 'auditory object'. Similar to visual objects, auditory objects are the computational result of the auditory system's capacity to detect, extract, segregate and group spectrotemporal regularities in the acoustic environment; the multitude of acoustic stimuli around us together form the auditory scene. However, unlike the visual scene, resolving the component objects within the auditory scene crucially depends on their temporal structure. Neural correlates of auditory objects are found throughout the auditory system. However, neural responses do not become correlated with a listener's perceptual reports until the level of the cortex. The roles of different neural structures and the contribution of different cognitive states to the perception of auditory objects are not yet fully understood.},
  pmid = {24052177},
  file = {/Users/jonny/Papers/BizleyJ/2013/Bizley_2013_The what, where and how of auditory-object perception2.pdf}
}

@article{Blank2016,
  title = {Prediction {{Errors}} but {{Not Sharpened Signals Simulate Multivoxel fMRI Patterns}} during {{Speech Perception}}.},
  author = {Blank, Helen and Davis, Matthew H},
  editor = {Zatorre, Robert},
  year = {2016},
  month = nov,
  journal = {PLoS Biology},
  volume = {14},
  number = {11},
  pages = {e1002577},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002577},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/27846209},
  urldate = {2016-11-23},
  abstract = {Successful perception depends on combining sensory input with prior knowledge. However, the underlying mechanism by which these two sources of information are combined is unknown. In speech perception, as in other domains, two functionally distinct coding schemes have been proposed for how expectations influence representation of sensory evidence. Traditional models suggest that expected features of the speech input are enhanced or sharpened via interactive activation (Sharpened Signals). Conversely, Predictive Coding suggests that expected features are suppressed so that unexpected features of the speech input (Prediction Errors) are processed further. The present work is aimed at distinguishing between these two accounts of how prior knowledge influences speech perception. By combining behavioural, univariate, and multivariate fMRI measures of how sensory detail and prior expectations influence speech perception with computational modelling, we provide evidence in favour of Prediction Error computations. Increased sensory detail and informative expectations have additive behavioural and univariate neural effects because they both improve the accuracy of word report and reduce the BOLD signal in lateral temporal lobe regions. However, sensory detail and informative expectations have interacting effects on speech representations shown by multivariate fMRI in the posterior superior temporal sulcus. When prior knowledge was absent, increased sensory detail enhanced the amount of speech information measured in superior temporal multivoxel patterns, but with informative expectations, increased sensory detail reduced the amount of measured information. Computational simulations of Sharpened Signals and Prediction Errors during speech perception could both explain these behavioural and univariate fMRI observations. However, the multivariate fMRI observations were uniquely simulated by a Prediction Error and not a Sharpened Signal model. The interaction between prior expectation and sensory detail provides evidence for a Predictive Coding account of speech perception. Our work establishes methods that can be used to distinguish representations of Prediction Error and Sharpened Signals in other perceptual domains.},
  pmid = {27846209},
  file = {/Users/jonny/Dropbox/papers/zotero/B/BlankH/blank_2016_prediction_errors_but_not_sharpened_signals_simulate_multivoxel_fmri_patterns.pdf}
}

@article{Boersma2001,
  title = {Praat, a System for Doing Phonetics by Computer},
  author = {Boersma, Paul},
  year = {2001},
  journal = {Glot International},
  volume = {5},
  number = {9/10},
  pages = {341--347},
  issn = {0196-0202},
  doi = {10.1097/AUD.0b013e31821473f7},
  abstract = {See, stats, and : http : / / www . researchgate . net / publication / 208032992 PRAAT , a computer ARTICLE CITATIONS 942 DOWNLOADS 870 VIEWS 1 , 365 2 : Paul University 104 , 780 SEE David University 19 , 721 SEE Available : Paul Retrieved : 28},
  isbn = {1381-3439},
  pmid = {61},
  keywords = {\#nosource}
}

@article{Bornkessel-Schlesewsky2015,
  title = {Neurobiological Roots of Language in Primate Audition: Common Computational Properties},
  author = {{Bornkessel-Schlesewsky}, Ina and Schlesewsky, Matthias and Small, Steven L. and Rauschecker, Josef P.},
  year = {2015},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {19},
  number = {3},
  pages = {142--150},
  issn = {13646613},
  doi = {10.1016/j.tics.2014.12.008},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661314002757},
  urldate = {2017-08-08},
  keywords = {\#nosource}
}

@article{Brenowitz1997,
  title = {An Introduction to Birdsong and the Avian Song System},
  author = {Brenowitz, E. A. and Margoliash, D. and Nordeen, K. W.},
  year = {1997},
  journal = {Journal of Neurobiology},
  volume = {33},
  number = {5},
  pages = {495--500},
  issn = {00223034},
  abstract = {This special issue of the Journal of Neurobiology is devoted to a consideration of the avian song control system. In the 20 years that have passed since Nottebohm et al. 1976) first identified forebrain circuits that control song in birds, the song system has emerged as a leading model in behavioral neuroscience. To mark the beginning of the third decade of study of this model, we invited several leading investigators to contribute to this volume. We set two goals for the authors: to review progress in their area of study and, more important, to identify critical directions for future research. The modern study of birdsong began with the work of William Thorpe (1958, 1961). He showed that chaffinches (Fringilla coelebs) collected as nestlings and reared in the laboratory in isolation from conspecific adult males produced very abnormal songs. If these young birds were exposed to tape recordings of wild chaffinch songs, however, they eventually produced normal songs that matched those heard on the recordings. These studies demonstrated for the first time that young birds must learn the song of their species by listening to adult conspecifics. Thorpe's student Peter Marler greatly expanded upon this early work. Marler and his colleagues demonstrated the existence of local geographic song "dialects," that song learning is characterized by early sensitive periods, and that birds have innate predispositions to learn the song of their species. Masakazu Konishi, while a student with Marler, showed that birds must be able to hear themselves sing to develop song normally. Fernando Nottebohm, also while a student with Marler, showed that the peripheral control of song production is lateralized. Nottebohm and his colleagues subsequently identified neural circuits in the avian forebrain that control song behavior. This important discovery paved the way for many investigators who have subsequently contributed to our understanding of song behavior and its neural control. The birdsong system offers several advantages as a model for identifying neural mechanisms that underlie biologically relevant behavior: 1. Song is a learned behavior that is controlled by discrete neural circuits. 2. There are distinct phases in the development of song, with well-defined sensitive periods. One can relate the ontogeny of song behavior to the development of the underlying neural circuits. 3. Song is the product of stereotyped motor programs, with hierarchical organization of the premotor and motor nuclei. 4. Song behavior and the associated neural circuits are sexually dimorphic in most species. 5. Gonadal steroid hormones have pronounced effects on the development and adult function of the song control circuits, as well as on song behavior. 6. There is extensive plasticity of the adult song system, including ongoing neurogenesis and seasonal changes in morphology. 7. There is pronounced species diversity in different aspects of song behavior, including the timing of vocal learning, sex patterns of song production, number of songs that are learned, and seasonality of song behavior. This diversity provides opportunities for comparative studies of the song control system.},
  isbn = {0022-3034},
  pmid = {9369455},
  keywords = {\#nosource}
}

@article{Bub2001,
  title = {Maxwell's {{Demon}} and the {{Thermodynamics}} of {{Computation}}},
  author = {Bub, Jeffrey},
  year = {2001},
  journal = {Studies in History and Philosophy of Science Part B - Studies in History and Philosophy of Modern Physics},
  volume = {32},
  number = {4},
  eprint = {quant-ph/0203017},
  eprinttype = {arxiv},
  pages = {569--579},
  issn = {13552198},
  doi = {10.1016/S1355-2198(01)00023-5},
  abstract = {It is generally accepted, following Landauer and Bennett, that the process of measurement involves no minimum entropy cost, but the erasure of information in resetting the memory register of a computer to zero requires dissipating heat into the environment. This thesis has been challenged recently in a two-part article by Earman and Norton. I review some relevant observations in the thermodynamics of computation and argue that Earman and Norton are mistaken: there is in principle no entropy cost to the acquisition of information, but the destruction of information does involve an irreducible entropy cost. ?? 2001 Elsevier Science Ltd.},
  archiveprefix = {arXiv},
  isbn = {1355-2198},
  keywords = {Information,Maxwell's Demon,Measurement,Thermodynamics of Computation},
  file = {/Users/jonny/Zotero/storage/W46VUPDS/Bub - 2001 - Maxwell's Demon and the Thermodynamics of Computation(3).pdf}
}

@article{Buchman1986,
  title = {Word Deafness: One Hundred Years Later.},
  author = {Buchman, A S and Garron, D C and {Trost-Cardamone}, J E and Wichter, M D and Schwartz, M},
  year = {1986},
  journal = {Journal of Neurology, Neurosurgery \& Psychiatry},
  volume = {49},
  number = {5},
  pages = {489--499},
  issn = {0022-3050},
  doi = {10.1136/jnnp.49.5.489},
  url = {http://jnnp.bmj.com/cgi/doi/10.1136/jnnp.49.5.489},
  abstract = {Since its original description the diagnosis of word deafness has been greatly expanded. Confusion has arisen with regard to the usage of the related terms pure word deafness, auditory agnosia, and cortical deafness. Three new cases of word deafness are presented including one case with CT and necropsy correlation. These cases are compared with 34 previously reported cases of various cortical auditory disorders. Our review establishes that patients with word deafness who have had formal testing of linguistic and non-linguistic sound comprehension and musical abilities always demonstrated a more pervasive auditory agnosia. Despite the spectrum of auditory deficits and associated language abnormalities, patients with word deafness share common features including aetiology, pathology, clinical presentation and course. These common features justify inclusion of heterogeneous cortical auditory disorders under the rubric of word deafness. Despite some limitations the term "word deafness" should be retained for this syndrome, since inability to comprehend spoken words is the most distinctive clinical deficit. Word deafness is most frequently caused by cerebrovascular accidents of presumed cardiac embolisation, with bitemporal cortico-subcortical lesions. The sequence of cerebral injury is not predictive of resulting auditory deficits. Impairment of musical abilities parallels the severity of the auditory disorder.},
  isbn = {0022-3050 (Print)\textbackslash r0022-3050 (Linking)},
  pmid = {2423648},
  keywords = {\#nosource}
}

@article{caiPanopticImagingTransparent2018,
  title = {Panoptic Imaging of Transparent Mice Reveals Whole-Body Neuronal Projections and Skull\textendash Meninges Connections},
  author = {Cai, Ruiyao and Pan, Chenchen and Ghasemigharagoz, Alireza and Todorov, Mihail Ivilinov and F{\"o}rstera, Benjamin and Zhao, Shan and Bhatia, Harsharan S. and {Parra-Damas}, Arnaldo and Mrowka, Leander and Theodorou, Delphine and Rempfler, Markus and Xavier, Anna L. R. and Kress, Benjamin T. and Benakis, Corinne and Steinke, Hanno and Liebscher, Sabine and Bechmann, Ingo and Liesz, Arthur and Menze, Bjoern and Kerschensteiner, Martin and Nedergaard, Maiken and Ert{\"u}rk, Ali},
  year = {2018},
  month = dec,
  journal = {Nature Neuroscience},
  pages = {1},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0301-3},
  url = {https://www.nature.com/articles/s41593-018-0301-3},
  urldate = {2019-01-22},
  abstract = {A nanobody-based immunolabeling method, vDISCO, boosts the signal of fluorescent proteins and allows imaging of subcellular details in intact transparent mice. It uncovers neuronal projections and skull\textendash meninges connections in whole adult mice.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/jonny/Papers/CaiR/2018/Cai_2018_Panoptic imaging of transparent mice reveals whole-body neuronal projections.pdf;/Users/jonny/Zotero/storage/294YP2C6/s41593-018-0301-3.html}
}

@article{Carbonell2014,
  title = {Speech Is Not Special\ldots{} Again.},
  author = {Carbonell, Kathy M. and Lotto, Andrew J.},
  year = {2014},
  month = jun,
  journal = {Frontiers in psychology},
  volume = {5},
  number = {June},
  pages = {427},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00427},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00427/abstract},
  urldate = {2017-02-02},
  pmid = {24917830},
  keywords = {_tablet,as is apparent from,auditory processing,motor theory,multisensory i,multisensory integration,of nearly any research,of speech,or review article,reading the first line,sensorimotor effects on perception,specialness,speech perception,the},
  file = {/Users/jonny/Dropbox/papers/zotero/C/CarbonellK/carbonell_2014_speech_is_not_special…_again.pdf;/Users/jonny/Zotero/storage/XCQ9XMBD/Carbonell, Lotto - 2014 - Speech is not special… again(3).pdf}
}

@article{casserlySpeechPerceptionProduction2010,
  title = {Speech Perception and Production.},
  author = {Casserly, Elizabeth D and Pisoni, David B},
  year = {2010},
  month = sep,
  journal = {Wiley interdisciplinary reviews. Cognitive science},
  volume = {1},
  number = {5},
  pages = {629--647},
  publisher = {{NIH Public Access}},
  issn = {1939-5086},
  doi = {10.1002/wcs.63},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/23946864},
  urldate = {2017-12-18},
  abstract = {Until recently, research in speech perception and speech production has largely focused on the search for psychological and phonetic evidence of discrete, abstract, context-free symbolic units corresponding to phonological segments or phonemes. Despite this common conceptual goal and intimately related objects of study, however, research in these two domains of speech communication has progressed more or less independently for more than 60 years. In this article, we present an overview of the foundational works and current trends in the two fields, specifically discussing the progress made in both lines of inquiry as well as the basic fundamental issues that neither has been able to resolve satisfactorily so far. We then discuss theoretical models and recent experimental evidence that point to the deep, pervasive connections between speech perception and production. We conclude that although research focusing on each domain individually has been vital in increasing our basic understanding of spoken language processing, the human capacity for speech communication is so complex that gaining a full understanding will not be possible until speech perception and production are conceptually reunited in a joint approach to problems shared by both modes. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd. For further resources related to this article, please visit the WIREs website.},
  pmid = {23946864},
  file = {/Users/jonny/Papers/CasserlyE/2010/Casserly_2010_Speech perception and production.pdf}
}

@article{Centanni2013,
  title = {Detection and Identification of Speech Sounds Using Cortical Activity Patterns},
  author = {Centanni, T. M. and Sloan, A. M. and Reed, A. C. and Engineer, C. T. and Rennaker, R. L. and Kilgard, M. P.},
  year = {2013},
  journal = {Neuroscience},
  volume = {258},
  pages = {292--306},
  publisher = {{IBRO}},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2013.11.030},
  url = {http://dx.doi.org/10.1016/j.neuroscience.2013.11.030},
  abstract = {We have developed a classifier capable of locating and identifying speech sounds using activity from rat auditory cortex with an accuracy equivalent to behavioral performance and without the need to specify the onset time of the speech sounds. This classifier can identify speech sounds from a large speech set within 40. ms of stimulus presentation. To compare the temporal limits of the classifier to behavior, we developed a novel task that requires rats to identify individual consonant sounds from a stream of distracter consonants. The classifier successfully predicted the ability of rats to accurately identify speech sounds for syllable presentation rates up to 10 syllables per second (up to 17.9 {$\pm$} 1.5 bits/s), which is comparable to human performance. Our results demonstrate that the spatiotemporal patterns generated in primary auditory cortex can be used to quickly and accurately identify consonant sounds from a continuous speech stream without prior knowledge of the stimulus onset times. Improved understanding of the neural mechanisms that support robust speech processing in difficult listening conditions could improve the identification and treatment of a variety of speech-processing disorders. \textcopyright{} 2013 IBRO.},
  isbn = {0306-4522},
  pmid = {24286757},
  keywords = {Auditory cortex,Classifier,Coding,Rat,Temporal patterns},
  file = {/Users/jonny/Zotero/storage/KQFT5JAN/Centanni et al. - 2013 - Detection and identification of speech sounds using cortical activity patterns(3).pdf}
}

@article{Chang2010,
  title = {Categorical Speech Representation in Human Superior Temporal Gyrus.},
  author = {Chang, Edward F and Rieger, Jochem W and Johnson, Keith and Berger, Mitchel S and Barbaro, Nicholas M and Knight, Robert T},
  year = {2010},
  month = nov,
  journal = {Nature neuroscience},
  volume = {13},
  number = {11},
  pages = {1428--32},
  publisher = {{NIH Public Access}},
  issn = {1546-1726},
  doi = {10.1038/nn.2641},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/20890293},
  urldate = {2017-01-20},
  abstract = {Speech perception requires the rapid and effortless extraction of meaningful phonetic information from a highly variable acoustic signal. A powerful example of this phenomenon is categorical speech perception, in which a continuum of acoustically varying sounds is transformed into perceptually distinct phoneme categories. We found that the neural representation of speech sounds is categorically organized in the human posterior superior temporal gyrus. Using intracranial high-density cortical surface arrays, we found that listening to synthesized speech stimuli varying in small and acoustically equal steps evoked distinct and invariant cortical population response patterns that were organized by their sensitivities to critical acoustic features. Phonetic category boundaries were similar between neurometric and psychometric functions. Although speech-sound responses were distributed, spatially discrete cortical loci were found to underlie specific phonetic discrimination. Our results provide direct evidence for acoustic-to-higher order phonetic level encoding of speech sounds in human language receptive cortex.},
  pmid = {20890293},
  file = {/Users/jonny/Dropbox/papers/zotero/C/ChangE/chang_2010_categorical_speech_representation_in_human_superior_temporal_gyrus.pdf}
}

@inproceedings{Chetouani2002,
  title = {Discriminative Training for Neural Predictive Coding Applied to Speech Features Extraction},
  booktitle = {Proceedings of the 2002 {{International Joint Conference}} on {{Neural Networks}}. {{IJCNN}}'02 ({{Cat}}. {{No}}.{{02CH37290}})},
  author = {Chetouani, M. and Gas, B. and Zarader, J.L. and Chavy, C.},
  year = {2002},
  pages = {852-857 vol.1},
  publisher = {{IEEE}},
  doi = {10.1109/IJCNN.2002.1005585},
  url = {http://ieeexplore.ieee.org/document/1005585/},
  urldate = {2017-01-26},
  isbn = {0-7803-7278-6},
  keywords = {\#nosource}
}

@article{Chevillet2013,
  title = {Automatic {{Phoneme Category Selectivity}} in the {{Dorsal Auditory Stream}}},
  author = {Chevillet, Mark A. and Jiang, Xiong and Rauschecker, Josef P. and Riesenhuber, Maximilian},
  year = {2013},
  journal = {Journal of Neuroscience},
  volume = {33},
  number = {12},
  url = {http://www.jneurosci.org/content/33/12/5208.short},
  urldate = {2017-08-08},
  keywords = {\#nosource}
}

@article{Clauset2007a,
  title = {Power-Law Distributions in Empirical Data},
  author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, M. E. J.},
  year = {2007},
  month = jun,
  eprint = {0706.1062},
  eprinttype = {arxiv},
  doi = {10.1137/070710111},
  url = {http://arxiv.org/abs/0706.1062},
  urldate = {2017-05-05},
  abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.},
  archiveprefix = {arXiv},
  file = {/Users/jonny/Papers/ClausetA/2007/Clauset_2007_Power-law distributions in empirical data.pdf}
}

@article{clayardsDifferencesCueWeights2018,
  title = {Differences in Cue Weights for Speech Perception Are Correlated for Individuals within and across Contrasts},
  author = {Clayards, Meghan},
  year = {2018},
  month = sep,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {3},
  pages = {EL172-EL177},
  issn = {0001-4966},
  doi = {10.1121/1.5052025},
  url = {https://asa.scitation.org/doi/10.1121/1.5052025},
  urldate = {2019-01-22},
  abstract = {Speech perception requires multiple acoustic cues. Cue weighting may differ across individuals but be systematic within individuals. The current study compared individuals' cue weights within and across contrasts. Forty-two listeners performed a two-alternative forced choice task for four out of five sets of minimal pairs, each varying orthogonally in two dimensions. Individuals' cue weights within a contrast were positively correlated for bet-bat, Luce-lose, and sock-shock, but not for bog-dog and dear-tear. Importantly, individuals' cue weights were also positively correlated across contrasts. This indicates that some individuals are better able to extract and use phonetic information across different dimensions.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/C/ClayardsM/clayards_2018_differences_in_cue_weights_for_speech_perception_are_correlated_for_individuals.pdf;/Users/jonny/Zotero/storage/8FAQBUMB/Clayards - 2018 - Differences in cue weights for speech perception a.pdf;/Users/jonny/Zotero/storage/KTIAW3S9/1.html}
}

@article{Clerkin2016,
  title = {Real-World Visual Statistics and Infants' First-Learned Object Names},
  author = {Clerkin, Elizabeth M. and Hart, Elizabeth and Rehg, James M. and Yu, Chen and Smith, Linda B.},
  year = {2016},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {372},
  number = {1711},
  url = {http://rstb.royalsocietypublishing.org/content/372/1711/20160055.article-info},
  urldate = {2017-05-05},
  keywords = {\#nosource}
}

@article{Clerkin2016a,
  title = {Real-World Visual Statistics and Infants' First-Learned Object Names},
  author = {Clerkin, Elizabeth M. and Hart, Elizabeth and Rehg, James M. and Yu, Chen and Smith, Linda B.},
  year = {2016},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {372},
  number = {1711},
  url = {http://rstb.royalsocietypublishing.org/content/372/1711/20160055.long},
  urldate = {2017-05-03},
  file = {/Users/jonny/Papers/ClerkinE/2016/Clerkin_2016_Real-world visual statistics and infants' first-learned object names.pdf}
}

@article{Clevert2015,
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  year = {2015},
  month = nov,
  eprint = {1511.07289},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.07289},
  urldate = {2017-03-24},
  abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
  archiveprefix = {arXiv},
  file = {/Users/jonny/Papers/ClevertD/2015/Clevert_2015_Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)2.pdf}
}

@book{Collins2003,
  title = {Practical Phonetics and Phonology : A Resource Book for Students},
  author = {Collins, Beverley and Mees, Inger},
  year = {2003},
  journal = {Routledge English language introductions series},
  abstract = {Routledge English Language Introductions cover core areas of language study and are one-stop resources for students.Assuming no prior knowledge, books in the series offer an accessible overview of the subject, with activities, study questions, sample analyses, commentaries and key readings\textemdash all in the same volume. The innovative and flexible 'two-dimensional' structure is built around four sections\textemdash introduction, development, exploration and extension\textemdash which offer self-contained stages for study. Each topic can also be read across these sections, enabling the reader to build gradually on the knowledge gained. Revised and updated throughout, this third edition of Practical Phonetics and Phonology:presents the essentials of the subject and their day-to-day applications in an engaging and accessible manner covers all the core concepts of speech science, such as the phoneme, syllable structure, production of speech, vowel and consonant possibilities, glottal settings, stress, rhythm, intonation and the surprises of connected speechincorporates classic readings from key names in the discipline including David Abercrombie, David Crystal, Dennis Fry, Daniel Jones, Peter Ladefoged, Peter Trudgill and John Wellsincludes an audio CD containing a collection of samples provided by genuine speakers of 25 accent varieties from Britain, Ireland, the USA, Canada, Australia, New Zealand, South Africa, India, Singapore and West Africagives outlines of the sound systems of six key languages from around the worldcontains over a hundred activity exercises, many accompanied by audio materialis accompanied by a brand new companion website featuring additional guidance, audio files, keys to activities in the book, further exercises and activities, and extra practice in phonemic transcriptionNew features of this edition include an additional reading on teaching pronunciation, phonetic descriptions of three more languages (Japanese, Polish and Italian), expanded material on spelling/sound relationships, more information on acquiring the pronunciation of a foreign language, additional suggestions for further reading and much new illustrative material. Written by authors who are experienced teachers and researchers, this best-selling textbook will appeal to all students of English language and linguistics and those training for a certificate in TEFL.},
  isbn = {0415261333 (cased) 0415261341 (pbk.)},
  keywords = {\#nosource,English language Phonetics.}
}

@misc{Dahl2016,
  title = {Xtable: {{Export Tables}} to {{LaTeX}} or {{HTML}}},
  author = {Dahl, David B.},
  year = {2016},
  keywords = {\#nosource}
}

@article{Davis2007,
  title = {Hearing Speech Sounds: {{Top-down}} Influences on the Interface between Audition and Speech Perception},
  author = {Davis, Matthew H. and Johnsrude, Ingrid S.},
  year = {2007},
  journal = {Hearing Research},
  volume = {229},
  number = {1-2},
  pages = {132--147},
  issn = {03785955},
  doi = {10.1016/j.heares.2007.01.014},
  abstract = {This paper focuses on the cognitive and neural mechanisms of speech perception: the rapid, and highly automatic processes by which complex time-varying speech signals are perceived as sequences of meaningful linguistic units. We will review four processes that contribute to the perception of speech: perceptual grouping, lexical segmentation, perceptual learning and categorical perception, in each case presenting perceptual evidence to support highly interactive processes with top-down information flow driving and constraining interpretations of spoken input. The cognitive and neural underpinnings of these interactive processes appear to depend on two distinct representations of heard speech: an auditory, echoic representation of incoming speech, and a motoric/somatotopic representation of speech as it would be produced. We review the neuroanatomical system supporting these two key properties of speech perception and discuss how this system incorporates interactive processes and two parallel echoic and somato-motoric representations, drawing on evidence from functional neuroimaging studies in humans and from comparative anatomical studies. We propose that top-down interactive mechanisms within auditory networks play an important role in explaining the perception of spoken language. \textcopyright{} 2007 Elsevier B.V. All rights reserved.},
  isbn = {0378-5955 (Print)\textbackslash r0378-5955 (Linking)},
  pmid = {17317056},
  keywords = {Auditory cortex,Categorical perception,Feedback,fMRI,Frontal lobe,Lexical segmentation,Perceptual grouping,Perceptual learning,Speech perception,Temporal lobe},
  file = {/Users/jonny/Zotero/storage/VD9N4VH5/Davis, Johnsrude - 2007 - Hearing speech sounds Top-down influences on the interface between audition and speech perception(3).pdf}
}

@article{Diehl2004,
  title = {Speech {{Perception}}},
  author = {Diehl, Randy L. and Lotto, Andrew J. and Holt, Lori L.},
  year = {2004},
  month = feb,
  journal = {Annual Review of Psychology},
  volume = {55},
  number = {1},
  pages = {149--179},
  issn = {0066-4308},
  doi = {10.1146/annurev.psych.55.090902.142028},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/14744213},
  urldate = {2017-01-23},
  abstract = {This chapter focuses on one of the first steps in comprehending spoken language: How do listeners extract the most fundamental linguistic elements-consonants and vowels, or the distinctive features which compose them-from the acoustic signal? We begin by describing three major theoretical perspectives on the perception of speech. Then we review several lines of research that are relevant to distinguishing these perspectives. The research topics surveyed include categorical perception, phonetic context effects, learning of speech and related nonspeech categories, and the relation between speech perception and production. Finally, we describe challenges facing each of the major theoretical perspectives on speech perception.},
  pmid = {14744213}
}

@article{Dooling1995,
  title = {Discrimination of Synthetic Full-Formant and Sinewave /Ra\textendash La/ Continua by Budgerigars ({{Melopsittacus}} Undulatus) and Zebra Finches ({{Taeniopygia}} Guttata)},
  author = {Dooling, Robert J and Best, Catherine T. and Brown, Susan D.},
  year = {1995},
  journal = {The Journal of the Acoustical Society of America},
  volume = {97},
  number = {3},
  pages = {1839--1846},
  issn = {00014966},
  doi = {10.1121/1.412058},
  url = {http://scitation.aip.org/content/asa/journal/jasa/97/3/10.1121/1.412058},
  abstract = {Discrimination of three synthetic versions of a /ra\textendash la/ speech continuum was studied in two species of birds. The stimuli used in these experiments were identical to those used in a previous study of speech perception by humans [Best et al., Percept. Psychophys. 45, 237\textendash 250 (1989)]. Budgerigars and zebra finches were trained using operant conditioning and tested on three different series of acoustic stimuli: three-formant synthetic speech, sinewave versions of those tokens, and isolated F3 tones from the sinewave speech. Both species showed enhanced discrimination performance near the /l/\textendash/r/ boundary in the full-formant speech continuum, whereas for the F3 continuum, neither species showed a peak near this boundary. These results are similar to human discrimination of the same continua. Budgerigars also showed a peak in discrimination of the sinewave analog continuum paralleling that for full-formant syllables, similar to humans who are induced to perceive sinewave speech as speech. Zebra finches, by contrast, showed a relatively flat function mirroring their performance for F3 sinewaves, similar to humans who are induced to perceive sinewave speech as nonspeech. These data provide new evidence of species similarities and differences in the discrimination of speech and speechlike sounds. These data also strengthen and refine previous findings on the sensitivities of the vertebrate auditory system to the acoustic distinctions between speechsound categories.},
  isbn = {0001-4966 (Print)},
  pmid = {7699165},
  keywords = {\#nosource}
}

@article{Dorman1977,
  title = {Stop-Consonant Recognition: {{Release}} Bursts and Formant Transitions as Functionally Equivalent, Context-Dependent Cues},
  author = {Dorman, M. F. and {Studdert-Kennedy}, M. and Raphael, L. J.},
  year = {1977},
  month = mar,
  journal = {Perception \& Psychophysics},
  volume = {22},
  number = {2},
  pages = {109--122},
  publisher = {{Springer-Verlag}},
  issn = {0031-5117},
  doi = {10.3758/BF03198744},
  url = {http://www.springerlink.com/index/10.3758/BF03198744},
  urldate = {2017-12-22},
  file = {/Users/jonny/Papers/DormanM/1977/Dorman_1977_Stop-consonant recognition.pdf}
}

@article{Dresher2008,
  title = {The Contrastive Hierarchy in Phonology},
  author = {Dresher, B Elan},
  year = {2008},
  journal = {Contrast in phonology: theory, perception, acquisition},
  volume = {13},
  pages = {11},
  issn = {1718-3510},
  doi = {10.1017/CBO9780511642005},
  abstract = {I will show that phonologists have vacillated between two different and incompatible approaches to determining whether a feature is contrastive in any particular phoneme. One approach involves extracting contrastive features from fully-specified minimal pairs. I will show that this approach is provably untenable. A second approach arrives at contrastive specifications by ordering features into a hierarchy, and splitting up the inventory by successive divisions until all phonemes have been distinguished. I will show that this hierarchical approach solves the problems encountered by the minimal-pairs method. Moreover, a hierarchical approach to contrastiveness is implicit in much descriptive phonological practice, and can be found even in the work of theorists who argue against it. Given the centrality of the issue, it is remarkable that it has received almost no attention in the literature. Recovering this missing chapter of phonological theory sheds new light on a number of controversies over contrast in phonology.},
  isbn = {9780521889735},
  keywords = {_tablet,\#nosource},
  file = {/Users/jonny/Dropbox/papers/zotero/D/DresherB/dresher_2008_the_contrastive_hierarchy_in_phonology.pdf}
}

@article{Eimas1973,
  title = {Selective Adaptation of Linguistic Feature Detectors},
  author = {Eimas, Peter D. and Corbit, John D.},
  year = {1973},
  journal = {Cognitive Psychology},
  volume = {4},
  number = {1},
  pages = {99--109},
  issn = {00100285},
  doi = {10.1016/0010-0285(73)90006-6},
  url = {http://www.sciencedirect.com/science/article/pii/0010028573900066},
  urldate = {2017-05-05},
  abstract = {Using a selective adaptation procedure, evidence was obtained for the existence of linguistic feature detectors, analogous to visual feature detectors. These detectors are each sensitive to a restricted range of voice onset times, the physical continuum underlying the perceived phonetic distinctions between voiced and voiceless stop consonants. The sensitivity of a particular detector can be reduced selectively by repetitive presentation of its adequate stimulus. This results in a shift in the locus of the phonetic boundary separating the voiced and voiceless stops.},
  keywords = {\#nosource}
}

@article{Elman1988,
  title = {Learning the Hidden Structure of Speech.},
  author = {Elman, J L and Zipser, D},
  year = {1988},
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {83},
  number = {4},
  pages = {1615--26},
  issn = {0001-4966},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/3372872},
  urldate = {2017-01-23},
  abstract = {In the work described here, the backpropagation neural network learning procedure is applied to the analysis and recognition of speech. This procedure takes a set of input/output pattern pairs and attempts to learn their functional relationship; it develops the necessary representational features during the course of learning. A series of computer simulation studies was carried out to assess the ability of these networks to accurately label sounds, to learn to recognize sounds without labels, and to learn feature representations of continuous speech. These studies demonstrated that the networks can learn to label presegmented test tokens with accuracies of up to 95\%. Networks trained on segmented sounds using a strategy that requires no external labels were able to recognize and delineate sounds in continuous speech. These networks developed rich internal representations that included units which corresponded to such traditional distinctions as vowels and consonants, as well as units that were sensitive to novel and nonstandard features. Networks trained on a large corpus of unsegmented, continuous speech without labels also developed interesting feature representations, which may be useful in both segmentation and label learning. The results of these studies, while preliminary, demonstrate that backpropagation learning can be used with complex, natural data to identify a feature structure that can serve as the basis for both analysis and nontrivial pattern recognition.},
  pmid = {3372872},
  keywords = {\#nosource}
}

@article{Engineer2008,
  title = {Cortical Activity Patterns Predict Speech Discrimination Ability.},
  author = {Engineer, Crystal T and Perez, Claudia A and Chen, YeTing H and Carraway, Ryan S and Reed, Amanda C and Shetake, Jai A and Jakkamsetti, Vikram and Chang, Kevin Q and Kilgard, Michael P},
  year = {2008},
  month = may,
  journal = {Nature neuroscience},
  volume = {11},
  number = {5},
  pages = {603--8},
  publisher = {{NIH Public Access}},
  issn = {1097-6256},
  doi = {10.1038/nn.2109},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/18425123},
  urldate = {2017-01-26},
  abstract = {Neural activity in the cerebral cortex can explain many aspects of sensory perception. Extensive psychophysical and neurophysiological studies of visual motion and vibrotactile processing show that the firing rate of cortical neurons averaged across 50-500 ms is well correlated with discrimination ability. In this study, we tested the hypothesis that primary auditory cortex (A1) neurons use temporal precision on the order of 1-10 ms to represent speech sounds shifted into the rat hearing range. Neural discrimination was highly correlated with behavioral performance on 11 consonant-discrimination tasks when spike timing was preserved and was not correlated when spike timing was eliminated. This result suggests that spike timing contributes to the auditory cortex representation of consonant sounds.},
  pmid = {18425123},
  file = {/Users/jonny/Zotero/storage/ERKARKPT/Engineer et al. - 2008 - Cortical activity patterns predict speech discrimination ability(3).pdf}
}

@article{Engineer2015,
  title = {Speech Training Alters Consonant and Vowel Responses in Multiple Auditory Cortex Fields},
  author = {Engineer, Crystal T. and Rahebi, Kimiya C. and Buell, Elizabeth P. and Fink, Melyssa K. and Kilgard, Michael P.},
  year = {2015},
  journal = {Behavioural Brain Research},
  volume = {287},
  pages = {256--264},
  publisher = {{Elsevier B.V.}},
  issn = {18727549},
  doi = {10.1016/j.bbr.2015.03.044},
  url = {http://dx.doi.org/10.1016/j.bbr.2015.03.044},
  urldate = {2016-09-14},
  abstract = {Speech sounds evoke unique neural activity patterns in primary auditory cortex (A1). Extensive speech sound discrimination training alters A1 responses. While the neighboring auditory cortical fields each contain information about speech sound identity, each field processes speech sounds differently. We hypothesized that while all fields would exhibit training-induced plasticity following speech training, there would be unique differences in how each field changes. In this study, rats were trained to discriminate speech sounds by consonant or vowel in quiet and in varying levels of background speech-shaped noise. Local field potential and multiunit responses were recorded from four auditory cortex fields in rats that had received 10 weeks of speech discrimination training. Our results reveal that training alters speech evoked responses in each of the auditory fields tested. The neural response to consonants was significantly stronger in anterior auditory field (AAF) and A1 following speech training. The neural response to vowels following speech training was significantly weaker in ventral auditory field (VAF) and posterior auditory field (PAF). This differential plasticity of consonant and vowel sound responses may result from the greater paired pulse depression, expanded low frequency tuning, reduced frequency selectivity, and lower tone thresholds, which occurred across the four auditory fields. These findings suggest that alterations in the distributed processing of behaviorally relevant sounds may contribute to robust speech discrimination.},
  isbn = {1872-7549 (Electronic) 0166-4328 (Linking)},
  pmid = {25827927},
  keywords = {Auditory processing,Map reorganization,Receptive field plasticity,Speech therapy},
  file = {/Users/jonny/Dropbox/papers/zotero/E/EngineerC/engineer_2015_speech_training_alters_consonant_and_vowel_responses_in_multiple_auditory.pdf}
}

@article{Erickson1998,
  title = {Rules and {{Exemplars}} in {{Category Learning}}},
  author = {Erickson, Michael A. and Kruschke, John K.},
  year = {1998},
  journal = {Journal of Experimental Psychology: General},
  volume = {127},
  number = {2},
  pages = {107--140},
  issn = {0096-3445},
  doi = {10.1037/0096-3445.127.2.107},
  abstract = {Psychological theories of categorization generally focus on either rule- or exemplar-based\textbackslash r\textbackslash nexplanations. We present 2 experiments that show evidence of both rule induction and\textbackslash r\textbackslash nexemplar encoding as well as a connectionist model, ATRn.rM, that specifies a mechanism for\textbackslash r\textbackslash ncombining rule- and exemplar-based representation. In 2 experiments participants learned to\textbackslash r\textbackslash nclassify items, most of which followed a simple rule, although there were a few frequently\textbackslash r\textbackslash noccurring exceptions. Experiment 1 examined how people extrapolate beyond the range of\textbackslash r\textbackslash ntraining. Experiment 2 examined the effect of instance frequency on generalization.\textbackslash r\textbackslash nCategorization behavior was well described by the model, in which exemplar representation is\textbackslash r\textbackslash nused for both rule and exception processing. A key element in correctly modeling these results\textbackslash r\textbackslash nwas capturing the interaction between the rule- and exemplar-based representations by using\textbackslash r\textbackslash nshifts of attention between rules and exemplars.},
  isbn = {0096-3445; 1939-2222},
  pmid = {9622910}
}

@article{Estes2015,
  title = {Listening through Voices: {{Infant}} Statistical Word Segmentation across Multiple Speakers.},
  author = {Estes, Katharine Graf and {Lew-Williams}, Casey},
  year = {2015},
  month = nov,
  journal = {Developmental Psychology},
  volume = {51},
  number = {11},
  pages = {1517--1528},
  issn = {1939-0599},
  doi = {10.1037/a0039725},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/26389607},
  urldate = {2017-08-08},
  abstract = {To learn from their environments, infants must detect structure behind pervasive variation. This presents substantial and largely untested learning challenges in early language acquisition. The current experiments address whether infants can use statistical learning mechanisms to segment words when the speech signal contains acoustic variation produced by changes in speakers' voices. In Experiment 1, 8- and 10-month-old infants listened to a continuous stream of novel words produced by 8 different female voices. The voices alternated frequently, potentially interrupting infants' detection of transitional probability patterns that mark word boundaries. Infants at both ages successfully segmented words in the speech stream. In Experiment 2, 8-month-olds demonstrated the ability to generalize their learning about the speech stream when presented with a new, acoustically distinct voice during testing. However, in Experiments 3 and 4, when the same speech stream was produced by only 2 female voices, infants failed to segment the words. The results of these experiments indicate that low acoustic variation may interfere with infants' efficiency in segmenting words from continuous speech, but that infants successfully use statistical cues to segment words in conditions of high acoustic variation. These findings contribute to our understanding of whether statistical learning mechanisms can scale up to meet the demands of natural learning environments.},
  pmid = {26389607},
  file = {/Users/jonny/Papers/EstesK/2015/Estes_2015_Listening through voices.pdf}
}

@incollection{Farnetani1990,
  title = {V-{{C-V Lingual Coarticulation}} and {{Its Spatiotemporal Domain}}},
  booktitle = {Speech {{Production}} and {{Speech Modelling}}},
  author = {Farnetani, E.},
  year = {1990},
  pages = {93--130},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-009-2037-8_5},
  url = {http://www.springerlink.com/index/10.1007/978-94-009-2037-8_5},
  urldate = {2017-02-14},
  file = {/Users/jonny/Papers/FarnetaniE/1990/Farnetani_1990_V-C-V Lingual Coarticulation and Its Spatiotemporal Domain.pdf}
}

@article{Feldman2009,
  title = {The Influence of Categories on Perception: {{Explaining}} the Perceptual Magnet Effect as Optimal Statistical Inference.},
  author = {Feldman, Naomi H. and Griffiths, Thomas L. and Morgan, James L.},
  year = {2009},
  journal = {Psychological Review},
  volume = {116},
  number = {4},
  pages = {752--782},
  publisher = {{American Psychological Association}},
  issn = {1939-1471},
  doi = {10.1037/a0017196},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0017196},
  urldate = {2017-01-20},
  keywords = {Bayesian inference,categorical perception,noise,perceptual magnet effect,phonetic categorization,rational analysis,speech perception},
  file = {/Users/jonny/Zotero/storage/PH8IPU29/Feldman, Griffiths, Morgan - 2009 - The influence of categories on perception Explaining the perceptual magnet effect as optimal stat(3).pdf}
}

@article{fengRoleHumanAuditory2018,
  title = {The {{Role}} of the {{Human Auditory Corticostriatal Network}} in {{Speech Learning}}},
  author = {Feng, Gangyi and Yi, Han Gyol and Chandrasekaran, Bharath},
  year = {2018},
  month = dec,
  journal = {Cerebral Cortex (New York, N.Y.: 1991)},
  issn = {1460-2199},
  doi = {10.1093/cercor/bhy289},
  abstract = {We establish a mechanistic account of how the mature human brain functionally reorganizes to acquire and represent new speech sounds. Native speakers of English learned to categorize Mandarin lexical tone categories produced by multiple talkers using trial-by-trial feedback. We hypothesized that the corticostriatal system is a key intermediary in mediating temporal lobe plasticity and the acquisition of new speech categories in adulthood. We conducted a functional magnetic resonance imaging experiment in which participants underwent a sound-to-category mapping task. Diffusion tensor imaging data were collected, and probabilistic fiber tracking analysis was employed to assay the auditory corticostriatal pathways. Multivariate pattern analysis showed that talker-invariant novel tone category representations emerged in the left superior temporal gyrus (LSTG) within a few hundred training trials. Univariate analysis showed that the putamen, a subregion of the striatum, was sensitive to positive feedback in correctly categorized trials. With learning, functional coupling between the putamen and LSTG increased during error processing. Furthermore, fiber tractography demonstrated robust structural connectivity between the feedback-sensitive striatal regions and the LSTG regions that represent the newly learned tone categories. Our convergent findings highlight a critical role for the auditory corticostriatal circuitry in mediating the acquisition of new speech categories.},
  langid = {english},
  pmid = {30535138},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/F/FengG/feng_2018_the_role_of_the_human_auditory_corticostriatal_network_in_speech_learning.pdf}
}

@book{Feynman1998,
  title = {Feynman {{Lectures}} on {{Computation}}},
  author = {Feynman, Richard P.},
  editor = {Hey, J.G. and Allen, Robin W.},
  year = {1998},
  publisher = {{Addison-Wesley Longman Publishing Co.}},
  address = {{Boston, MA}},
  abstract = {From 1983 to 1986, the legendary physicist and teacher Richard Feynman gave a course at Caltech called "Potentialities and Limitations of Computing Machines." Although the lectures are over ten years old, most of the material is timeless and presents a "Feynmanesque" overview of many standard and some not-so-standard topics in computer science. These include compatibility, Turing machines (or as Feynman said, "Mr. Turing's machines"), information theory, Shannon's Theorem, reversible computation, the thermodynamics of computation, the quantum limits to computation, and the physics of VLSI devices. Taken together, these lectures represent a unique exploration of the fundamental limitations of digital computers. Feynman's philosophy of learning and discovery comes through strongly in these lectures. He constantly points out the benefits of playing around with concepts and working out solutions to problems on your own - before looking at the back of the book for the answers. As Feynman says in the lectures: "If you keep proving stuff that others have done, getting confidence, increasing the complexities of your solutions - for the fun of it - then one day you'll turn around and discover that nobody actually did that one! And that's the way to become a computer scientist."},
  isbn = {0-201-38628-3},
  keywords = {\#nosource}
}

@article{Fitch2000,
  title = {The Evolution of Speech: A Comparative Review},
  author = {Fitch, W.Tecumseh},
  year = {2000},
  journal = {Trends in Cognitive Sciences},
  volume = {4},
  number = {7},
  pages = {258--267},
  issn = {13646613},
  doi = {10.1016/S1364-6613(00)01494-7},
  abstract = {The evolution of speech can be studied independently of the evolution of language, with the advantage that most aspects of speech acoustics, physiology and neural control are shared with animals, and thus open to empirical investigation. At least two changes were necessary prerequisites for modern human speech abilities: (1) modification of vocal tract morphology, and (2) development of vocal imitative ability. Despite an extensive literature, attempts to pinpoint the timing of these changes using fossil data have proven inconclusive. However, recent comparative data from nonhuman primates have shed light on the ancestral use of formants (a crucial cue in human speech) to identify individuals and gauge body size. Second, comparative analysis of the diverse vertebrates that have evolved vocal imitation (humans, cetaceans, seals and birds) provides several distinct, testable hypotheses about the adaptive function of vocal mimicry. These developments suggest that, for understanding the evolution of speech, comparative analysis of living species provides a viable alternative to fossil data. However, the neural basis for vocal mimicry and for mimesis in general remains unknown.},
  file = {/Users/jonny/Zotero/storage/DKSNJX3Q/Fitch - 2000 - The evolution of speech a comparative review(3).pdf}
}

@incollection{Flemming2005,
  title = {Speech {{Perception}} and {{Phonological Contrast}}},
  booktitle = {The {{Handbook}} of {{Speech Perception}}},
  author = {Flemming, Edward},
  year = {2005},
  pages = {156--181},
  publisher = {{Blackwell Publishing Ltd}},
  address = {{Oxford, UK}},
  doi = {10.1002/9780470757024.ch7},
  url = {http://doi.wiley.com/10.1002/9780470757024.ch7},
  urldate = {2017-02-19},
  isbn = {978-0-470-75702-4},
  keywords = {optimality theory,phonetic descriptions,phonological contrast,phonological patterns,speech perception},
  file = {/Users/jonny/Papers/FlemmingE/2005/Flemming_2005_Speech Perception and Phonological Contrast2.pdf}
}

@article{Fox2016,
  title = {Top-down Effects of Syntactic Sentential Context on Phonetic Processing.},
  author = {Fox, Neal P. and Blumstein, Sheila E.},
  year = {2016},
  month = may,
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {42},
  number = {5},
  pages = {730--741},
  issn = {1939-1277},
  doi = {10.1037/a0039965},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/26689310},
  urldate = {2017-05-01},
  abstract = {Although much evidence suggests that the identification of phonetically ambiguous target words can be biased by preceding sentential context, interactive and autonomous models of speech perception disagree as to the mechanism by which higher level information affects subjects' responses. Some have suggested that the time course of context effects is incompatible with interactive models (e.g., TRACE). Two experiments examine this issue. In Experiment 1, subjects heard noun- and verb-biasing sentence contexts (e.g., Valerie hated the . . . vs. Brett hated to . . .), followed by stimuli from 2 voice-onset time continua: bay-pay (noun-verb) versus buy-pie (verb-noun). Consistent with prior research, identification of phonetically ambiguous targets was biased by the preceding context, and the size of this bias diminished in slower compared with faster responses. In Experiment 2, tokens from the same continua were embedded among filler target words beginning with /b/ or /p/ to elicit phonemically driven identification decisions and discourage word-level strategies. Results again revealed contextually biased responding, but this bias was as strong in slow as in fast responses. Together, these results suggest that phoneme identification decisions reflect robust, lasting top-down effects of lexical feedback on prelexical representations, as predicted by interactive models of speech perception.},
  pmid = {26689310}
}

@article{Fritz2003,
  title = {Active Listening: {{Task-dependent}} Plasticity of Spectrotemporal Receptive Fields in Primary Auditory Cortex},
  author = {Fritz, Jonathan and Elhilali, Mounya and Shamma, Shihab},
  year = {2005},
  journal = {Hearing Research},
  volume = {206},
  number = {1-2},
  pages = {159--176},
  issn = {03785955},
  doi = {10.1016/j.heares.2005.01.015},
  url = {http://www.nature.com/doifinder/10.1038/nn1141},
  abstract = {Listening is an active process in which attentive focus on salient acoustic features in auditory tasks can influence receptive field properties of cortical neurons. Recent studies showing rapid task-related changes in neuronal spectrotemporal receptive fields (STRFs) in primary auditory cortex of the behaving ferret are reviewed in the context of current research on cortical plasticity. Ferrets were trained on spectral tasks, including tone detection and two-tone discrimination, and on temporal tasks, including gap detection and click-rate discrimination. STRF changes could be measured on-line during task performance and occurred within minutes of task onset. During spectral tasks, there were specific spectral changes (enhanced response to tonal target frequency in tone detection and discrimination, suppressed response to tonal reference frequency in tone discrimination). However, only in the temporal tasks, the STRF was changed along the temporal dimension by sharpening temporal dynamics. In ferrets trained on multiple tasks, distinctive and task-specific STRF changes could be observed in the same cortical neurons in successive behavioral sessions. These results suggest that rapid task-related plasticity is an ongoing process that occurs at a network and single unit level as the animal switches between different tasks and dynamically adapts cortical STRFs in response to changing acoustic demands. ?? 2005 Elsevier B.V. All rights reserved.},
  isbn = {0378-5955 (Print)\textbackslash r0378-5955 (Linking)},
  pmid = {16081006},
  keywords = {Adaptive,Attention,Auditory,Behavior,Cortex,Plasticity},
  file = {/Users/jonny/Zotero/storage/PTQ7UE82/Fritz, Elhilali, Shamma - 2005 - Active listening Task-dependent plasticity of spectrotemporal receptive fields in primary auditory c(3).pdf}
}

@article{Froemke2007,
  title = {A Synaptic Memory Trace for Cortical Receptive Field Plasticity.},
  author = {Froemke, Robert C and Merzenich, Michael M and Schreiner, Christoph E},
  year = {2007},
  journal = {Nature},
  volume = {450},
  number = {7168},
  pages = {425--429},
  issn = {0028-0836},
  doi = {10.1038/nature06289},
  url = {http://dx.doi.org/10.1038/nature06289},
  abstract = {Receptive fields of sensory cortical neurons are plastic, changing in response to alterations of neural activity or sensory experience. In this way, cortical representations of the sensory environment can incorporate new information about the world, depending on the relevance or value of particular stimuli. Neuromodulation is required for cortical plasticity, but it is uncertain how subcortical neuromodulatory systems, such as the cholinergic nucleus basalis, interact with and refine cortical circuits. Here we determine the dynamics of synaptic receptive field plasticity in the adult primary auditory cortex (also known as AI) using in vivo whole-cell recording. Pairing sensory stimulation with nucleus basalis activation shifted the preferred stimuli of cortical neurons by inducing a rapid reduction of synaptic inhibition within seconds, which was followed by a large increase in excitation, both specific to the paired stimulus. Although nucleus basalis was stimulated only for a few minutes, reorganization of synaptic tuning curves progressed for hours thereafter: inhibition slowly increased in an activity-dependent manner to rebalance the persistent enhancement of excitation, leading to a retuned receptive field with new preference for the paired stimulus. This restricted period of disinhibition may be a fundamental mechanism for receptive field plasticity, and could serve as a memory trace for stimuli or episodes that have acquired new behavioural significance.},
  isbn = {1476-4687 (Electronic)\textbackslash r0028-0836 (Linking)},
  pmid = {18004384},
  keywords = {Afferent,Animals,Auditory Cortex,cytology/physiology,Excitatory Postsynaptic Pote,Female,Memory,metabolism,Neuronal Plasticity,Neurons,physiology,Rats,Sprague-Dawley,Synapses,Time Factors},
  file = {/Users/jonny/Dropbox/papers/zotero/F/FroemkeR/froemke_2007_a_synaptic_memory_trace_for_cortical_receptive_field_plasticity.pdf}
}

@article{Fugate2013,
  title = {Categorical {{Perception}} for {{Emotional Faces}}},
  author = {Fugate, Jennifer M. B.},
  year = {2013},
  month = jan,
  journal = {Emotion Review},
  volume = {5},
  number = {1},
  pages = {84--89},
  publisher = {{SAGE PublicationsSage UK: London, England}},
  issn = {1754-0739},
  doi = {10.1177/1754073912451350},
  url = {http://journals.sagepub.com/doi/10.1177/1754073912451350},
  urldate = {2017-01-20},
  abstract = {Categorical perception (CP) refers to how similar things look different depending on whether they are classified as the same category. Many studies demonstrate that adult humans show CP for human emotional faces. It is widely debated whether the effect can be accounted for solely by perceptual differences (structural differences among emotional faces) or whether additional perceiver-based conceptual knowledge is required. In this review, I discuss the phenomenon of CP and key studies showing CP for emotional faces. I then discuss a new model of emotion which highlights how perceptual and conceptual knowledge interact to explain how people see discrete emotions in others' faces. In doing so, I discuss how language (emotion words included in the paradigm) contribute to CP.},
  keywords = {categorical perception,emotional faces,language},
  file = {/Users/jonny/Zotero/storage/B2UWQ75W/Fugate - 2013 - Categorical Perception for Emotional Faces(3).pdf}
}

@techreport{Gagnepain2012,
  title = {Temporal {{Predictive Codes}} for {{Spoken Words}} in {{Auditory Cortex}}},
  author = {Gagnepain, Pierre and Henson, Richard~N. and Davis, Matthew~H.},
  year = {2012},
  journal = {Current Biology},
  volume = {22},
  number = {7},
  pages = {615--621},
  issn = {09609822},
  doi = {10.1016/j.cub.2012.02.015},
  abstract = {Humans can recognize spoken words with unmatched speed and accuracy. Hearing the initial portion of a word such as ``formu\ldots '' is sufficient for the brain to identify ``formula'' from the thousands of other words that partially match [1\textendash 6]. Two alternative computational accounts propose that partially matching words (1) inhibit each other until a single word is selected (``formula'' inhibits ``formal'' by lexical competition [7\textendash 9]) or (2) are used to predict upcoming speech sounds more accurately (segment prediction error is minimal after sequences like ``formu\ldots '' [10\textendash 12]). To distinguish these theories we taught participants novel words (e.g., ``formubo'') that sound like existing words (``formula'') on two successive days [13\textendash 16]. Computational simulations show that knowing ``formubo'' increases lexical competition when hearing ``formu\ldots '', but reduces segment prediction error. Conversely, when the sounds in ``formula'' and ``formubo'' diverge, the reverse is observed. The time course of magnetoencephalographic brain responses in the superior temporal gyrus (STG) is uniquely consistent with a segment prediction account. We propose a predictive coding model of spoken word recognition in which STG neurons represent the difference between predicted and heard speech sounds. This prediction error signal explains the efficiency of human word recognition and simulates neural responses in auditory regions.},
  file = {/Users/jonny/Zotero/storage/K42AS2S9/Gagnepain, Henson, Davis - 2012 - Temporal Predictive Codes for Spoken Words in Auditory Cortex(3).pdf}
}

@article{Gaskell1997,
  title = {Integrating {{Form}} and {{Meaning}}: {{A Distributed Model}} of {{Speech Perception}}},
  author = {Gaskell, M. Gareth and {Marslen-Wilson}, William D.},
  year = {1997},
  month = oct,
  journal = {Language and Cognitive Processes},
  volume = {12},
  number = {5-6},
  pages = {613--656},
  publisher = {{Taylor \& Francis Group}},
  issn = {0169-0965},
  doi = {10.1080/016909697386646},
  url = {http://www.tandfonline.com/doi/abs/10.1080/016909697386646},
  urldate = {2017-05-26},
  abstract = {We present a new distributed connectionist model of the perception of spoken words. The model employs a representation of speech that combines lexical information with abstract phonological information, with lexical access modelled as a direct mapping onto this single distributed representation. We first examine the integration of partial cues to phonological identity, showing that the model provides a sound basis for simulating phonetic and lexical decision data from Marslen-Wilson and Warren (1994). We then investigate the time course of lexical access, and argue that the process of competition between word candidates during lexical access can be interpreted in terms of interference between distributed lexical representations. The relation between our model and other models of spoken word recognition is discussed.},
  keywords = {\#nosource}
}

@article{Ghazanfar1999,
  title = {The Neuroethology of Primate Vocal Communication: {{Substrates}} for the Evolution of Speech},
  author = {Ghazanfar, Asif A. and Hauser, Marc D.},
  year = {1999},
  journal = {Trends in Cognitive Sciences},
  volume = {3},
  number = {10},
  pages = {377--384},
  issn = {13646613},
  doi = {10.1016/S1364-6613(99)01379-0},
  abstract = {In this article, we review behavioral and neurobiological studies of the perception and use of species-specific vocalizations by non-human primates. At the behavioral level, primate vocal perception shares many features with speech perception by humans. These features include a left-hemisphere bias towards conspecific vocalizations, the use of temporal features for identifying different calls, and the use of calls to refer to objects and events in the environment. The putative neural bases for some of these behaviors have been revealed by recent studies of the primate auditory and prefrontal cortices. These studies also suggest homologies with the human language circuitry. Thus, a synthesis of cognitive, ethological and neurobiological approaches to primate vocal behavior is likely to yield the richest understanding of the neural bases of speech perception, and might also shed light on the evolutionary precursors to language.},
  isbn = {1364-6613},
  pmid = {10498928},
  keywords = {\#nosource}
}

@article{Giraud2012a,
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  author = {Giraud, Anne-Lise and Poeppel, David},
  year = {2012},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {15},
  number = {4},
  pages = {511--517},
  publisher = {{Nature Research}},
  issn = {1097-6256},
  doi = {10.1038/nn.3063},
  url = {http://www.nature.com/doifinder/10.1038/nn.3063},
  urldate = {2017-01-26},
  keywords = {\#nosource}
}

@article{Goldinger2003,
  title = {Puzzle-Solving Science: {{The}} Quixotic Quest for Units in Speech Perception},
  author = {Goldinger, Stephen D. and Azuma, Tamiko},
  year = {2003},
  journal = {Journal of Phonetics},
  volume = {31},
  number = {3-4},
  pages = {305--320},
  issn = {00954470},
  doi = {10.1016/S0095-4470(03)00030-5},
  abstract = {Although speech signals are continuous and variable, listeners experience segmentation and linguistic structure in perception. For years, researchers have tried to identify the basic building-block of speech perception. In that time, experimental methods have evolved, constraints on stimulus materials have evolved, sources of variance have been identified, and computational models have been advanced. As a result, the slate of candidate units has increased, each with its own empirical support. In this article, we endorse Grossberg's adaptive resonance theory (ART), proposing that speech units are emergent properties of perceptual dynamics. By this view, units only "exist" when disparate features achieve resonance, a level of perceptual coherence that allows conscious encoding. We outline basic principles of ART, then summarize five experiments. Three experiments assessed the power of social influence to affect phonemesyllable competitions. Two other experiments assessed repetition effects in monitoring data. Together the data suggest that "primary" speech units are strongly and symmetrically affected by bottom-up and top-down knowledge sources. ?? 2003 Elsevier Ltd. All rights reserved.},
  isbn = {0095-4470},
  pmid = {18292779},
  file = {/Users/jonny/Zotero/storage/J68RKIZ2/Goldinger, Azuma - 2003 - Puzzle-solving science The quixotic quest for units in speech perception(3).pdf}
}

@book{Goodfellow2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  url = {http://www.deeplearningbook.org/},
  urldate = {2017-03-24},
  keywords = {\#nosource}
}

@article{Gourevitch2009,
  title = {Spectrotemporal {{Receptive Fields}} in {{Anesthetized Cat Primary Auditory Cortex Are Context Dependent}}},
  author = {Gour{\'e}vitch, Boris and Nore{\~n}a, Arnaud and Shaw, Gregory and Eggermont, Jos J.},
  year = {2009},
  month = jun,
  journal = {Cerebral Cortex},
  volume = {19},
  number = {6},
  pages = {1448--1461},
  issn = {1460-2199},
  doi = {10.1093/cercor/bhn184},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/18854580},
  urldate = {2017-08-08},
  abstract = {In order to investigate how the auditory scene is analyzed and perceived, auditory spectrotemporal receptive fields (STRFs) are generally used as a convenient way to describe how frequency and temporal sound information is encoded. However, using broadband sounds to estimate STRFs imperfectly reflects the way neurons process complex stimuli like conspecific vocalizations insofar as natural sounds often show limited bandwidth. Using recordings in the primary auditory cortex of anesthetized cats, we show that presentation of narrowband stimuli not including the best frequency of neurons provokes the appearance of residual peaks and increased firing rate at some specific spectral edges of stimuli compared with classical STRFs obtained from broadband stimuli. This result is the same for STRFs obtained from both spikes and local field potentials. Potential mechanisms likely involve release from inhibition. We thus emphasize some aspects of context dependency of STRFs, that is, how the balance of inhibitory and excitatory inputs is able to shape the neural response from the spectral content of stimuli.},
  pmid = {18854580}
}

@article{greenbergSpeechDynamicProcess2018,
  title = {Speech: {{A Dynamic Process}}},
  shorttitle = {Speech},
  author = {Greenberg, Steven},
  year = {2018},
  month = dec,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {6},
  pages = {3210--3210},
  issn = {0001-4966},
  doi = {10.1121/1.5082302},
  url = {https://asa.scitation.org/doi/10.1121/1.5082302},
  urldate = {2019-01-22},
  file = {/Users/jonny/Zotero/storage/5ELRZHX4/Greenberg - 2018 - Speech A Dynamic Process.pdf;/Users/jonny/Zotero/storage/5XZFGELL/1.html}
}

@article{Grossberg2013,
  title = {Adaptive {{Resonance Theory}}: {{How}} a Brain Learns to Consciously Attend, Learn, and Recognize a Changing World},
  author = {Grossberg, Stephen},
  year = {2013},
  journal = {Neural Networks},
  volume = {37},
  pages = {1--47},
  publisher = {{Elsevier Ltd}},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.09.017},
  url = {http://dx.doi.org/10.1016/j.neunet.2012.09.017},
  abstract = {Adaptive Resonance Theory, or ART, is a cognitive and neural theory of how the brain autonomously learns to categorize, recognize, and predict objects and events in a changing world. This article reviews classical and recent developments of ART, and provides a synthesis of concepts, principles, mechanisms, architectures, and the interdisciplinary data bases that they have helped to explain and predict. The review illustrates that ART is currently the most highly developed cognitive and neural theory available, with the broadest explanatory and predictive range. Central to ART's predictive power is its ability to carry out fast, incremental, and stable unsupervised and supervised learning in response to a changing world. ART specifies mechanistic links between processes of consciousness, learning, expectation, attention, resonance, and synchrony during both unsupervised and supervised learning. ART provides functional and mechanistic explanations of such diverse topics as laminar cortical circuitry; invariant object and scenic gist learning and recognition; prototype, surface, and boundary attention; gamma and beta oscillations; learning of entorhinal grid cells and hippocampal place cells; computation of homologous spatial and temporal mechanisms in the entorhinal-hippocampal system; vigilance breakdowns during autism and medial temporal amnesia; cognitive-emotional interactions that focus attention on valued objects in an adaptively timed way; item-order-rank working memories and learned list chunks for the planning and control of sequences of linguistic, spatial, and motor information; conscious speech percepts that are influenced by future context; auditory streaming in noise during source segregation; and speaker normalization. Brain regions that are functionally described include visual and auditory neocortex; specific and nonspecific thalamic nuclei; inferotemporal, parietal, prefrontal, entorhinal, hippocampal, parahippocampal, perirhinal, and motor cortices; frontal eye fields; supplementary eye fields; amygdala; basal ganglia: cerebellum; and superior colliculus. Due to the complementary organization of the brain, ART does not describe many spatial and motor behaviors whose matching and learning laws differ from those of ART. ART algorithms for engineering and technology are listed, as are comparisons with other types of models. ?? 2012 Elsevier Ltd.},
  isbn = {0893-6080},
  pmid = {23149242},
  keywords = {Adaptive Resonance Theory,Adaptive timing,Amygdala,Attention,Basal ganglia,Consciousness,Entorhinal cortex,Expectation,Gamma and beta oscillations,Hippocampal cortex,Inferotemporal cortex,Learning,Parietal cortex,Prefrontal cortex,Recognition,Reinforcement learning,Speech perception,Synchrony,Working memory},
  file = {/Users/jonny/Zotero/storage/XJYBPBDW/Grossberg - 2013 - Adaptive Resonance Theory How a brain learns to consciously attend, learn, and recognize a changing world(3).pdf}
}

@article{hamiltonSpatialMapOnset2018,
  title = {A {{Spatial Map}} of {{Onset}} and {{Sustained Responses}} to {{Speech}} in the {{Human Superior Temporal Gyrus}}},
  author = {Hamilton, Liberty S. and Edwards, Erik and Chang, Edward F.},
  year = {2018},
  month = may,
  journal = {Current Biology},
  issn = {09609822},
  doi = {10.1016/j.cub.2018.04.033},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/29861132},
  urldate = {2018-06-14},
  abstract = {To derive meaning from speech, we must extract multiple dimensions of concurrent information from incoming speech signals. That is, equally important to processing phonetic features is the detection of acoustic cues that give structure and context to the information we hear. How the brain organizes this information is unknown. Using data-driven computational methods on high-density intracranial recordings from 27 human participants, we reveal the functional distinction of neural responses to speech in the posterior superior temporal gyrus according to either onset or sustained response profiles. Though similar response types have been observed throughout the auditory system, we found novel evidence for a major spatial parcellation in which a distinct caudal zone detects acoustic onsets and a rostral-surround zone shows sustained, relatively delayed responses to ongoing speech stimuli. While posterior onset and anterior sustained responses are used substantially during natural speech perception, they are not limited to speech stimuli and are seen even for reversed or spectrally rotated speech. Single-electrode encoding of phonetic features in each zone depended upon whether the sound occurred at sentence onset, suggesting joint encoding of phonetic features and their temporal context. Onset responses in the caudal zone could accurately decode sentence and phrase onset boundaries, providing a potentially important internal mechanism for detecting temporal landmarks in speech and other natural sounds. These findings suggest that onset and sustained responses not only define the basic spatial organization of high-order auditory cortex but also have direct implications for how speech information is parsed in the cortex. VIDEO ABSTRACT.},
  pmid = {29861132},
  keywords = {auditory,ECoG,electrocorticography,intracranial recordings,natural speech,neurolinguistics,spectrotemporal receptive field,unsupervised learning}
}

@incollection{Harnad1987,
  title = {Psychophysical and Cognitive Aspects of Categorical Perception: {{A}} Critical Overview},
  booktitle = {Categorical {{Perception}}: {{The Groundwork}} of {{Cognition}}},
  author = {Harnad, Stevan},
  year = {1987},
  pages = {1--52},
  publisher = {{Cambridge University Press}},
  abstract = {Categorization is a very basic cognitive activity. It is involved in any task that calls for differential responding, from operant discrimination to pattern recognition to naming and describing objects and states-of-affairs. Explanations of categorization range from nativist theories denying that any nontrivial categories are acquired by learning to inductivist theories claiming that most categories are learned. "Categorical perception" (CP) is the name given to a suggestive perceptual phenomenon that may serve as a useful model for categorization in general: For certain perceptual categories, within-category differences look much smaller than between-category differences even when they are of the same size physically. For example, in color perception, differences between reds and differences between yellows look much smaller than equal-sized differences that cross the red/yellow boundary; the same is true of the phoneme categories /ba/ and /da/. Indeed, the effect of the category boundary is not merely quantitative, but qualitative.},
  keywords = {\#nosource}
}

@article{healdSpeechPerceptionActive2014,
  title = {Speech Perception as an Active Cognitive Process.},
  author = {Heald, Shannon L M and Nusbaum, Howard C},
  year = {2014},
  journal = {Frontiers in systems neuroscience},
  volume = {8},
  pages = {35},
  publisher = {{Frontiers Media SA}},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2014.00035},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/24672438},
  urldate = {2017-12-18},
  abstract = {One view of speech perception is that acoustic signals are transformed into representations for pattern matching to determine linguistic structure. This process can be taken as a statistical pattern-matching problem, assuming realtively stable linguistic categories are characterized by neural representations related to auditory properties of speech that can be compared to speech input. This kind of pattern matching can be termed a passive process which implies rigidity of processing with few demands on cognitive processing. An alternative view is that speech recognition, even in early stages, is an active process in which speech analysis is attentionally guided. Note that this does not mean consciously guided but that information-contingent changes in early auditory encoding can occur as a function of context and experience. Active processing assumes that attention, plasticity, and listening goals are important in considering how listeners cope with adverse circumstances that impair hearing by masking noise in the environment or hearing loss. Although theories of speech perception have begun to incorporate some active processing, they seldom treat early speech encoding as plastic and attentionally guided. Recent research has suggested that speech perception is the product of both feedforward and feedback interactions between a number of brain regions that include descending projections perhaps as far downstream as the cochlea. It is important to understand how the ambiguity of the speech signal and constraints of context dynamically determine cognitive resources recruited during perception including focused attention, learning, and working memory. Theories of speech perception need to go beyond the current corticocentric approach in order to account for the intrinsic dynamics of the auditory encoding of speech. In doing so, this may provide new insights into ways in which hearing disorders and loss may be treated either through augementation or therapy.},
  pmid = {24672438},
  keywords = {active processing,attention,learning,passive processing,perception,speech,theories of speech perception},
  file = {/Users/jonny/Papers/HealdS/2014/Heald_2014_Speech perception as an active cognitive process.pdf}
}

@article{Hefner1986,
  title = {Effect of Unilateral and Bilateral Auditory Cortex Lesions on the Discrimination of Vocalizations by {{Japanese}} Macaques},
  author = {Hefner, H. E. and Heffner, R. S.},
  year = {1986},
  journal = {Journal of Neurophysiology},
  volume = {56},
  number = {3},
  url = {http://jn.physiology.org/content/56/3/683.short},
  urldate = {2017-06-06},
  keywords = {\#nosource}
}

@article{Hickok2007,
  title = {The Cortical Organization of Speech Processing},
  author = {Hickok, Gregory and Poeppel, David},
  year = {2007},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {8},
  number = {5},
  pages = {393--402},
  publisher = {{Nature Publishing Group}},
  issn = {1471-003X},
  doi = {10.1038/nrn2113},
  url = {http://www.nature.com/doifinder/10.1038/nrn2113},
  urldate = {2017-06-06},
  file = {/Users/jonny/Papers/HickokG/2007/Hickok_2007_The cortical organization of speech processing2.pdf}
}

@article{Hickok2012a,
  title = {The Cortical Organization of Speech Processing: {{Feedback}} Control and Predictive Coding the Context of a Dual-Stream Model},
  author = {Hickok, Gregory},
  year = {2012},
  journal = {Journal of Communication Disorders},
  volume = {45},
  number = {6},
  pages = {393--402},
  issn = {00219924},
  doi = {10.1016/j.jcomdis.2012.06.004},
  abstract = {Speech recognition is an active process that involves some form of predictive coding. This statement is relatively uncontroversial. What is less clear is the source of the prediction. The dual-stream model of speech processing suggests that there are two possible sources of predictive coding in speech perception: the motor speech system and the lexical-conceptual system. Here I provide an overview of the dual-stream model of speech processing and then discuss evidence concerning the source of predictive coding during speech recognition. I conclude that, in contrast to recent theoretical trends, the dorsal sensory-motor stream is not a source of forward prediction that can facilitate speech recognition. Rather, it is forward prediction coming out of the ventral stream that serves this function.Learning outcomes: Readers will (1) be able to explain the dual route model of speech processing including the function of the dorsal and ventral streams in language processing, (2) describe how disruptions to certain components of the dorsal stream can cause conduction aphasia, (3) be able to explain the fundamental principles of state feedback control in motor behavior, and (4) identify the role of predictive coding in motor control and in perception and how predictive coding coming out of the two streams may have different functional consequences. \textcopyright{} 2012 Elsevier Inc.},
  arxiv = {NIHMS150003},
  isbn = {1873-7994 (Electronic)\textbackslash n0021-9924 (Linking)},
  pmid = {22766458},
  keywords = {\#nosource,Aphasia,Language,Motor control,Speech perception,Speech production}
}

@article{Hillenbrand1994,
  title = {Acoustic Characteristics of {{American English}} Vowels},
  author = {Hillenbrand, James and Getty, Laura A. and Wheeler, Kimberlee and Clark, Michael J.},
  year = {1994},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {95},
  number = {5},
  pages = {2875--2875},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.409456},
  url = {http://asa.scitation.org/doi/10.1121/1.409456},
  urldate = {2017-03-24},
  abstract = {This study was designed as a replication and extension of the classic study of vowel acoustics by Peterson and Barney (PB) [J. Acoust. Soc. Am. 24, 175\textendash 184 (1952)]. Recordings were made of 50 men, 50 women, and 50 children producing the vowels /i, i, eh, \ae, hooked backward eh, inverted vee), a, open oh, u, u/ in h\textendash V\textendash d syllables. Formant contours for F1\textendash F4 were measured from LPC spectra using a custom interactive editing tool. For comparison with the PB data, formant patterns were sampled at a time that was judged by visual inspection to be maximally steady. Preliminary analysis shows numerous differences between the present data and those of PB, both in terms of average formant frequencies for vowels, and the degree of overlap among adjacent vowels. As with the original study, listening tests showed that the signals were nearly always identified as the vowel intended by the talker.},
  keywords = {COMPARATIVE EVALUATIONS,PERFORMANCE TESTING,SPEECH RECOGNITION,VOWELS},
  file = {/Users/jonny/Papers/HillenbrandJ/1994/Hillenbrand_1994_Acoustic characteristics of American English vowels2.pdf}
}

@article{hjortkjaerTaskModulatedCorticalRepresentations2018,
  title = {Task-{{Modulated Cortical Representations}} of {{Natural Sound Source Categories}}},
  author = {Hjortkj{\ae}r, Jens and Kassuba, Tanja and Madsen, Kristoffer H and Skov, Martin and Siebner, Hartwig R},
  year = {2018},
  month = jan,
  journal = {Cerebral Cortex},
  volume = {28},
  number = {1},
  pages = {295--306},
  publisher = {{Oxford University Press}},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhx263},
  url = {http://academic.oup.com/cercor/article/28/1/295/4561562},
  urldate = {2017-12-21},
  file = {/Users/jonny/Papers/HjortkjærJ/2018/Hjortkjær_2018_Task-Modulated Cortical Representations of Natural Sound Source Categories.pdf}
}

@misc{Hlavac2015,
  title = {Stargazer: {{Well-Formatted Regression}} and {{Summary Statistics Tables}}},
  author = {Hlavac, Marek},
  year = {2015},
  address = {{Cambridge, MA}},
  keywords = {\#nosource}
}

@article{Holt2001,
  title = {Influence of Fundamental Frequency on Stop-Consonant Voicing Perception: {{A}} Case of Learned Covariation or Auditory Enhancement?},
  author = {Holt, Lori L. and Lotto, Andrew J. and Kluender, Keith R.},
  year = {2001},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {109},
  number = {2},
  pages = {764--774},
  publisher = {{Acoustical Society of AmericaASA}},
  issn = {0001-4966},
  doi = {10.1121/1.1339825},
  url = {http://asa.scitation.org/doi/10.1121/1.1339825},
  urldate = {2017-02-02},
  abstract = {For stimuli modeling stop consonants varying in the acoustic correlates of voice onset time (VOT), human listeners are more likely to perceive stimuli with lower f0's as voiced consonants\textemdash a pattern of perception that follows regularities in English speech production. The present study examines the basis of this observation. One hypothesis is that lower f0's enhance perception of voiced stops by virtue of perceptual interactions that arise from the operating characteristics of the auditory system. A second hypothesis is that this perceptual pattern develops as a result of experience with f0-voicing covariation. In a test of these hypotheses, Japanese quail learned to respond to stimuli drawn from a series varying in VOT through training with one of three patterns of f0-voicing covariation. Voicing and f0 varied in the natural pattern (shorter VOT, lower f0), in an inverse pattern (shorter VOT, higher f0), or in a random pattern (no f0-voicing covariation). Birds trained with stimuli that had no f0-voicing ...},
  keywords = {speech intelligibility},
  file = {/Users/jonny/Zotero/storage/VCYC2EF9/Holt, Lotto, Kluender - 2001 - Influence of fundamental frequency on stop-consonant voicing perception A case of learned covariation (3).pdf}
}

@article{Holt2010,
  title = {Speech Perception as Categorization},
  author = {Holt, L. L. and Lotto, A. J.},
  year = {2010},
  month = jul,
  journal = {Attention, Perception \& Psychophysics},
  volume = {72},
  number = {5},
  pages = {1218--1227},
  publisher = {{Springer-Verlag}},
  issn = {1943-3921},
  doi = {10.3758/APP.72.5.1218},
  url = {http://www.springerlink.com/index/10.3758/APP.72.5.1218},
  urldate = {2017-01-23},
  file = {/Users/jonny/Zotero/storage/AX7RKNZZ/Holt, Lotto - 2010 - Speech perception as categorization(3).pdf}
}

@article{Howard2000,
  title = {Auditory Cortex on the Human Posterior Superior Temporal Gyrus},
  author = {Howard, M.A. and Volkov, I.O. and Mirsky, R. and Garell, P.C. and Noh, M.D. and Granner, M. and Damasio, H. and Steinschneider, M. and Reale, R.A. and Hind, J.E. and Brugge, J.F.},
  year = {2000},
  month = jan,
  journal = {The Journal of Comparative Neurology},
  volume = {416},
  number = {1},
  pages = {79--92},
  publisher = {{John Wiley \& Sons, Inc.}},
  issn = {0021-9967},
  doi = {10.1002/(SICI)1096-9861(20000103)416:1<79::AID-CNE6>3.0.CO;2-2},
  url = {http://doi.wiley.com/10.1002/\%28SICI\%291096-9861\%2820000103\%29416\%3A1\%3C79\%3A\%3AAID-CNE6\%3E3.0.CO\%3B2-2},
  urldate = {2017-01-20},
  keywords = {audition,auditory cortex,hearing},
  file = {/Users/jonny/Zotero/storage/RR4HU48J/Howard et al. - 2000 - Auditory cortex on the human posterior superior temporal gyrus(3).pdf}
}

@article{Howard2009,
  title = {Odor Quality Coding and Categorization in Human Posterior Piriform Cortex},
  author = {Howard, James D and Plailly, Jane and Grueschow, Marcus and Haynes, John-Dylan and Gottfried, Jay A},
  year = {2009},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {12},
  number = {7},
  pages = {932--938},
  publisher = {{Nature Publishing Group}},
  issn = {1097-6256},
  doi = {10.1038/nn.2324},
  url = {http://www.nature.com/doifinder/10.1038/nn.2324},
  urldate = {2017-01-20},
  keywords = {\#nosource}
}

@article{Hubel1962,
  title = {Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  year = {1962},
  month = jan,
  journal = {The Journal of Physiology},
  volume = {160},
  number = {1},
  pages = {106-154.2},
  publisher = {{Wiley-Blackwell}},
  issn = {0022-3751},
  doi = {10.1523/JNEUROSCI.1991-09.2009},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/14449617},
  urldate = {2017-01-26},
  abstract = {Images\textbackslash nnull},
  isbn = {0270-6474},
  pmid = {19776262},
  keywords = {CEREBRAL CORTEX/physiology},
  file = {/Users/jonny/Zotero/storage/BQ9NWTVH/Hubel, Wiesel - 1962 - Receptive fields, binocular interaction and functional architecture in the cat's visual cortex(3).pdf}
}

@article{Idemaru2011,
  title = {Word Recognition Reflects Dimension-Based Statistical Learning.},
  author = {Idemaru, Kaori and Holt, Lori L.},
  year = {2011},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {37},
  number = {6},
  pages = {1939--1956},
  publisher = {{American Psychological Association}},
  issn = {1939-1277},
  doi = {10.1037/a0025641},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0025641},
  urldate = {2017-05-05},
  keywords = {dimension-based learning,perceptual learning,speech perception,statistical learning,talker adaptation,word recognition},
  file = {/Users/jonny/Papers/IdemaruK/2011/Idemaru_2011_Word recognition reflects dimension-based statistical learning2.pdf}
}

@article{Ison2007a,
  title = {Age-Related Hearing Loss in {{C57BL}}/{{6J}} Mice Has Both Frequency-Specific and Non-Frequency-Specific Components That Produce a Hyperacusis-like Exaggeration of the Acoustic Startle Reflex},
  author = {Ison, James R. and Allen, Paul D. and O'Neill, William E.},
  year = {2007},
  month = nov,
  journal = {JARO - Journal of the Association for Research in Otolaryngology},
  volume = {8},
  number = {4},
  pages = {539--550},
  publisher = {{Springer-Verlag}},
  issn = {15253961},
  doi = {10.1007/s10162-007-0098-3},
  url = {http://link.springer.com/10.1007/s10162-007-0098-3},
  urldate = {2016-11-14},
  abstract = {Auditory brainstem-evoked response (ABR) thresholds were obtained in a longitudinal study of C57BL/6J mice between 10 and 53 weeks old, with repeated testing every 2 weeks. On alternate weeks, acoustic startle reflex (ASR) amplitudes were measured, elicited by tone pips with stimulus frequencies of 3, 6, 12, and 24 kHz, and intensities from subthreshold up to 110 dB sound pressure level. The increase in ABR thresholds for 3 and 6 kHz test stimuli followed a linear time course with increasing age from 10 to 53 weeks, with a slope of about 0.7 dB/week, and for 48 kHz a second linear time course, but beginning at 10 weeks with a slope of about 2.3 dB/week. ABR thresholds for 12, 24, and 32 kHz increased after one linear segment with a 0.7 dB slope, then after a variable delay related to the test frequency, shifted to a second segment having slopes of 3-5 dB/week. Hearing loss initially reduced the ASR for all eliciting stimuli, but at about 6 months of age, the response elicited by intense 3 and 6 kHz stimuli began to increase to reach values about three times above normal, and previously subthreshold stimuli came to elicit vigorous responses seen at first only for the intense stimuli. This hyperacusis-like effect appeared in all mice but was especially pronounced in mice with more serious hearing loss. These ABR data, together with a review of histopathological data in the C57BL/6 literature, suggest that the non-frequency-specific slow time course of hearing loss results from pathology in the lateral wall of the cochlea, whereas the stimulus-specific hearing loss with a rapid time course results from hair cell loss. Delayed exaggeration of the ASR with hearing loss reveals a deficit in centrifugal inhibitory control over the afferent reflex pathways after central neural reorganization, suggesting that this mouse may provide a useful model of age-related tinnitus and associated hyperacusis.},
  isbn = {1525-3961 (Print)\textbackslash n1438-7573 (Linking)},
  pmid = {17952509},
  keywords = {Aging,Hearing loss,Mixed strial/sensory presbycusis,Plasticity,Startle,Tinnitus/hyperacusis}
}

@book{James1890,
  title = {The {{Principles}} of {{Psychology}}, {{Vol}}. 1},
  author = {James, William},
  year = {1890},
  publisher = {{H. Holt}},
  address = {{New York}},
  keywords = {\#nosource}
}

@article{Jayaraman2015,
  title = {The {{Faces}} in {{Infant-Perspective Scenes Change}} over the {{First Year}} of {{Life}}},
  author = {Jayaraman, Swapnaa and Fausey, Caitlin M. and Smith, Linda B.},
  editor = {Nardini, Marko},
  year = {2015},
  month = may,
  journal = {PLOS ONE},
  volume = {10},
  number = {5},
  pages = {e0123780},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0123780},
  url = {http://dx.plos.org/10.1371/journal.pone.0123780},
  urldate = {2017-05-05},
  file = {/Users/jonny/Papers/JayaramanS/2015/Jayaraman_2015_The Faces in Infant-Perspective Scenes Change over the First Year of Life2.pdf}
}

@inproceedings{JiaDeng2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
  year = {2009},
  month = jun,
  pages = {248--255},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2009.5206848},
  url = {http://ieeexplore.ieee.org/document/5206848/},
  urldate = {2017-05-08},
  isbn = {978-1-4244-3992-8},
  file = {/Users/jonny/Papers/Jia Deng/2009/Jia Deng_2009_ImageNet.pdf}
}

@article{Kewley-Port1982a,
  title = {Measurement of Formant Transitions in Naturally Produced Stop Consonant-Vowel Syllables.},
  author = {{Kewley-Port}, D},
  year = {1982},
  month = aug,
  journal = {The Journal of the Acoustical Society of America},
  volume = {72},
  number = {2},
  pages = {379--89},
  issn = {0001-4966},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/7119280},
  urldate = {2017-01-25},
  abstract = {Formant transitions have been considered important context-dependent acoustic cues to place of articulation in stop-vowel syllables. However, the bulk of earlier research supporting their perceptual importance has been conducted primarily with synthetic speech stimuli. The present study examined the acoustic correlates of place of articulation in the voiced formant transitions from natural speech. Linear prediction analysis was used to provide detailed temporal and spectral measurements of the formant transitions for /b,d,g/ paired with eight vowels produced by one talker. Measurements of the transition onset and steady state frequencies, durations, and derived formant loci for F1, F2, and F3 are reported. Analysis of these measures showed little evidence of context invariant acoustic correlates of place. When vowel context was known, most transition parameters were not reliable acoustic correlates of place except for the F2 transition and a two-dimensional representation of F2 X F3 onset frequencies. The results indicated that the information contained in the formant transitions in these natural stop-vowel syllables was not sufficient to distinguish place across all the vowel contexts studied.},
  pmid = {7119280},
  keywords = {\#nosource}
}

@article{Kewley-Port1983,
  title = {Time-Varying Features as Correlates of Place of Articulation in Stop Consonants.},
  author = {{Kewley-Port}, D},
  year = {1983},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {73},
  number = {1},
  pages = {322--35},
  issn = {0001-4966},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/6826902},
  urldate = {2017-01-25},
  abstract = {Running spectral displays derived from linear prediction analysis were used to examine the initial 40 ms of stop-vowel CV syllables for possible acoustic correlates to place of articulation. Known spectral and temporal properties associated with the stop consonant release gesture were used to define a set of three-time-varying features observable in the visual displays. Judges identified place of articulation using these proposed features from running spectra of the syllables /b,d,g/paired with eight vowels produced by three talkers. Average correct identification of place was 88\%; identification was better for the male talkers (92\%) than the one female talker (78\%). Post hoc analyses suggested, however, that simple rules could be incorporated in the feature definitions to account for differences in vocal tract size. The nature of the information contained in linear prediction running spectra was analyzed further to take account of known properties of the peripheral auditory system. The three proposed time-varying features were shown to be displayed robustly in auditory filtered running spectra. The advantages of describing acoustic correlates for place from the dynamically varying temporal and spectral information in running spectra is discussed with regard to the static template matching approach advocated recently by Blumstein and Stevens [J. Acoust. Soc. Am. 66, 1001-1017 (1979)].},
  pmid = {6826902},
  keywords = {\#nosource}
}

@article{KewleyPort1983,
  title = {Perception of Static and Dynamic Acoustic Cues to Place of Articulation in Initial Stop Consonants},
  author = {Kewley-Port, Diane and Pisoni, David B. and Studdert-Kennedy, Michael},
  year = {1983},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {73},
  number = {5},
  pages = {1779--1793},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.389402},
  url = {http://asa.scitation.org/doi/10.1121/1.389402},
  urldate = {2017-12-13},
  abstract = {Two recent accounts of the acoustic cues which specify place of articulation in syllable-initial stop consonants claim that they are located in the initial portions of the CV waveform and are context-free. Stevens and Blumstein [J. Acoust. Soc. Am. 64, 1358\textendash 1368 (1978)] have described the perceptually relevant spectral properties of these cues as static, while Kewley-Port [J. Acoust. Soc. Am. 73, 322\textendash 335 (1983)] describes these cues as dynamic. Three perceptual experiments were conducted to test predictions derived from these accounts. Experiment 1 confirmed that acoustic cues for place of articulation are located in the initial 20\textendash 40 ms of natural stop-vowel syllables. Next, short synthetic CV's modeled after natural syllables were generated using either a digital, parallel-resonance synthesizer in experiment 2 or linear prediction synthesis in experiment 3. One set of synthetic stimuli preserved the static spectral properties proposed by Stevens and Blumstein. Another set of synthetic stimuli preserved ...},
  file = {/Users/jonny/Papers/Kewley‐PortD/1983/Kewley‐Port_1983_Perception of static and dynamic acoustic cues to place of articulation in.pdf}
}

@article{KewleyPort1983a,
  title = {Time-varying Features as Correlates of Place of Articulation in Stop Consonants},
  author = {Kewley-Port, Diane},
  year = {1983},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {73},
  number = {1},
  pages = {322--335},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.388813},
  url = {http://asa.scitation.org/doi/10.1121/1.388813},
  urldate = {2017-12-13},
  abstract = {Running spectral displays derived from linear prediction analysis were used to examine the initial 40 ms of stop-vowel CV syllables for possible acoustic correlates to place of articulation. Known spectral and temporal properties associated with the stop consonant release gesture were used to define a set of three-time-varying features observable in the visual displays. Judges identified place of articulation using these proposed features from running spectra of the syllables /b,d,g/ paired with eight vowels produced by three talkers. Average correct identification of place was 88\%; identification was better for the male talkers (92\%) than the one female talker (78\%). Post hoc analyses suggested, however, that simple rules could be incorporated in the feature definitions to account for differences in vocal tract size. The nature of the information contained in linear prediction running spectra was analyzed further to take account of known properties of the peripheral auditory system. The three proposed ti...},
  file = {/Users/jonny/Papers/Kewley‐PortD/1983/Kewley‐Port_1983_Time‐varying features as correlates of place of articulation in stop consonants.pdf}
}

@article{kimModulationAuditoryvocalFeedback2019,
  title = {Modulation of Auditory-Vocal Feedback Control Due to Planned Changes in Voice Fo},
  author = {Kim, Jason H. and Larson, Charles R.},
  year = {2019},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {145},
  number = {3},
  pages = {1482--1492},
  issn = {0001-4966},
  doi = {10.1121/1.5094414},
  url = {https://asa.scitation.org/doi/10.1121/1.5094414},
  urldate = {2019-04-29},
  abstract = {Previous studies have demonstrated that voice fundamental frequency (fo), or pitch, relies on auditory feedback to monitor and correct for errors in production. When voice-pitch auditory feedback is unexpectedly perturbed, individuals typically produce a compensatory change in fo that opposes the direction of the pitch-perturbation. Studies comparing steady vowel vocalizations and speech tasks have demonstrated task-dependent modulation of the compensatory response, but the effects of planning to volitionally change fo during active vocalization have yet to be explored. Ten musicians and ten non-musicians were asked to perform two vocal tasks. Both tasks started off at a conversational fo. In one task, pitch-shifted feedback was presented when the participants were planning to hold fo constant (steady fo), and in the other, feedback was shifted while participants were in the planning stage prior to raising fo (raised fo) from a steady state. Acoustical analyses of fo were performed to measure the peak magnitude and latency of both the compensatory response as well as the voluntary fo change. Results showed that planning to change pitch modulates the mechanisms controlling feedback-based error correction of fo, and musicality affects how individuals incorporate modulations in auditory feedback with the feedforward plans to increase voice fo.},
  file = {/Users/jonny/Papers/KimJ/2019/Kim_2019_Modulation of auditory-vocal feedback control due to planned changes in voice fo.pdf;/Users/jonny/Zotero/storage/ISTRXCBS/1.html}
}

@article{King2009,
  title = {Unraveling the Principles of Auditory Cortical Processing: Can We Learn from the Visual System?},
  author = {King, Andrew J and Nelken, Israel},
  year = {2009},
  month = jun,
  journal = {Nature Neuroscience},
  volume = {12},
  number = {6},
  pages = {698--701},
  publisher = {{Nature Publishing Group}},
  issn = {1097-6256},
  doi = {10.1038/nn.2308},
  url = {http://www.nature.com/doifinder/10.1038/nn.2308},
  urldate = {2017-08-08},
  keywords = {\#nosource}
}

@article{Kleinschmidt2015,
  title = {Robust Speech Perception: {{Recognize}} the Familiar, Generalize to the Similar, and Adapt to the Novel.},
  author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
  year = {2015},
  journal = {Psychological Review},
  volume = {122},
  number = {2},
  pages = {148--203},
  publisher = {{American Psychological Association}},
  issn = {1939-1471},
  doi = {10.1037/a0038695},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0038695},
  urldate = {2017-05-03},
  keywords = {adaptation,generalization,lack of invariance,speech perception,statistical learning},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KleinschmidtD/kleinschmidt_2015_robust_speech_perception.pdf;/Users/jonny/Papers/KleinschmidtD/2015/Kleinschmidt_2015_Robust speech perception2.pdf}
}

@article{Kleinschmidt2016,
  title = {Re-Examining Selective Adaptation: {{Fatiguing}} Feature Detectors, or Distributional Learning?},
  author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
  year = {2016},
  month = jun,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {3},
  pages = {678--691},
  issn = {1069-9384},
  doi = {10.3758/s13423-015-0943-z},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/26438255},
  urldate = {2017-02-08},
  abstract = {When a listener hears many good examples of a /b/ in a row, they are less likely to classify other sounds on, e.g., a /b/-to-/d/ continuum as /b/. This phenomenon is known as selective adaptation and is a well-studied property of speech perception. Traditionally, selective adaptation is seen as a mechanistic property of the speech perception system, and attributed to fatigue in acoustic-phonetic feature detectors. However, recent developments in our understanding of non-linguistic sensory adaptation and higher-level adaptive plasticity in speech perception and language comprehension suggest that it is time to re-visit the phenomenon of selective adaptation. We argue that selective adaptation is better thought of as a computational property of the speech perception system. Drawing on a common thread in recent work on both non-linguistic sensory adaptation and plasticity in language comprehension, we furthermore propose that selective adaptation can be seen as a consequence of distributional learning across multiple levels of representation. This proposal opens up new questions for research on selective adaptation itself, and also suggests that selective adaptation can be an important bridge between work on adaptation in low-level sensory systems and the complicated plasticity of the adult language comprehension system.},
  pmid = {26438255},
  keywords = {Computational models,Perceptual learning,Speech perception,Statistical inference}
}

@article{Kleinschmidt2016a,
  title = {Re-Examining Selective Adaptation: {{Fatiguing}} Feature Detectors, or Distributional Learning?},
  author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
  year = {2016},
  month = jun,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {3},
  pages = {678--691},
  publisher = {{Springer US}},
  issn = {1069-9384},
  doi = {10.3758/s13423-015-0943-z},
  url = {http://link.springer.com/10.3758/s13423-015-0943-z},
  urldate = {2017-05-05},
  file = {/Users/jonny/Papers/KleinschmidtD/2016/Kleinschmidt_2016_Re-examining selective adaptation2.pdf}
}

@article{Kluender1987,
  title = {Japanese Quail Can Learn Phonetic Categories.},
  author = {Kluender, K R and Diehl, R L and Killeen, P R},
  year = {1987},
  month = sep,
  journal = {Science (New York, N.Y.)},
  volume = {237},
  number = {4819},
  pages = {1195--1197},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.3629235},
  url = {http://science.sciencemag.org/content/237/4819/1195.abstract},
  urldate = {2016-05-25},
  abstract = {Japanese quail (Coturnix coturnix) learned a category for syllable-initial [d] followed by a dozen different vowels. After learning to categorize syllables consisting of [d], [b], or [g] followed by four different vowels, quail correctly categorized syllables in which the same consonants preceded eight novel vowels. Acoustic analysis of the categorized syllables revealed no single feature or pattern of features that could support generalization, suggesting that the quail adopted a more complex mapping of stimuli into categories. These results challenge theories of speech sound classification that posit uniquely human capacities.},
  isbn = {0036-8075 (Print)\textbackslash r0036-8075 (Linking)},
  langid = {english},
  pmid = {3629235},
  keywords = {\#nosource,Animals,Coturnix,Coturnix: physiology,Female,Humans,Learning,Phonetics,Quail,Quail: physiology,Reinforcement (Psychology),Speech Perception}
}

@article{Kluender1994,
  title = {Effects of First Formant Onset Frequency on [-Voice] Judgments Result from Auditory Processes Not Specific to Humans.},
  author = {Kluender, K R and Lotto, a J},
  year = {1994},
  journal = {The Journal of the Acoustical Society of America},
  volume = {95},
  number = {2},
  pages = {1044--52},
  issn = {0001-4966},
  doi = {10.1121/1.408466},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/8132898},
  abstract = {When F1-onset frequency is lower, longer F1 cut-back (VOT) is required for human listeners to perceive synthesized stop consonants as voiceless. K. R. Kluender [J. Acoust. Soc. Am. 90, 83-96 (1991)] found comparable effects of F1-onset frequency on the "labeling" of stop consonants by Japanese quail (coturnix coturnix japonica) trained to distinguish stop consonants varying in F1 cut-back. In that study, CVs were synthesized with natural-like rising F1 transitions, and endpoint training stimuli differed in the onset frequency of F1 because a longer cut-back resulted in a higher F1 onset. In order to assess whether earlier results were due to auditory predispositions or due to animals having learned the natural covariance between F1 cut-back and F1-onset frequency, the present experiment was conducted with synthetic continua having either a relatively low (375 Hz) or high (750 Hz) constant-frequency F1. Six birds were trained to respond differentially to endpoint stimuli from three series of synthesized /CV/s varying in duration of F1 cut-back. Second and third formant transitions were appropriate for labial, alveolar, or velar stops. Despite the fact that there was no opportunity for animal subjects to use experienced covariation of F1-onset frequency and F1 cut-back, quail typically exhibited shorter labeling boundaries (more voiceless stops) for intermediate stimuli of the continua when F1 frequency was higher. Responses by human subjects listening to the same stimuli were also collected. Results lend support to the earlier conclusion that part or all of the effect of F1 onset frequency on perception of voicing may be adequately explained by general auditory processes.(ABSTRACT TRUNCATED AT 250 WORDS)},
  isbn = {0001-4966 (Print)},
  pmid = {8132898},
  keywords = {\#nosource,Animals,Coturnix,Female,Humans,Male,Phonetics,Speech Acoustics,Speech Perception}
}

@article{Kluender2000,
  title = {Contributions of Nonhuman Animal Models to Understanding Human Speech Perception},
  author = {Kluender, Keith R.},
  year = {2000},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {107},
  number = {5},
  pages = {2835--2835},
  publisher = {{Acoustical Society of AmericaASA}},
  issn = {0001-4966},
  doi = {10.1121/1.429153},
  url = {http://asa.scitation.org/doi/10.1121/1.429153},
  urldate = {2017-02-02},
  abstract = {Broadly speaking, nonhuman animal models contribute to understanding speech perception by humans in two ways\textemdash by analogy and by homology. The former is generally easier and examples are more abundant. Because demonstrating homology requires deeper explication of underlying mechanisms, claims can be more precarious but carry potentially greater explanatory payoff. When studying nonhuman organisms as an analogy, the emphasis is typically upon how animal physiological or behavioral processes have adapted to fulfill requirements of particular ecological niches. By contrast, study of animals as homology often violates ecology in search of common underlying processes, and the animal becomes a method more than an object of study. Examples of findings for animal analogies and homologies will be reviewed. Data will be presented from experiments in which nonhuman subjects play the role of homology in revealing both foundational sensory processes and more plastic processes of perceptual development. Animal models pro...},
  keywords = {\#nosource}
}

@incollection{Kluender2013a,
  title = {Perception of {{Vowel Sounds Within}} a {{Biologically Realistic Model}} of {{Efficient Coding}}},
  booktitle = {Vowel {{Inherent Spectral Change}}},
  author = {Kluender, Keith R and Stilp, Christian E and Kiefte, Michael and Kluender, K R and Stilp, C E and Kiefte, M},
  year = {2013},
  pages = {117--151},
  doi = {10.1007/978-3-642-14209-3_6},
  abstract = {Predicated upon principles of information theory, efficient coding has proven valuable for understanding visual perception. Here, we illustrate how efficient coding provides a powerful explanatory framework for understanding speech perception. This framework dissolves debates about objects of perception, instead focusing on the objective of perception: optimizing information transmis-sion between the environment and perceivers. A simple measure of physiologically significant information is shown to predict intelligibility of variable-rate speech and discriminability of vowel sounds. Reliable covariance between acoustic attributes in complex sounds, both speech and nonspeech, is demonstrated to be amply available in natural sounds and efficiently coded by listeners. An efficient coding framework provides a productive approach to answer questions concerning perception of vowel sounds (including vowel inherent spectral change), perception of speech, and perception most broadly.},
  isbn = {978-3-642-14209-3},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KluenderK/kluender_2013_perception_of_vowel_sounds_within_a_biologically_realistic_model_of_efficient.pdf}
}

@article{kluenderLongstandingProblemsSpeech2019,
  ids = {kluenderLongstandingProblemsSpeech2019a},
  title = {Long-Standing Problems in Speech Perception Dissolve within an Information-Theoretic Perspective},
  author = {Kluender, Keith R. and Stilp, Christian E. and Lucas, Fernando Llanos},
  year = {2019},
  month = may,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {81},
  number = {4},
  pages = {861--883},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01702-x},
  url = {https://doi.org/10.3758/s13414-019-01702-x},
  urldate = {2019-07-10},
  abstract = {An information theoretic framework is proposed to have the potential to dissolve (rather than attempt to solve) multiple long-standing problems concerning speech perception. By this view, speech perception can be reframed as a series of processes through which sensitivity to information\textemdash that which changes and/or is unpredictable\textemdash becomes increasingly sophisticated and shaped by experience. Problems concerning appropriate objects of perception (gestures vs. sounds), rate normalization, variance consequent to articulation, and talker normalization are reframed, or even dissolved, within this information-theoretic framework. Application of discriminative models founded on information theory provides a productive approach to answer questions concerning perception of speech, and perception most broadly.},
  langid = {english},
  keywords = {_tablet,Perceptual learning,Psychoacoustics,Speech perception},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KluenderK/kluender_2019_long-standing_problems_in_speech_perception_dissolve_within_an.pdf;/Users/jonny/Papers/KluenderK/2019/Kluender_2019_Long-standing problems in speech perception dissolve within an.pdf;/Users/jonny/Papers/KluenderK/2019/Kluender_2019_Long-standing problems in speech perception dissolve within an2.pdf}
}

@techreport{Knight2014,
  title = {Categorical Perception of Tactile Distance},
  author = {Knight, Frances Le Cornu and Longo, Matthew R. and Bremner, Andrew J.},
  year = {2014},
  journal = {Cognition},
  volume = {131},
  number = {2},
  pages = {254--262},
  issn = {00100277},
  doi = {10.1016/j.cognition.2014.01.005},
  abstract = {The tactile surface forms a continuous sheet covering the body. And yet, the perceived distance between two touches varies across stimulation sites. Perceived tactile distance is larger when stimuli cross over the wrist, compared to when both fall on either the hand or the forearm. This effect could reflect a categorical distortion of tactile space across body-part boundaries (in which stimuli crossing the wrist boundary are perceptually elongated) or may simply reflect a localised increased in acuity surrounding anatomical landmarks (in which stimuli near the wrist are perceptually elongated). We tested these two interpretations across two experiments, by comparing a well-documented bias to perceive mediolateral tactile distances across the forearm/hand as larger than proximodistal ones along the forearm/hand at three different sites (hand, wrist, and forearm). According to the `categorical' interpretation, tactile distances should be elongated selectively in the proximodistal axis thus reducing the anisotropy. According to the `localised acuity' interpretation, distances will be perceptually elongated in the vicinity of the wrist regardless of orientation, leading to increased overall size without affecting anisotropy. Consistent with the categorical account, we found a reduction in the magnitude of anisotropy at the wrist, with no evidence of a corresponding localised increase in precision. These findings demonstrate that we reference touch to a representation of the body that is categorically segmented into discrete parts, which consequently influences the perception of tactile distance.},
  file = {/Users/jonny/Zotero/storage/BHLQX9CS/Knight, Longo, Bremner - 2014 - Categorical perception of tactile distance(3).pdf}
}

@article{Kobler1985,
  title = {Echo Intensity Compensation by Echolocating Bats},
  author = {Kobler, J.B. and Wilson, B.S. and Henson, O.W. and Bishop, A.L.},
  year = {1985},
  month = jan,
  journal = {Hearing Research},
  volume = {20},
  number = {2},
  pages = {99--108},
  issn = {03785955},
  doi = {10.1016/0378-5955(85)90161-3},
  url = {http://linkinghub.elsevier.com/retrieve/pii/0378595585901613},
  urldate = {2017-08-08},
  keywords = {\#nosource}
}

@article{Kosem2016,
  title = {High-Frequency Neural Activity Predicts Word Parsing in Ambiguous Speech Streams},
  author = {K{\"o}sem, Anne and Basirat, Anahita and Azizi, Leila and {van Wassenhove}, Virginie},
  year = {2016},
  journal = {Journal of Neurophysiology},
  volume = {116},
  number = {6},
  pages = {2497--2512},
  issn = {0022-3077},
  doi = {10.1152/jn.00074.2016},
  url = {http://jn.physiology.org/lookup/doi/10.1152/jn.00074.2016},
  urldate = {2016-12-30},
  keywords = {\#nosource}
}

@article{kriengwatanaRevisitingVocalPerception2014,
  title = {Revisiting Vocal Perception in Non-Human Animals: {{A}} Review of Vowel Discrimination, Speaker Voice Recognition, and Speaker Normalization},
  author = {Kriengwatana, Buddhamas and Escudero, Paola and ten Cate, Carel},
  year = {2014},
  journal = {Frontiers in Psychology},
  issn = {16641078},
  doi = {10.3389/fpsyg.2014.01543},
  abstract = {The extent to which human speech perception evolved by taking advantage of predispositions and pre-existing features of vertebrate auditory and cognitive systems remains a central question in the evolution of speech. This paper reviews asymmetries in vowel perception, speaker voice recognition, and speaker normalization in non-human animals \textendash{} topics that have not been thoroughly discussed in relation to the abilities of non-human animals, but are nonetheless important aspects of vocal perception. Throughout this paper we demonstrate that addressing these issues in non-human animals is relevant and worthwhile because many non-human animals must deal with similar issues in their natural environment. That is, they must also discriminate between similar-sounding vocalizations, determine signaler identity from vocalizations, and resolve signaler-dependent variation in vocalizations from conspecifics. Overall, we find that, although plausible, the current evidence is insufficiently strong to conclude that directional asymmetries in vowel perception are specific to humans, or that non-human animals can use voice characteristics to recognize human individuals. However, we do find some indication that non-human animals can normalize speaker differences. Accordingly, we identify avenues for future research that would greatly improve and advance our understanding of these topics.},
  isbn = {1664-1078 (Electronic)\textbackslash r1664-1078 (Linking)},
  pmid = {25628583},
  keywords = {Animal behavior,Asymmetries in vowel perception,Comparative cognition,General auditory approach,Language evolution,Speaker normalization,Voice perception},
  file = {/Users/jonny/Papers/KriengwatanaB/2014/Kriengwatana_2014_Revisiting vocal perception in non-human animals.pdf}
}

@article{Kronrod2016a,
  title = {A Unified Account of Categorical Effects in Phonetic Perception},
  author = {Kronrod, Yakov and Coppess, Emily and Feldman, Naomi H.},
  year = {2016},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {23},
  number = {6},
  pages = {1681--1712},
  publisher = {{Springer US}},
  issn = {1069-9384},
  doi = {10.3758/s13423-016-1049-y},
  url = {http://link.springer.com/10.3758/s13423-016-1049-y},
  urldate = {2017-01-20},
  keywords = {_tablet,\#nosource},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KronrodY/kronrod_2016_a_unified_account_of_categorical_effects_in_phonetic_perception.pdf}
}

@article{Kuhl1978,
  title = {Speech Perception by the Chinchilla: {{Identification}} Functions for Synthetic {{VOT}} Stimuli},
  author = {Kuhl, Patricia K. and Miller, James D.},
  year = {1978},
  journal = {The Journal of the Acoustical Society of America},
  volume = {63},
  number = {3},
  pages = {905--917},
  issn = {00014966},
  doi = {10.1121/1.381770},
  url = {http://link.aip.org/link/?JAS/63/905/1\%5Cnhttp://asadl.org/jasa/resource/1/jasman/v63/i3/p905_s1?isAuthorized=no},
  abstract = {In an attempt to clearly differentiate perceptual effects that are attributable to ''auditory'' and ''phonetic'' levels of processing in speech perception we have undertaken a series of experiments with animal listeners. Four chinchillas (Chinchilla laniger) were trained to respond differently to the ''endpoints'' of a synthetic alveolar speech continuum (0 ms VOT and +80 ms VOT) and were then tested in a generalization paradigm with the VOT stimuli between these endpoints. The resulting identification functions were nearly identical to those obtained with adult English-speaking listeners. To test the generality of this agreement, the animals were then tested with synthetic stimuli that had labial and velar places of articulation. As a whole, the functions produced by the two species were very similar; the same relative locations of the phonetic boundaries, with lowest VOT boundaries for labial stimuli and highest for velar stimuli, were obtained for each animal and human subject. No significant differences between species on the absolute values of the phonetic boundaries were obtained, but chinchillas produced identification functions that were slightly, but significantly, less steep. These results are discussed with regard to theories of speech perception, the evolution of a speech-sound repertoire, and current interpretations of the human infant's perceptual proclivities with regard to speech-sound perception.},
  isbn = {0001-4966 (Print)\textbackslash r0001-4966 (Linking)},
  pmid = {670558},
  keywords = {\#nosource}
}

@article{Kuhl1983,
  title = {Enhanced Discriminability at the Phonetic Boundaries for the Place Feature in Macaques.},
  author = {Kuhl, P K and Padden, D M},
  year = {1983},
  journal = {The Journal of the Acoustical Society of America},
  volume = {73},
  number = {3},
  pages = {1003--1010},
  issn = {0001-4966},
  doi = {10.3758/BF03204208},
  abstract = {Discrimination of speech-sound pairs drawn from a computer-generated continuum in which syllables varied along the place of articulation phonetic feature (/b,d,g/) was tested with macaques. The acoustic feature that was varied along the two-formant 15-step continuum was the starting frequency of the second-formant transition. Discrimination of stimulus pairs separated by two steps was tested along the entire continuum in a same-different task. Results demonstrated that peaks in the discrimination functions occur for macaques at the "phonetic boundaries" which separate the /b-d/ and /d-g/ categories for human listeners. The data support two conclusions. First, although current theoretical accounts of place perception by human adults suggest that isolated second-formant transitions are "secondary" cues, learned by association with primary cues, the animal data are more compatible with the notion that second-formant transitions are sufficient to allow the appropriate partitioning of a place continuum in the absence of associative pairing with other more complex cues. Second, we discuss two potential roles played by audition in the evolution of the acoustics of language. One is that audition provided a set of "natural psychophysical boundaries," based on rather simple acoustic properties, which guided the selection of the phonetic repertoire but did not solely determine it; the other is that audition provided a set of rules for the formation of "natural classes" of sound and that phonetic units met those criteria. The data provided in this experiment provide support for the former. Experiments that could more clearly differentiate the two hypotheses are described.},
  isbn = {0001-4966 (Print)\textbackslash r0001-4966 (Linking)},
  pmid = {6221040},
  keywords = {\#nosource}
}

@article{Kuhl1992,
  title = {Linguistic Experience Alters Phonetic Perception in Infants by 6 Months of Age},
  author = {Kuhl, PK and Williams, KA and Lacerda, F and Stevens, KN and Lindblom, B},
  year = {1992},
  journal = {Science},
  volume = {255},
  number = {5044},
  keywords = {\#nosource}
}

@article{kuhlNewViewLanguage2000,
  title = {A New View of Language Acquisition},
  author = {Kuhl, Patricia K.},
  year = {2000},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {97},
  number = {22},
  pages = {11850--11857},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.97.22.11850},
  url = {https://www.pnas.org/content/97/22/11850},
  urldate = {2019-07-28},
  abstract = {At the forefront of debates on language are new data demonstrating infants' early acquisition of information about their native language. The data show that infants perceptually ``map'' critical aspects of ambient language in the first year of life before they can speak. Statistical properties of speech are picked up through exposure to ambient language. Moreover, linguistic experience alters infants' perception of speech, warping perception in the service of language. Infants' strategies are unexpected and unpredicted by historical views. A new theoretical position has emerged, and six postulates of this position are described.},
  copyright = {Copyright \textcopyright{} 2000, The National Academy of Sciences},
  langid = {english},
  pmid = {11050219},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KuhlP/kuhl_2000_a_new_view_of_language_acquisition.pdf;/Users/jonny/Papers/KuhlP/2000/Kuhl_2000_A new view of language acquisition.pdf;/Users/jonny/Zotero/storage/S9RMDR2I/11850.html}
}

@article{Lachlan2015a,
  title = {Context-Dependent Categorical Perception in a Songbird.},
  author = {Lachlan, Robert F and Nowicki, Stephen},
  year = {2015},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {112},
  number = {6},
  pages = {1892--7},
  issn = {1091-6490},
  doi = {10.1073/pnas.1410844112},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/25561538},
  abstract = {Some of the psychological abilities that underlie human speech are shared with other species. One hallmark of speech is that linguistic context affects both how speech sounds are categorized into phonemes, and how different versions of phonemes are produced. We here confirm earlier findings that swamp sparrows categorically perceive the notes that constitute their learned songs and then investigate how categorical boundaries differ according to context. We clustered notes according to their acoustic structure, and found statistical evidence for clustering into 10 population-wide note types. Examining how three related types were perceived, we found, in both discrimination and labeling tests, that an "intermediate" note type is categorized with a "short" type when it occurs at the beginning of a song syllable, but with a "long" type at the end of a syllable. In sum, three produced note-type clusters appear to be underlain by two perceived categories. Thus, in birdsong, as in human speech, categorical perception is context-dependent, and as is the case for human phonology, there is a complex relationship between underlying categorical representations and surface forms. Our results therefore suggest that complex phonology can evolve even in the absence of rich linguistic components, like syntax and semantics.},
  pmid = {25561538},
  keywords = {Acoustic Stimulation,Analysis of Variance,Animal,Animal: physiology,Animals,Cluster Analysis,Discrimination (Psychology),Discrimination (Psychology): physiology,Flight,Linear Models,New York,Pennsylvania,Principal Component Analysis,Sound Spectrography,Sparrows,Sparrows: physiology,Speech Perception,Speech Perception: physiology,Vocalization,Wetlands,Wing,Wing: physiology},
  file = {/Users/jonny/Zotero/storage/YUGPEDHY/Lachlan, Nowicki - 2015 - Context-dependent categorical perception in a songbird(3).pdf}
}

@article{Leaver2010,
  title = {Cortical {{Representation}} of {{Natural Complex Sounds}}: {{Effects}} of {{Acoustic Features}} and {{Auditory Object Category}}},
  author = {Leaver, Amber M. and Rauschecker, Josef P.},
  year = {2010},
  journal = {Journal of Neuroscience},
  volume = {30},
  number = {22},
  url = {http://www.jneurosci.org/content/30/22/7604.short},
  urldate = {2017-08-08},
  keywords = {\#nosource}
}

@article{Lein2007,
  title = {Genome-Wide Atlas of Gene Expression in the Adult Mouse Brain},
  author = {Lein, Ed S and Hawrylycz, M J and Ao, N and Ayres, M and Bensinger, A and Bernard, A and Boe, A F and Boguski, M S and Brockway, K S and Byrnes, E J and Chen, L and Chen, T M and Chin, M C and Chong, J and Crook, B E and Czaplinska, A and Dang, C N and Datta, S and Dee, N R and Desaki, A L and Desta, T and Diep, E and Dolbeare, T A and Donelan, M J and Dong, H W and Dougherty, J G and Duncan, B J and Ebbert, A J and Eichele, G and Estin, L K and Faber, C and Facer, B A and Fields, R and Fischer, S R and Fliss, T P and Frensley, C and Gates, S N and Glattfelder, K J and Halverson, K R and Hart, M R and Hohmann, J G and Howell, M P and Jeung, D P and Johnson, R A and Karr, P T and Kawal, R and Kidney, J M and Knapik, R H and Kuan, C L and Lake, J H and Laramee, A R and Larsen, K D and Lau, C and Lemon, T A and Liang, A J and Liu, Y and Luong, L T and Michaels, J and Morgan, J J and Morgan, R J and Mortrud, M T and Mosqueda, N F and Ng, L L and Ng, R and Orta, G J and Overly, C C and Pak, T H and Parry, S E and Pathak, S D and Pearson, O C and Puchalski, R B and Riley, Z L and Rockett, H R and Rowland, S A and Royall, J J and Ruiz, M J and Sarno, N R and Schaffnit, K and Shapovalova, N V and Sivisay, T and Slaughterbeck, C R and Smith, S C and Smith, K A and Smith, B I and Sodt, A J and Stewart, N N and Stumpf, K R and Sunkin, S M and Sutram, M and Tam, A and Teemer, C D and Thaller, C and Thompson, C L and Varnam, L R and Visel, A and Whitlock, R M and Wohnoutka, P E and Wolkey, C K and Wong, V Y and Wood, M and Yaylaoglu, M B and Young, R C and Youngstrom, B L and Yuan, X F and Zhang, B and Zwingman, T A and Jones, A R},
  year = {2007},
  journal = {Nature},
  volume = {445},
  number = {7124},
  pages = {168--176},
  issn = {0028-0836},
  doi = {10.1038/nature05453},
  url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve\&db=PubMed\&dopt=Citation\&list_uids=17151600},
  abstract = {Molecular approaches to understanding the functional circuitry of the nervous system promise new insights into the relationship between genes, brain and behaviour. The cellular diversity of the brain necessitates a cellular resolution approach towards understanding the functional genomics of the nervous system. We describe here an anatomically comprehensive digital atlas containing the expression patterns of approximately 20,000 genes in the adult mouse brain. Data were generated using automated high-throughput procedures for in situ hybridization and data acquisition, and are publicly accessible online. Newly developed image-based informatics tools allow global genome-scale structural analysis and cross-correlation, as well as identification of regionally enriched genes. Unbiased fine-resolution analysis has identified highly specific cellular markers as well as extensive evidence of cellular heterogeneity not evident in classical neuroanatomical atlases. This highly standardized atlas provides an open, primary data resource for a wide variety of further studies concerning brain organization and function.},
  isbn = {1476-4687 (Electronic)},
  pmid = {17151600},
  keywords = {*Gene Expression Profiling,*Gene Expression Regulation,\#nosource,Animals,Brain/anatomy \& histology/cytology/*metabolism,Computational Biology,Genome/*genetics,Genomics,Hippocampus/anatomy \& histology/metabolism,Inbred C57BL,Male,Messenger/genetics/metabolism,Mice,Organ Specificity,RNA}
}

@article{LIBERMAN1957,
  title = {The Discrimination of Speech Sounds within and across Phoneme Boundaries.},
  author = {LIBERMAN, A M and HARRIS, K S and HOFFMAN, H S and GRIFFITH, B C},
  year = {1957},
  month = nov,
  journal = {Journal of experimental psychology},
  volume = {54},
  number = {5},
  pages = {358--68},
  issn = {0022-1015},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/13481283},
  urldate = {2017-01-23},
  pmid = {13481283},
  keywords = {\#nosource,HEARING,SPEECH}
}

@article{Liberman1967,
  title = {Perception of the Speech Code.},
  author = {Liberman, A M and Cooper, F S and Shankweiler, D P and {Studdert-Kennedy}, M},
  year = {1967},
  month = nov,
  journal = {Psychological review},
  volume = {74},
  number = {6},
  pages = {431--61},
  issn = {0033-295X},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/4170865},
  urldate = {2017-01-23},
  pmid = {4170865},
  keywords = {\#nosource}
}

@article{Liberman1985,
  title = {The Motor Theory of Speech Perception Revised},
  author = {Liberman, Alvin M. and Mattingly, Ignatius G.},
  year = {1985},
  month = oct,
  journal = {Cognition},
  volume = {21},
  number = {1},
  pages = {1--36},
  issn = {00100277},
  doi = {10.1016/0010-0277(85)90021-6},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/4075760},
  urldate = {2016-12-14},
  abstract = {A motor theory of speech perception, initially proposed to account for results of early experiments with synthetic speech, is now extensively revised to accommodate recent findings, and to relate the assumptions of the theory to those that might be made about other perceptual modes. According to the revised theory, phonetic information is perceived in a biologically distinct system, a 'module' specialized to detect the intended gestures of the speaker that are the basis for phonetic categories. Built into the structure of this module is the unique but lawful relationship between the gestures and the acoustic patterns in which they are variously overlapped. In consequence, the module causes perception of phonetic structure without translation from preliminary auditory impressions. Thus, it is comparable to such other modules as the one that enables an animal to localize sound. Peculiar to the phonetic module are the relation between perception and production it incorporates and the fact that it must compete with other modules for the same stimulus variations. ?? 1985.},
  arxiv = {NIHMS150003},
  isbn = {0010-0277, 0010-0277},
  pmid = {4075760},
  keywords = {\#nosource}
}

@article{Liberman1985a,
  title = {The Motor Theory of Speech Perception Revised},
  author = {Liberman, Alvin M. and Mattingly, Ignatius G.},
  year = {1985},
  journal = {Cognition},
  volume = {21},
  number = {1},
  pages = {1--36},
  issn = {00100277},
  doi = {10.1016/0010-0277(85)90021-6},
  abstract = {A motor theory of speech perception, initially proposed to account for results of early experiments with synthetic speech, is now extensively revised to accommodate recent findings, and to relate the assumptions of the theory to those that might be made about other perceptual modes. According to the revised theory, phonetic information is perceived in a biologically distinct system, a `module' specialized to detect the intended gestures of the speaker that are the basis for phonetic categories. Built into the structure of this module is the unique but lawful relationship between the gestures and the acoustic patterns in which they are variously overlapped. In consequence, the module causes perception of phonetic structure without translation from preliminary auditory impressions. Thus, it is comparable to such other modules as the one that enables an animal to localize sound. Peculiar to the phonetic module are the relation between perception and production it incorporates and the fact that it must compete with other modules for the same stimulus variations. Une th\'eorie motrice de la perception propos\'ee initialement pour rendre compte des r\'esultats des premi\`eres exp\'eriences avec de la parole synth\'etique a \'et\'e largement r\'evis\'ee afin d'interpr\'eter les donn\'ees r\'ecentes et de relier les propositions de cette th\'eorie \`a celles que l'on peut faire pour d'autres modalit\'es de perception. La r\'evision de cette th\'eorie stipule que l'information phon\'etique est fournie par un syst\`eme biologique distinct, un `module' sp\'ecialis\'e pour d\'etecter les gestes que le locuteur a eu l'intention de faire: ces gestes fondent les cat\'egories phon\'etiques. La relation entre les gestes et les patterns acoustiques dans lesquels ceux-ci sont imbriqu\'es de facon vari\'ee est unique mais r\'egul\'ee. Cette relation est construite dans la structure du module. En cons\'equence le module provoque la perception de la structure phon\'etique sans traduction \`a partir d'impressions auditives pr\'eliminaires. Ce module est ainsi comparable \`a d'autres modules tels que celui qui permet \`a l'animal de localiser les sons. La particularit\'e de ce module tient \`a la relation entre perception et production qu'il incorpore et an fait qu'il doit rivaliser avec d'autres modules pour de m\^emes variations de stimulus.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LibermanA/liberman_1985_the_motor_theory_of_speech_perception_revised.pdf;/Users/jonny/Zotero/storage/GHZFWMHT/Liberman, Mattingly - 1985 - The motor theory of speech perception revised(3).pdf}
}

@book{Lieberman1984,
  title = {The Biology and Evolution of Language},
  author = {Lieberman, Philip.},
  year = {1984},
  publisher = {{Harvard University Press}},
  abstract = {Introduction : The biological framework -- Neurophysiology, neural models, and language -- Distributed neural computers and feature detectors -- Automatization and syntax -- Syntax, words, and meaning -- Respiration, speech, and meaning -- Elephant ears, frogs, and human speech -- Speech is special -- Linguistic distinctions and auditory processes -- Man on the flying trapeze : the acquisition of speech -- Apes and children -- Evolution of human speech : comparative studies -- Evolution of human speech : the fossil record -- Conclusion : On the nature and evolution of the biological bases of language.},
  isbn = {0-674-07413-0},
  keywords = {\#nosource}
}

@misc{LimHowMay,
  title = {Lim: {{How}} May the Basal Ganglia Contribute to Auditory... - {{Google Scholar}}},
  url = {https://scholar.google.com/scholar_lookup?title=How\%20may\%20the\%20basal\%20ganglia\%20contribute\%20to\%20auditory\%20categorization\%20and\%20speech\%20perception\%3F\&author=S-J\%20Lim\&author=J\%20a.\%20Fiez\&author=LL\%20Holt\&publication_year=2014\&journal=Front\%20Neurosci\&volume=8\&pages=1-18},
  urldate = {2019-01-02},
  file = {/Users/jonny/Zotero/storage/T9WC42XT/scholar_lookup.html}
}

@incollection{Lindblom1986,
  title = {Phonetic {{Universals}} in {{Vowel Systems}}},
  booktitle = {Experimental {{Phonology}}},
  author = {Lindblom, Bjorn},
  year = {1986},
  pages = {13--44},
  isbn = {0-12-524940-3 (alk. paper) 0-12-524941-3 (paperback)},
  pmid = {308},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LindblomB/lindblom_1986_phonetic_universals_in_vowel_systems.pdf}
}

@incollection{Lindblom1988,
  title = {Phonetic {{Universals}} in {{Consonant Systems}}},
  booktitle = {Language, Speech, and Mind},
  author = {Lindblom, Bjorn and Maddieson, Ian},
  year = {1988},
  pages = {62--78},
  isbn = {0-12-524940-3 (alk. paper) 0-12-524941-3 (paperback)},
  pmid = {308}
}

@article{Lindblom2012,
  title = {Dissecting Coarticulation: {{How}} Locus Equations Happen},
  author = {Lindblom, Bj{\"o}rn and Sussman, Harvey M.},
  year = {2012},
  journal = {Journal of Phonetics},
  volume = {40},
  number = {1},
  pages = {1--19},
  issn = {00954470},
  doi = {10.1016/j.wocn.2011.09.005},
  abstract = {A programmatic series of studies aimed at expanding our understanding of coarticulation in V 1{$\cdot$}CV 2 sequences is presented. The common thread was examining coarticulatory dynamics through the prism of locus equations (LEs). Multiple experimental methodologies (articulatory synthesis, X-ray film, Principal Component Analysis, and extraction of time constants for F2 transitions), guided by a few theoretical assumptions about speech motor planning and control, were used to uncover the articulatory underpinnings responsible for the trademark acoustic form of LE scatterplots. Specific findings were: (1) the concept of a stop consonantal 'target' was quantitatively derived as a vowel-neutral, 'deactivated,' tongue contour; (2) the linearity of LEs is significantly enhanced by the uniformity of F2 transition time constants, which normalize with respect to F2 transition extents, and an inherent linear bias created by the smaller frequency range of [F2 onset-F2 vowel] relative to F2 vowel frequencies; (3) realistic LE slopes and y-intercepts were derived by modeling different extents of V 2 overlap onto stop consonantal target shapes at closure; and (4) a conceptually simple model, viz. interpolation between successive articulatory target shapes, followed by derivation of their formant values expressed as LEs, came surprisingly close to matching actual LEs obtained from our speaker. \textcopyright{} 2011 Elsevier Ltd.},
  isbn = {0095-4470},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LindblomB/lindblom_2012_dissecting_coarticulation.pdf}
}

@article{Lisker1977,
  title = {Rapid versus Rabid: {{A}} Catalogue of Acoustic Features That May Cue the Distinction},
  author = {Lisker, Leigh},
  year = {1977},
  journal = {The Journal of the Acoustical Society of America},
  volume = {62},
  number = {S1},
  pages = {S77},
  issn = {00014966},
  doi = {10.1121/1.2016377},
  abstract = {InAmerican English, initial /bdg/ often lack the acoustic feature takenas the defining feature of voiced stops; intervocalically before unstressedvowel /ptk/ lack aspiration, without which initial stops are notlabeled /ptk/. Initially, the two categories differ in the timingof vocal fold adduction and onset of fold vibration, andseveral acoustic cues, all tied to the VOT difference, havebeen studied. Medially there is also a difference in themanagement of the larynx, though it results in a phoneticallysimpler contrast, one of voicing with no accompanying aspiration difference.Acoustically, however, the list of features that play, or mightplausibly play a role is quite large. The word pairrapid-rabid, for example, might be affected by the following: (1)presence/absence of low-frequency buzz during the closure interval, (2) durationof closure, (3) F1 offset frequency before closure, (4) F1offset transition duration, (5) F1 onset frequency following closure, (6)F1 onset transition duration, (7) \ae{} duration, (8) F1 cut-backbefore closure, (9) F1 cutback following closure, (10) VOT cutbackbefore closure, (11) VOT delay after closure, (12) F0 contourbefore closure, (13) F0 contour after closure, (14) amplitude ofi relative to \ae, (15) decay time of glottal signalpreceding closure, (16) intensity of burst following closure. Even ifsome of these should turn out to be perceptually negligible,enough of them surely have cue value to make ita formidable task to justify preferring an acoustic to anarticulatory account of the distinction between the two English words.The support of the National Institute of Child Health andHuman Development is gratefully acknowledged. 1977 Acoustical Society of America},
  isbn = {0001-4966},
  keywords = {\#nosource}
}

@article{Lotto1997,
  title = {Animal Models of Speech Perception Phenomena},
  author = {Lotto, AJ and Kluender, KR and Holt, LL},
  year = {1997},
  journal = {Chicago Linguistic Society},
  url = {https://www.researchgate.net/profile/Keith_Kluender/publication/237280984_(from_K._Singer_R._Eggert__G._Anderson_(Eds.)_Chicago_Linguistic_Society_Volume_33_(Chicago_Linguistic_Society_Chicago)._pp._357-367_(1997).)_Animal_models_of_speech_perception_phen},
  urldate = {2017-02-02},
  keywords = {\#nosource,archived},
  annotation = {https://web.archive.org/web/20211013212325/https://www.researchgate.net/publication/237280984\_Animal\_Models\_of\_Speech\_Perception\_Phenomena},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LottoA/lotto_1997_animal_models_of_speech_perception_phenomena.pdf}
}

@article{Lowe2004,
  title = {Distinctive {{Image Features}} from {{Scale-Invariant Keypoints}}},
  author = {Lowe, D.},
  year = {2004},
  journal = {International Journal of Computer Vision},
  volume = {60},
  number = {2},
  pages = {91--110},
  keywords = {\#nosource}
}

@misc{Maechler2017,
  title = {Cluster: {{Cluster Analysis Basics}} and {{Extensions}}},
  author = {Maechler, Martin and Rousseeuw, Peter and Struyf, Anja and Hubert, Mia and Hornik, Kurt},
  year = {2017},
  keywords = {\#nosource}
}

@article{Malone2015,
  title = {Diverse Cortical Codes for Scene Segmentation in Primate Auditory Cortex},
  author = {Malone, Brian J. and Scott, Brian H. and Semple, Malcolm N.},
  year = {2015},
  journal = {Journal of Neurophysiology},
  volume = {113},
  number = {7},
  url = {http://jn.physiology.org/content/113/7/2934},
  urldate = {2017-08-08},
  keywords = {\#nosource}
}

@article{Margoliash1992,
  title = {Temporal and Harmonic Combination-Sensitive Neurons in the Zebra Finch's {{HVc}}.},
  author = {Margoliash, D and Fortune, E S},
  year = {1992},
  month = nov,
  journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  volume = {12},
  number = {11},
  pages = {4309--26},
  issn = {0270-6474},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/1432096},
  urldate = {2017-08-08},
  abstract = {Song learning shapes the response properties of auditory neurons in the song system to become highly selective for the individual bird's own ("autogenous") song. The auditory representation of autogenous song is achieved in part by neurons that exhibit facilitated responses to combinations of components of song. To understand the circuits that underlie these complex properties, the combination sensitivity of single units in the hyperstriatum ventrale, pars caudale (HVc) of urethane-anesthetized zebra finches was studied. Some neurons exhibited nonlinear temporal summation, spectral summation, or both. The majority of these neurons exhibited low spontaneous rates and phasic responses. Most combination-sensitive neurons required highly accurate copies of sounds derived from the autogenous song and responded weakly to tone bursts, combinations of simple stimuli, or conspecific songs. Temporal combination-sensitive (TCS) neurons required either two or more segments of a single syllable, or two or more syllables of the autogenous song, to elicit a facilitated, excitatory response. TCS neurons integrated auditory input over periods ranging from 80 to 350 msec, although this represents a lower limit. Harmonic combination-sensitive (HCS) neurons required combinations of two harmonics with particular frequency and temporal characteristics that were similar to autogenous song syllables. Both TCS and HCS neurons responded much more weakly when the dynamical spectral features of the autogenous song or syllables were modified than when the dynamical amplitude (waveform) features of the songs were modified. These results suggest that understanding the temporal dynamics of auditory responses in HVc may provide insight into neuronal circuits modified by song learning.},
  pmid = {1432096},
  keywords = {\#nosource}
}

@article{Mesgarani2014,
  title = {Phonetic Feature Encoding in Human Superior Temporal Gyrus.},
  author = {Mesgarani, Nima and Cheung, Connie and Johnson, Keith and Chang, Edward F},
  year = {2014},
  journal = {Science (New York, N.Y.)},
  volume = {343},
  number = {6174},
  pages = {1006--10},
  issn = {1095-9203},
  doi = {10.1126/science.1245994},
  url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4350233\&tool=pmcentrez\&rendertype=abstract},
  abstract = {During speech perception, linguistic elements such as consonants and vowels are extracted from a complex acoustic speech signal. The superior temporal gyrus (STG) participates in high-order auditory processing of speech, but how it encodes phonetic information is poorly understood. We used high-density direct cortical surface recordings in humans while they listened to natural, continuous speech to reveal the STG representation of the entire English phonetic inventory. At single electrodes, we found response selectivity to distinct phonetic features. Encoding of acoustic properties was mediated by a distributed population response. Phonetic features could be directly related to tuning for spectrotemporal acoustic cues, some of which were encoded in a nonlinear fashion or by integration of multiple cues. These findings demonstrate the acoustic-phonetic representation of speech in human STG.},
  isbn = {0036-8075},
  pmid = {24482117},
  keywords = {Auditory Cortex,Auditory Cortex: anatomy \& histology,Auditory Cortex: physiology,Female,Humans,Magnetic Resonance Imaging,Male,Phonetics,Speech Acoustics,Speech Perception},
  file = {/Users/jonny/Dropbox/papers/zotero/M/MesgaraniN/mesgarani_2014_phonetic_feature_encoding_in_human_superior_temporal_gyrus.pdf;/Users/jonny/Zotero/storage/AB44HGAW/Mesgarani et al. - 2014 - Phonetic feature encoding in human superior temporal gyrus(3).pdf}
}

@article{Mines1978,
  title = {Frequency of {{Occurrence}} of {{Phonemes}} in {{Conversational English}}},
  author = {Mines, M. Ardussi and Hanson, Barbara F. and Shoup, June E.},
  year = {1978},
  journal = {Language and Speech},
  volume = {21},
  number = {3},
  pages = {221=241},
  publisher = {{Sage PublicationsSage CA: Thousand Oaks, CA}},
  doi = {10.1177/002383097802100302},
  url = {http://journals.sagepub.com/doi/abs/10.1177/002383097802100302},
  urldate = {2017-05-04},
  abstract = {The phoneme identification process of an automatic speech recognition system may be aided through the use of statistics of phoneme occurrence in conversational English. These statistics are also applicable to the fields of linguistics and speech, to teaching English as a foreign language and to speech pathology. In this study a data base containing 103,887 phoneme occurrences taken from casual conversational American English was obtained through interviews of sixteen adult males and ten adult females. The speech was transcribed using a quasi-phonemic system, known as ARPAbet, plus selected phoneme alternates and was analysed with computer assistance to obtain the rank order of phonemes according to frequency of occurrence. Also, the radius of the confidence interval for the observed frequency of occurrence was calculated at the 95\% level for each phoneme. The top ten phonemes (in order, / a, n, t, i, s, r, i, l, d, {$\epsilon$} /) account for 47\% of all the data. As expected, the results of the present study correla...},
  file = {/Users/jonny/Zotero/storage/SC65EGXS/Mines et al. - 1978 - Frequency of Occurrence of Phonemes in Conversatio.pdf;/Users/jonny/Papers/MinesM/1978/Mines_1978_Frequency of Occurrence of Phonemes in Conversational English.pdf}
}

@article{Miyawaki1975,
  title = {An Effect of Linguistic Experience: {{The}} Discrimination of [r] and [1] by Native Speakers of {{Japanese}} and {{English}}},
  author = {Miyawaki, Kuniko and Strange, Winifred and Verbrugge, Robert and Liberman, Alvin M and Jenkins, James J and Fujimura, Osamu},
  year = {1975},
  month = sep,
  journal = {Perception \& Psychophysics},
  volume = {18},
  number = {5},
  pages = {331--340},
  publisher = {{Springer-Verlag}},
  issn = {0031-5117},
  doi = {10.3758/BF03211209},
  url = {http://www.springerlink.com/index/10.3758/BF03211209},
  urldate = {2016-10-21},
  abstract = {To test the effect of linguistic experience on the perception of a cue that is known to be effective in distinguishing between [1') and [I) in English, 21 Japanese and 39 American adults were tested on discrimination of a set of synthetic speech-like stimuli. The 13 "speech" stimuli in this set varied in the initial stationary frequency of the third formant (F3) and its subsequent transition into the vowel over a range sufficient to produce the perception of [1' a) and [I a) for American subjects and to produce [1' a) (which is not in phonemic contrast to [I a)) for Japanese subjects. Discrimination tests of a comparable set of stimuli consisting of the isolated F3 components provided a "nonspeech" control. For Americans, the discrimination of the speech stimuli was nearly categorical, i.e., comparison pairs which were identified as different phonemes were discriminated with high accuracy, while pairs which were identified as the same phoneme were discriminated relatively poorly. In comparison, discrimination of speech stimuli by Japanese subjects was only slightly better than chance for all comparison pairs. Performance on nonspeech stimuli, however, was virtually identical for Japanese and American subjects; both groups showed highly accurate discrimination of all comparison pairs. These results suggest that the effect of linguistic experience is specific to perception in the "speech mode." One way to examine the effect of linguistic experience on the perception of speech is to compare the discrimination of phonetic segments by two groups of speakers: one group speaks a language in},
  isbn = {0031-5117},
  keywords = {\#nosource}
}

@article{Moczulska2013,
  title = {Dynamics of Dendritic Spines in the Mouse Auditory Cortex during Memory Formation and Memory Recall.},
  author = {Moczulska, Kaja Ewa and {Tinter-Thiede}, Juliane and Peter, Manuel and Ushakova, Lyubov and Wernle, Tanja and Bathellier, Brice and Rumpel, Simon},
  year = {2013},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {110},
  number = {45},
  pages = {18315--20},
  issn = {1091-6490},
  doi = {10.1073/pnas.1312508110},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/24151334\%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3831433},
  abstract = {Long-lasting changes in synaptic connections induced by relevant experiences are believed to represent the physical correlate of memories. Here, we combined chronic in vivo two-photon imaging of dendritic spines with auditory-cued classical conditioning to test if the formation of a fear memory is associated with structural changes of synapses in the mouse auditory cortex. We find that paired conditioning and unpaired conditioning induce a transient increase in spine formation or spine elimination, respectively. A fraction of spines formed during paired conditioning persists and leaves a long-lasting trace in the network. Memory recall triggered by the reexposure of mice to the sound cue did not lead to changes in spine dynamics. Our findings provide a synaptic mechanism for plasticity in sound responses of auditory cortex neurons induced by auditory-cued fear conditioning; they also show that retrieval of an auditory fear memory does not lead to a recapitulation of structural plasticity in the auditory cortex as observed during initial memory consolidation.},
  isbn = {1091-6490 (Electronic)\textbackslash r0027-8424 (Linking)},
  pmid = {24151334},
  keywords = {auditory fear conditioning,learning,reconsolidation},
  file = {/Users/jonny/Dropbox/papers/zotero/M/MoczulskaK/moczulska_2013_dynamics_of_dendritic_spines_in_the_mouse_auditory_cortex_during_memory.pdf;/Users/jonny/Dropbox/papers/zotero/M/MoczulskaK/moczulska_2013_dynamics_of_dendritic_spines_in_the_mouse_auditory_cortex_during_memory2.pdf}
}

@article{Mottonen2006,
  title = {Perceiving Identical Sounds as Speech or Non-Speech Modulates Activity in the Left Posterior Superior Temporal Sulcus},
  author = {M{\"o}tt{\"o}nen, Riikka and Calvert, Gemma A. and J{\"a}{\"a}skel{\"a}inen, Iiro P. and Matthews, Paul M. and Thesen, Thomas and Tuomainen, Jyrki and Sams, Mikko},
  year = {2006},
  month = apr,
  journal = {NeuroImage},
  volume = {30},
  number = {2},
  pages = {563--569},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2005.10.002},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811905007676},
  abstract = {The left superior temporal cortex shows greater responsiveness to speech than to non-speech sounds according to previous neuroimaging studies, suggesting that this brain region has a special role in speech processing. However, since speech sounds differ acoustically from the non-speech sounds, it is possible that this region is not involved in speech perception per se, but rather in processing of some complex acoustic features. "Sine wave speech" (SWS) provides a tool to study neural speech specificity using identical acoustic stimuli, which can be perceived either as speech or non-speech, depending on previous experience of the stimuli. We scanned 21 subjects using 3T functional MRI in two sessions, both including SWS and control stimuli. In the pre-training session, all subjects perceived the SWS stimuli as non-speech. In the post-training session, the identical stimuli were perceived as speech by 16 subjects. In these subjects, SWS stimuli elicited significantly stronger activity within the left posterior superior temporal sulcus (STSp) in the post- vs. pre-training session. In contrast, activity in this region was not enhanced after training in 5 subjects who did not perceive SWS stimuli as speech. Moreover, the control stimuli, which were always perceived as non-speech, elicited similar activity in this region in both sessions. Altogether, the present findings suggest that activation of the neural speech representations in the left STSp might be a pre-requisite for hearing sounds as speech. ?? 2005 Elsevier Inc. All rights reserved.},
  isbn = {1053-8119 (Print)\textbackslash n1053-8119 (Linking)},
  pmid = {16275021},
  keywords = {fMRI,Sine wave speech,Speech,Superior temporal sulcus},
  file = {/Users/jonny/Zotero/storage/LJF9D5HG/Möttönen et al. - 2006 - Perceiving identical sounds as speech or non-speech modulates activity in the left posterior superior temp(3).pdf}
}

@article{Newell2002,
  title = {Categorical Perception of Familiar Objects},
  author = {Newell, Fiona N and B{\"u}lthoff, Heinrich H},
  year = {2002},
  journal = {Cognition},
  volume = {85},
  number = {2},
  pages = {113--143},
  issn = {00100277},
  doi = {10.1016/S0010-0277(02)00104-X},
  abstract = {We report three experiments where the categorical perception of familiar, three-dimensional objects was investigated. A continuum of shape change between 15 pairs of objects was created and the images along the continuum were used as stimuli. In Experiment 1 participants were first required to discriminate pairs of images of objects that lay along the shape continuum. Then participants were asked to classify each morph-image into one of two pre-specified classes. We found evidence for categorical perception in some but not all of our object pairs. In Experiment 2 we varied the viewpoint of the objects in the discrimination task and found that effects of categorical perception generalized across changes in view. In Experiment 3 similarity ratings for each object pair were collected. These similarity scores correlated with the degree of perceptual categorization found for the object pairs. Our findings suggest that some familiar objects are perceived categorically and that categorical perception is closely tied to inter-object perceptual similarity.},
  file = {/Users/jonny/Zotero/storage/Y5C4FHMJ/Newell, Bülthoff - 2002 - Categorical perception of familiar objects(3).pdf}
}

@article{Ng2002a,
  title = {On Generative vs. Discriminative Classifiers: {{A}} Comparison of Logistic Regression and Naive Bayes},
  author = {Ng, Andrew and Jordan, Michael I.},
  year = {2002},
  journal = {Proceedings of Advances in Neural Information Processing},
  volume = {28},
  number = {3},
  pages = {169--187},
  issn = {13704621},
  doi = {10.1007/s11063-008-9088-7},
  abstract = {Comparison of generative and discriminative classifiers is an ever-lasting topic. As an important contribution to this topic, based on their theoretical and empirical comparisons between the naive Bayes classifier and linear logistic regression, Ng and Jordan (NIPS 841-848, 2001) claimed that there exist two distinct regimes of performance between the generative and discriminative classifiers with regard to the training-set size. In this paper, our empirical and simulation studies, as a complement of their work, however, suggest that the existence of the two distinct regimes may not be so reliable. In addition, for real world datasets, so far there is no theoretically correct, general criterion for choosing between the discriminative and the generative approaches to classification of an observation x into a class y; the choice depends on the relative confidence we have in the correctness of the specification of either p(y vertical bar x) or p(x, y) for the data. This can be to some extent a demonstration of why Efron (J Am Stat Assoc 70(352):892-898, 1975) and O'Neill (J Am Stat Assoc 75(369):154-160, 1980) prefer normal-based linear discriminant analysis (LDA) when no model mis-specification occurs but other empirical studies may prefer linear logistic regression instead. Furthermore, we suggest that pairing of either LDA assuming a common diagonal covariance matrix (LDA-A) or the naive Bayes classifier and linear logistic regression may not be perfect, and hence it may not be reliable for any claim that was derived from the comparison between LDA-A or the naive Bayes classifier and linear logistic regression to be generalised to all generative and discriminative classifiers.},
  arxiv = {http://dx.doi.org/10.1007/s11063-008-9088-7},
  isbn = {1106300890},
  pmid = {25246403},
  keywords = {\#nosource,asymptotic relative efficiency,discriminative classifiers,generative classifiers,logistic regression,naive bayes classifier,normal based discriminant analysis}
}

@article{norman-haignereDivergenceFunctionalOrganization2019a,
  title = {Divergence in the Functional Organization of Human and Macaque Auditory Cortex Revealed by {{fMRI}} Responses to Harmonic Tones},
  author = {{Norman-Haignere}, Sam V. and Kanwisher, Nancy and McDermott, Josh H. and Conway, Bevil R.},
  year = {2019},
  month = jun,
  journal = {Nature Neuroscience},
  pages = {1},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0410-7},
  url = {https://www.nature.com/articles/s41593-019-0410-7},
  urldate = {2019-06-12},
  abstract = {Norman-Haignere et al. report that humans but not macaque monkeys possess cortical regions with a strong preference for harmonic tones compared to noise. This species difference may be driven by the demands of speech and music perception in humans.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/N/Norman-HaignereS/norman-haignere_2019_divergence_in_the_functional_organization_of_human_and_macaque_auditory_cortex.pdf;/Users/jonny/Zotero/storage/X6XLNTHQ/Norman-Haignere et al. - 2019 - Divergence in the functional organization of human.pdf;/Users/jonny/Zotero/storage/IYKYCJTX/s41593-019-0410-7.html}
}

@article{Norris2016,
  title = {Prediction, {{Bayesian}} Inference and Feedback in Speech Recognition.},
  author = {Norris, Dennis and McQueen, James M and Cutler, Anne},
  year = {2016},
  month = jan,
  journal = {Language, cognition and neuroscience},
  volume = {31},
  number = {1},
  pages = {4--18},
  publisher = {{Taylor \& Francis}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2015.1081703},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/26740960},
  urldate = {2017-01-26},
  abstract = {Speech perception involves prediction, but how is that prediction implemented? In cognitive models prediction has often been taken to imply that there is feedback of activation from lexical to pre-lexical processes as implemented in interactive-activation models (IAMs). We show that simple activation feedback does not actually improve speech recognition. However, other forms of feedback can be beneficial. In particular, feedback can enable the listener to adapt to changing input, and can potentially help the listener to recognise unusual input, or recognise speech in the presence of competing sounds. The common feature of these helpful forms of feedback is that they are all ways of optimising the performance of speech recognition using Bayesian inference. That is, listeners make predictions about speech because speech recognition is optimal in the sense captured in Bayesian models.},
  pmid = {26740960},
  keywords = {Bayesian inference,feedback,prediction,Speech recognition},
  file = {/Users/jonny/Zotero/storage/MCSABL2B/Norris, McQueen, Cutler - 2016 - Prediction, Bayesian inference and feedback in speech recognition(3).pdf}
}

@article{Nosofsky1988,
  title = {Similarity, Frequency, and Category Representations.},
  author = {Nosofsky, Robert M. and M., Robert},
  year = {1988},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {14},
  number = {1},
  pages = {54--65},
  publisher = {{American Psychological Association}},
  issn = {0278-7393},
  doi = {10.1037/0278-7393.14.1.54},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.14.1.54},
  urldate = {2017-05-05},
  keywords = {adults,classification \& typicality ratings,stimulus similarity \& frequency},
  file = {/Users/jonny/Papers/NosofskyR/1988/Nosofsky_1988_Similarity, frequency, and category representations.pdf}
}

@article{Ohala1996,
  title = {Speech Perception Is Hearing Sounds, Not Tongues.},
  author = {Ohala, J J},
  year = {1996},
  month = mar,
  journal = {The Journal of the Acoustical Society of America},
  volume = {99},
  number = {3},
  pages = {1718--25},
  issn = {0001-4966},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/8819861},
  urldate = {2017-02-08},
  abstract = {Three types of evidence are reviewed which cast doubt on claims that recovery of the speaker's articulations is an inherent part of speech perception: (a) Phonological data (e.g., universal tendencies of languages' segment inventories, phonotactic patterns, sound changes, etc.) show unmistakably that the acoustic-auditory properties of speech sounds, not their articulations, are the primary determinant of their behavior. (b) Infants and various nonhuman species can differentiate certain sound contrasts in human speech even though it is highly unlikely that they can deduce the vocal tract movements generating the sounds. (c) Humans can differentiate many nonspeech sounds almost as complex as speech, e.g., music, machine noises, as well as bird and monkey vocalizations, where there is little or no possibility of recovering the mechanisms producing the sounds.},
  pmid = {8819861},
  keywords = {\#nosource}
}

@article{Ohl,
  title = {Bilateral Ablation of Auditory Cortex in {{Mongolian}} Gerbil Affects Discrimination of Frequency Modulated Tones but Not of Pure Tones.},
  author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
  journal = {Learning \& memory (Cold Spring Harbor, N.Y.)},
  volume = {6},
  number = {4},
  pages = {347--62},
  issn = {1072-0502},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706},
  urldate = {2017-06-30},
  abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
  pmid = {10509706}
}

@article{Ohl1999,
  title = {Bilateral Ablation of Auditory Cortex in {{Mongolian}} Gerbil Affects Discrimination of Frequency Modulated Tones but Not of Pure Tones.},
  author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
  year = {1999},
  journal = {Learning \& memory (Cold Spring Harbor, N.Y.)},
  volume = {6},
  number = {4},
  pages = {347--62},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  issn = {1072-0502},
  doi = {10.1101/LM.6.4.347},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706},
  urldate = {2017-06-06},
  abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
  pmid = {10509706},
  file = {/Users/jonny/Papers/OhlF/1999/Ohl_1999_Bilateral ablation of auditory cortex in Mongolian gerbil affects2.pdf}
}

@article{Ohl1999a,
  title = {Bilateral Ablation of Auditory Cortex in {{Mongolian}} Gerbil Affects Discrimination of Frequency Modulated Tones but Not of Pure Tones.},
  author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
  year = {1999},
  journal = {Learning \& memory (Cold Spring Harbor, N.Y.)},
  volume = {6},
  number = {4},
  pages = {347--62},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  issn = {1072-0502},
  doi = {10.1101/LM.6.4.347},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706},
  urldate = {2017-06-06},
  abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
  pmid = {10509706},
  file = {/Users/jonny/Papers/OhlF/1999/Ohl_1999_Bilateral ablation of auditory cortex in Mongolian gerbil affects.pdf}
}

@article{Ohl2001,
  title = {Change in Pattern of Ongoing Cortical Activity with Auditory Category Learning},
  author = {Ohl, F. W. and Scheich, H. and Freeman, W. J.},
  year = {2001},
  month = aug,
  journal = {Nature},
  volume = {412},
  number = {6848},
  pages = {733--736},
  publisher = {{Nature Publishing Group}},
  issn = {0028-0836},
  doi = {10.1038/35089076},
  url = {http://www.nature.com/doifinder/10.1038/35089076},
  urldate = {2017-05-10},
  keywords = {\#nosource}
}

@article{Ohla,
  title = {Bilateral Ablation of Auditory Cortex in {{Mongolian}} Gerbil Affects Discrimination of Frequency Modulated Tones but Not of Pure Tones.},
  author = {Ohl, F W and Wetzel, W and Wagner, T and Rech, A and Scheich, H},
  journal = {Learning \& memory (Cold Spring Harbor, N.Y.)},
  volume = {6},
  number = {4},
  pages = {347--62},
  issn = {1072-0502},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/10509706},
  urldate = {2017-06-30},
  abstract = {This study examines the role of auditory cortex in the Mongolian gerbil in differential conditioning to pure tones and to linearly frequency-modulated (FM) tones by analyzing the effects of bilateral auditory cortex ablation. Learning behavior and performance were studied in a GO/NO-GO task aiming at avoidance of a mild foot shock by crossing a hurdle in a two-way shuttle box. Hurdle crossing as the conditioned response to the reinforced stimulus (CR+), as false alarm in response to the unreinforced stimulus (CR-), intertrial activity, and reaction times were monitored. The analysis revealed no effects of lesion on pure tone discrimination but impairment of FM tone discrimination. In the latter case lesion effects were dependent on timing of lesion relative to FM tone discrimination training. Lesions before training in naive animals led to a reduced CR+ rate and had no effect on CR- rate. Lesions in pretrained animals led to an increased CR- rate without effects on the CR+ rate. The results suggest that auditory cortex plays a more critical role in discrimination of FM tones than in discrimination of pure tones. The different lesion effects on FM tone discrimination before and after training are compatible with both the hypothesis of a purely sensory deficit in FM tone processing and the hypothesis of a differential involvement of auditory cortex in acquisition and retention, respectively.},
  pmid = {10509706}
}

@article{Pasley2012,
  title = {Reconstructing Speech from Human Auditory Cortex},
  author = {Pasley, Brian N. and David, Stephen V. and Mesgarani, Nima and Flinker, Adeen and Shamma, Shihab A. and Crone, Nathan E. and Knight, Robert T. and Chang, Edward F.},
  year = {2012},
  journal = {PLoS Biology},
  volume = {10},
  number = {1},
  issn = {15449173},
  doi = {10.1371/journal.pbio.1001251},
  abstract = {How the human auditory system extracts perceptually relevant acoustic features of speech is unknown. To address this question, we used intracranial recordings from nonprimary auditory cortex in the human superior temporal gyrus to determine what acoustic information in speech sounds can be reconstructed from population neural activity. We found that slow and intermediate temporal fluctuations, such as those corresponding to syllable rate, were accurately reconstructed using a linear model based on the auditory spectrogram. However, reconstruction of fast temporal fluctuations, such as syllable onsets and offsets, required a nonlinear sound representation based on temporal modulation energy. Reconstruction accuracy was highest within the range of spectro-temporal fluctuations that have been found to be critical for speech intelligibility. The decoded speech representations allowed readout and identification of individual words directly from brain activity during single trial sound presentations. These findings reveal neural encoding mechanisms of speech acoustic parameters in higher order human auditory cortex.},
  isbn = {1545-7885},
  pmid = {22303281},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PasleyB/pasley_2012_reconstructing_speech_from_human_auditory_cortex.pdf;/Users/jonny/Zotero/storage/N5PE7ILM/Pasley et al. - 2012 - Reconstructing speech from human auditory cortex(3).pdf}
}

@article{Patterson1971,
  title = {Recovery of {{Inter-Block Information}} When {{Block Sizes}} Are {{Unequal}}},
  author = {Patterson, H. D. and Thompson, R.},
  year = {1971},
  month = dec,
  journal = {Biometrika},
  volume = {58},
  number = {3},
  pages = {545},
  issn = {00063444},
  doi = {10.2307/2334389},
  url = {http://www.jstor.org/stable/2334389?origin=crossref},
  urldate = {2017-05-05},
  keywords = {\#nosource}
}

@misc{PDFBalancingPrediction,
  title = {(2) ({{PDF}}) {{Balancing Prediction}} and {{Sensory Input}} in {{Speech Comprehension}}: {{The Spatiotemporal Dynamics}} of {{Word Recognition}} in {{Context}}},
  shorttitle = {(2) ({{PDF}}) {{Balancing Prediction}} and {{Sensory Input}} in {{Speech Comprehension}}},
  journal = {ResearchGate},
  url = {https://www.researchgate.net/publication/329095360_Balancing_Prediction_and_Sensory_Input_in_Speech_Comprehension_The_Spatiotemporal_Dynamics_of_Word_Recognition_in_Context},
  urldate = {2019-07-12},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  langid = {english},
  file = {/Users/jonny/Zotero/storage/QZ6TKS5A/329095360_Balancing_Prediction_and_Sensory_Input_in_Speech_Comprehension_The_Spatiotemporal_Dyn.html}
}

@article{Pedregosa2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  eprint = {1201.0490},
  eprinttype = {arxiv},
  pages = {2825--2830},
  issn = {15324435},
  doi = {10.1007/s13398-014-0173-7.2},
  url = {http://dl.acm.org/citation.cfm?id=2078195\%5Cnhttp://arxiv.org/abs/1201.0490},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  archiveprefix = {arXiv},
  isbn = {1532-4435},
  pmid = {1000044560},
  keywords = {\#nosource}
}

@book{Perkell1986,
  title = {Invariance and Variability in Speech Processes},
  author = {Perkell, Joseph S. and Klatt, Dennis H. and Stevens, Kenneth N. and {Symposium on Invariance and Variability of Speech Processes (1983 : Massachusetts Institute of Technology)}},
  year = {1986},
  publisher = {{Lawrence Erlbaum Associates}},
  abstract = {Proceedings, in honor of Kenneth Stevens, of the Symposium on Invariance and Variability of Speech Processes, held Oct. 8-10, 1983 at M.I.T.},
  isbn = {0-89859-545-2},
  keywords = {\#nosource}
}

@inproceedings{Petek1993,
  title = {Exploiting Prediction Error in a Predictive-Based Connectionist Speech Recognition System},
  booktitle = {{{IEEE International Conference}} on {{Acoustics Speech}} and {{Signal Processing}}},
  author = {Petek, B. and Ferligoj, A.},
  year = {1993},
  pages = {267-270 vol.2},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.1993.319287},
  url = {http://ieeexplore.ieee.org/document/319287/},
  urldate = {2017-01-26},
  isbn = {0-7803-0946-4},
  keywords = {\#nosource}
}

@article{Peterson1952,
  title = {Control Methods Used in a Study of the Vowels},
  author = {Peterson, Gordon E and Barney, Harold L},
  year = {1952},
  journal = {The Journal of the Acoustical Society of America},
  volume = {24},
  number = {2},
  pages = {175--184},
  issn = {00014966},
  doi = {10.1121/1.1906875},
  abstract = {Relationships between a listener's identification of a spoken vowel and its properties as revealed from acoustic measurement of its sound wave have been a subject of study by many investigators.Both the utterance and the identification of a vowel depend upon the language and dialectal backgrounds and the vocal and auditory characteristics of the individuals concerned.The purpose of this paper is to discuss some of the control methods that have been used in the evaluation of these effects in a vowel study program at Bell Telephone Laboratories.The plan of the study, calibration of recording and measureing equipment, and methods for checking the performance of both speakers and listeners are described.The methods are illustrated from results of tests involving some 76 speakers and 70 listerners.},
  isbn = {0001-4966},
  keywords = {\#nosource}
}

@article{Poeppel2008,
  title = {Speech Perception at the Interface of Neurobiology and Linguistics},
  author = {Poeppel, D. and Idsardi, W. J and {van Wassenhove}, V.},
  year = {2008},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {363},
  number = {1493},
  pages = {1071--1086},
  issn = {0962-8436},
  doi = {10.1098/rstb.2007.2160},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/17890189},
  urldate = {2017-01-26},
  abstract = {Speech perception consists of a set of computations that take continuously varying acoustic waveforms as input and generate discrete representations that make contact with the lexical representations stored in long-term memory as output. Because the perceptual objects that are recognized by the speech perception enter into subsequent linguistic computation, the format that is used for lexical representation and processing fundamentally constrains the speech perceptual processes. Consequently, theories of speech perception must, at some level, be tightly linked to theories of lexical representation. Minimally, speech perception must yield representations that smoothly and rapidly interface with stored lexical items. Adopting the perspective of Marr, we argue and provide neurobiological and psychophysical evidence for the following research programme. First, at the implementational level, speech perception is a multi-time resolution process, with perceptual analyses occurring concurrently on at least two time scales (approx. 20-80 ms, approx. 150-300 ms), commensurate with (sub)segmental and syllabic analyses, respectively. Second, at the algorithmic level, we suggest that perception proceeds on the basis of internal forward models, or uses an 'analysis-by-synthesis' approach. Third, at the computational level (in the sense of Marr), the theory of lexical representation that we adopt is principally informed by phonological research and assumes that words are represented in the mental lexicon in terms of sequences of discrete segments composed of distinctive features. One important goal of the research programme is to develop linking hypotheses between putative neurobiological primitives (e.g. temporal primitives) and those primitives derived from linguistic inquiry, to arrive ultimately at a biologically sensible and theoretically satisfying model of representation and computation in speech.},
  pmid = {17890189},
  file = {/Users/jonny/Papers/PoeppelD/2008/Poeppel_2008_Speech perception at the interface of neurobiology and linguistics.pdf}
}

@article{Polley2006,
  title = {Perceptual {{Learning Directs Auditory Cortical Map Reorganization}} through {{Top-Down Influences}}},
  author = {Polley, D. B.},
  year = {2006},
  journal = {Journal of Neuroscience},
  volume = {26},
  number = {18},
  pages = {4970--4982},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.3771-05.2006},
  url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.3771-05.2006},
  abstract = {The primary sensory cortex is positioned at a confluence of bottom-up dedicated sensory inputs and top-down inputs related to higher-order sensory features, attentional state, and behavioral reinforcement. We tested whether topographic map plasticity in the adult primary auditory cortex and a secondary auditory area, the suprarhinal auditory field, was controlled by the statistics of bottom-up sensory inputs or by top-down task-dependent influences. Rats were trained to attend to independent parameters, either frequency or intensity, within an identical set of auditory stimuli, allowing us to vary task demands while holding the bottom-up sensory inputs constant. We observed a clear double-dissociation in map plasticity in both cortical fields. Rats trained to attend to frequency cues exhibited an expanded representation of the target frequency range within the tonotopic map but no change in sound intensity encoding compared with controls. Rats trained to attend to intensity cues expressed an increased proportion of nonmonotonic intensity response profiles preferentially tuned to the target intensity range but no change in tonotopic map organization relative to controls. The degree of topographic map plasticity within the task-relevant stimulus dimension was correlated with the degree of perceptual learning for rats in both tasks. These data suggest that enduring receptive field plasticity in the adult auditory cortex may be shaped by task-specific top-down inputs that interact with bottom-up sensory inputs and reinforcement-based neuromodulator release. Top-down inputs might confer the selectivity necessary to modify a single feature representation without affecting other spatially organized feature representations embedded within the same neural circuitry.},
  isbn = {1529-2401 (Electronic)},
  pmid = {16672673},
  keywords = {_tablet,attention,conditioning,cortex,plasticity,reward,topographic map},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PolleyD/polley_2006_perceptual_learning_directs_auditory_cortical_map_reorganization_through.pdf;/Users/jonny/Zotero/storage/MDHMPHT4/Polley, Steinberg, Merzenich - 2006 - Perceptual learning directs auditory cortical map reorganization through top-down influences(3).pdf}
}

@article{pritchettHighlevelLanguageProcessing2018,
  title = {High-Level Language Processing Regions Are Not Engaged in Action Observation or Imitation},
  author = {Pritchett, Brianna L. and Hoeflin, Caitlyn and Koldewyn, Kami and Dechter, Eyal and Fedorenko, Evelina},
  year = {2018},
  month = aug,
  journal = {Journal of Neurophysiology},
  volume = {120},
  number = {5},
  pages = {2555--2570},
  issn = {0022-3077},
  doi = {10.1152/jn.00222.2018},
  url = {https://www.physiology.org/doi/full/10.1152/jn.00222.2018},
  urldate = {2019-01-22},
  abstract = {A set of left frontal, temporal, and parietal brain regions respond robustly during language comprehension and production (e.g., Fedorenko E, Hsieh PJ, Nieto-Casta\~n\'on A, Whitfield-Gabrieli S, Kanwisher N. J Neurophysiol 104: 1177\textendash 1194, 2010; Menenti L, Gierhan SM, Segaert K, Hagoort P. Psychol Sci 22: 1173\textendash 1182, 2011). These regions have been further shown to be selective for language relative to other cognitive processes, including arithmetic, aspects of executive function, and music perception (e.g., Fedorenko E, Behr MK, Kanwisher N. Proc Natl Acad Sci USA 108: 16428\textendash 16433, 2011; Monti MM, Osherson DN. Brain Res 1428: 33\textendash 42, 2012). However, one claim about overlap between language and nonlinguistic cognition remains prominent. In particular, some have argued that language processing shares computational demands with action observation and/or execution (e.g., Rizzolatti G, Arbib MA. Trends Neurosci 21: 188\textendash 194, 1998; Koechlin E, Jubault T. Neuron 50: 963\textendash 974, 2006; Tettamanti M, Weniger D. Cortex 42: 491\textendash 494, 2006). However, the evidence for these claims is indirect, based on observing activation for language and action tasks within the same broad anatomical areas (e.g., on the lateral surface of the left frontal lobe). To test whether language indeed shares machinery with action observation/execution, we examined the responses of language brain regions, defined functionally in each individual participant (Fedorenko E, Hsieh PJ, Nieto-Casta\~n\'on A, Whitfield-Gabrieli S, Kanwisher N. J Neurophysiol 104: 1177\textendash 1194, 2010) to action observation (experiments 1, 2, and 3a) and action imitation (experiment 3b). With the exception of the language region in the angular gyrus, all language regions, including those in the inferior frontal gyrus (within ``Broca's area''), showed little or no response during action observation/imitation. These results add to the growing body of literature suggesting that high-level language regions are highly selective for language processing (see Fedorenko E, Varley R. Ann NY Acad Sci 1369: 132\textendash 153, 2016 for a review).NEW \& NOTEWORTHY Many have argued for overlap in the machinery used to interpret language and others' actions, either because action observation was a precursor to linguistic communication or because both require interpreting hierarchically-structured stimuli. However, existing evidence is indirect, relying on group analyses or reverse inference. We examined responses to action observation in language regions defined functionally in individual participants and found no response. Thus language comprehension and action observation recruit distinct circuits in the modern brain.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/P/PritchettB/pritchett_2018_high-level_language_processing_regions_are_not_engaged_in_action_observation_or.pdf;/Users/jonny/Zotero/storage/LWU2T3B2/Pritchett et al. - 2018 - High-level language processing regions are not eng.pdf;/Users/jonny/Zotero/storage/TT2WDCAB/jn.00222.html}
}

@article{Radziwon2009,
  title = {Behaviorally Measured Audiograms and Gap Detection Thresholds in {{CBA}}/{{CaJ}} Mice.},
  author = {Radziwon, Kelly E and June, Kristie M and Stolzberg, Daniel J and {Xu-Friedman}, Matthew A and Salvi, Richard J and Dent, Micheal L},
  year = {2009},
  month = oct,
  journal = {Journal of comparative physiology. A, Neuroethology, sensory, neural, and behavioral physiology},
  volume = {195},
  number = {10},
  pages = {961--9},
  publisher = {{NIH Public Access}},
  issn = {1432-1351},
  doi = {10.1007/s00359-009-0472-1},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/19756650},
  urldate = {2017-05-02},
  abstract = {Tone detection and temporal gap detection thresholds were determined in CBA/CaJ mice using a Go/No-go procedure and the psychophysical method of constant stimuli. In the first experiment, audiograms were constructed for five CBA/CaJ mice. Thresholds were obtained for eight pure tones ranging in frequency from 1 to 42 kHz. Audiograms showed peak sensitivity between 8 and 24 kHz, with higher thresholds at lower and higher frequencies. In the second experiment, thresholds for gap detection in broadband and narrowband noise bursts were measured at several sensation levels. For broadband noise, gap thresholds were between 1 and 2 ms, except at very low sensation levels, where thresholds increased significantly. Gap thresholds also increased significantly for low pass-filtered noise bursts with a cutoff frequency below 18 kHz. Our experiments revised absolute auditory thresholds in the CBA/CaJ mouse strain and demonstrated excellent gap detection ability in the mouse. These results add to the baseline behavioral data from normal-hearing mice which have become increasingly important for assessing auditory abilities in genetically altered mice.},
  pmid = {19756650},
  file = {/Users/jonny/Papers/RadziwonK/2009/Radziwon_2009_Behaviorally measured audiograms and gap detection thresholds in CBA-CaJ mice2.pdf}
}

@article{Ranasinghe2013,
  title = {Increasing Diversity of Neural Responses to Speech Sounds across the Central Auditory Pathway},
  author = {Ranasinghe, K. G. and Vrana, W. A. and Matney, C. J. and Kilgard, M. P.},
  year = {2013},
  journal = {Neuroscience},
  volume = {252},
  pages = {80--97},
  publisher = {{IBRO}},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2013.08.005},
  url = {http://dx.doi.org/10.1016/j.neuroscience.2013.08.005},
  abstract = {Neurons at higher stations of each sensory system are responsive to feature combinations not present at lower levels. As a result, the activity of these neurons becomes less redundant than lower levels. We recorded responses to speech sounds from the inferior colliculus and the primary auditory cortex neurons of rats, and tested the hypothesis that primary auditory cortex neurons are more sensitive to combinations of multiple acoustic parameters compared to inferior colliculus neurons. We independently eliminated periodicity information, spectral information and temporal information in each consonant and vowel sound using a noise vocoder. This technique made it possible to test several key hypotheses about speech sound processing. Our results demonstrate that inferior colliculus responses are spatially arranged and primarily determined by the spectral energy and the fundamental frequency of speech, whereas primary auditory cortex neurons generate widely distributed responses to multiple acoustic parameters, and are not strongly influenced by the fundamental frequency of speech. We found no evidence that inferior colliculus or primary auditory cortex was specialized for speech features such as voice onset time or formants. The greater diversity of responses in primary auditory cortex compared to inferior colliculus may help explain how the auditory system can identify a wide range of speech sounds across a wide range of conditions without relying on any single acoustic cue. ?? 2013 IBRO.},
  isbn = {1873-7544 (Electronic) 0306-4522 (Linking)},
  pmid = {23954862},
  keywords = {Multiple acoustic parameters,Neural response diversity,Noise-vocoded speech,Rat auditory system,Redundancy reduction},
  file = {/Users/jonny/Zotero/storage/FB537B2Y/Ranasinghe et al. - 2013 - Increasing diversity of neural responses to speech sounds across the central auditory pathway(3).pdf}
}

@article{Rauschecker1998b,
  title = {Cortical Processing of Complex Sounds},
  author = {Rauschecker, Josef P},
  year = {1998},
  month = aug,
  journal = {Current Opinion in Neurobiology},
  volume = {8},
  number = {4},
  pages = {516--521},
  issn = {09594388},
  doi = {10.1016/S0959-4388(98)80040-8},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0959438898800408},
  urldate = {2017-08-08},
  keywords = {\#nosource}
}

@article{Rauschecker2009a,
  title = {Maps and Streams in the Auditory Cortex: Nonhuman Primates Illuminate Human Speech Processing},
  author = {Rauschecker, Josef P and Scott, Sophie K},
  year = {2009},
  month = jun,
  journal = {Nature Neuroscience},
  volume = {12},
  number = {6},
  pages = {718--724},
  issn = {1097-6256},
  doi = {10.1038/nn.2331},
  url = {http://www.nature.com/doifinder/10.1038/nn.2331},
  abstract = {Speech and language are considered uniquely human abilities: animals have communication systems, but they do not match human linguistic skills in terms of recursive structure and combinatorial power. Yet, in evolution, spoken language must have emerged from neural mechanisms at least partially available in animals. In this paper, we will demonstrate how our understanding of speech perception, one important facet of language, has profited from findings and theory in nonhuman primate studies. Chief among these are physiological and anatomical studies showing that primate auditory cortex, across species, shows patterns of hierarchical structure, topographic mapping and streams of functional processing. We will identify roles for different cortical areas in the perceptual processing of speech and review functional imaging work in humans that bears on our understanding of how the brain decodes and monitors speech. A new model connects structures in the temporal, frontal and parietal lobes linking speech perception and production.},
  isbn = {1546-1726 (Electronic)\textbackslash r1097-6256 (Linking)},
  pmid = {19471271},
  keywords = {Anatomy,Animals,Auditory Cortex,Auditory Cortex: anatomy \& histology,Auditory Cortex: physiology,Auditory Pathways,Auditory Pathways: anatomy \& histology,Auditory Pathways: physiology,Biological Evolution,Brain Mapping,Comparative,Humans,Models,Nerve Net,Nerve Net: anatomy \& histology,Nerve Net: physiology,Neurological,Primates,Primates: anatomy \& histology,Primates: physiology,Speech Perception,Speech Perception: physiology},
  file = {/Users/jonny/Zotero/storage/HSTBTCD9/Rauschecker, Scott - 2009 - Maps and streams in the auditory cortex nonhuman primates illuminate human speech processing(3).pdf}
}

@article{Repp1989,
  title = {Acoustic Properties and Perception of Stop Consonant Release Transients.},
  author = {Repp, B H and Lin, H B},
  year = {1989},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {85},
  number = {1},
  pages = {379--96},
  issn = {0001-4966},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/2921420},
  urldate = {2017-02-14},
  abstract = {This study focuses on the initial component of the stop consonant release burst, the release transient. In theory, the transient, because of its impulselike source, should contain much information about the vocal tract configuration at release, but it is usually weak in intensity and difficult to isolate from the accompanying frication in natural speech. For this investigation, a human talker produced isolated release transients of /b,d,g/ in nine vocalic contexts by whispering these syllables very quietly. He also produced the corresponding CV syllables with regular phonation for comparison. Spectral analyses showed the isolated transients to have a clearly defined formant structure, which was not seen in natural release bursts, whose spectra were dominated by the frication noise. The formant frequencies varied systematically with both consonant place of articulation and vocalic context. Perceptual experiments showed that listeners can identify both consonants and vowels from isolated transients, though not very accurately. Knowing one of the two segments in advance did not help, but when the transients were followed by a compatible synthetic, steady-state vowel, consonant identification improved somewhat. On the whole, isolated transients, despite their clear formant structure, provided only partial information for consonant identification, but no less so, it seems, than excerpted natural release bursts. The information conveyed by artificially isolated transients and by natural (frication-dominated) release bursts appears to be perceptually equivalent.},
  pmid = {2921420},
  keywords = {\#nosource}
}

@article{Rosenblum2008,
  title = {Speech {{Perception}} as a {{Multimodal Phenomenon}}},
  author = {Rosenblum, Lawrence D.},
  year = {2008},
  month = dec,
  journal = {Current Directions in Psychological Science},
  volume = {17},
  number = {6},
  pages = {405--409},
  publisher = {{SAGE PublicationsSage CA: Los Angeles, CA}},
  issn = {0963-7214},
  doi = {10.1111/j.1467-8721.2008.00615.x},
  url = {http://journals.sagepub.com/doi/10.1111/j.1467-8721.2008.00615.x},
  urldate = {2017-05-01},
  abstract = {Speech perception is inherently multimodal. Visual speech (lip-reading) information is used by all perceivers and readily integrates with auditory speech. Imaging research suggests that the brain treats auditory and visual speech similarly. These findings have led some researchers to consider that speech perception works by extracting amodal information that takes the same form across modalities. From this perspective, speech integration is a property of the input information itself. Amodal speech information could explain the reported automaticity, immediacy, and completeness of audiovisual speech integration. However, recent findings suggest that speech integration can be influenced by higher cognitive properties such as lexical status and semantic context. Proponents of amodal accounts will need to explain these results.},
  keywords = {audiovisual,lip reading,multimodal,speech},
  file = {/Users/jonny/Papers/RosenblumL/2008/Rosenblum_2008_Speech Perception as a Multimodal Phenomenon2.pdf}
}

@article{Rutishauser2015,
  title = {Computation in {{Dynamically Bounded Asymmetric Systems}}},
  author = {Rutishauser, Ueli and Slotine, Jean Jacques and Douglas, Rodney},
  year = {2015},
  journal = {PLoS Computational Biology},
  volume = {11},
  number = {1},
  pages = {e1004039},
  issn = {15537358},
  doi = {10.1371/journal.pcbi.1004039},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1004039},
  abstract = {Previous explanations of computations performed by recurrent networks have focused on symmetrically connected saturating neurons and their convergence toward attractors. Here we analyze the behavior of asymmetrical connected networks of linear threshold neurons, whose positive response is unbounded. We show that, for a wide range of parameters, this asymmetry brings interesting and computationally useful dynamical properties. When driven by input, the network explores potential solutions through highly unstable 'expansion' dynamics. This expansion is steered and constrained by negative divergence of the dynamics, which ensures that the dimensionality of the solution space continues to reduce until an acceptable solution manifold is reached. Then the system contracts stably on this manifold towards its final solution trajectory. The unstable positive feedback and cross inhibition that underlie expansion and divergence are common motifs in molecular and neuronal networks. Therefore we propose that very simple organizational constraints that combine these motifs can lead to spontaneous computation and so to the spontaneous modification of entropy that is characteristic of living systems.},
  pmid = {25617645},
  file = {/Users/jonny/Dropbox/papers/zotero/R/RutishauserU/rutishauser_2015_computation_in_dynamically_bounded_asymmetric_systems.pdf}
}

@article{Sadagopan2009,
  title = {Nonlinear {{Spectrotemporal Interactions Underlying Selectivity}} for {{Complex Sounds}} in {{Auditory Cortex}}},
  author = {Sadagopan, S. and Wang, X.},
  year = {2009},
  month = sep,
  journal = {Journal of Neuroscience},
  volume = {29},
  number = {36},
  pages = {11192--11202},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.1286-09.2009},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/19741126},
  urldate = {2017-08-08},
  abstract = {In the auditory cortex of awake animals, a substantial number of neurons do not respond to pure tones. These neurons have historically been classified as "unresponsive" and even been speculated as being nonauditory. We discovered, however, that many of these neurons in the primary auditory cortex (A1) of awake marmoset monkeys were in fact highly selective for complex sound features. We then investigated how such selectivity might arise from the tone-tuned inputs that these neurons likely receive. We found that these non-tone responsive neurons exhibited nonlinear combination-sensitive responses that require precise spectral and temporal combinations of two tone pips. The nonlinear spectrotemporal maps derived from these neurons were correlated with their selectivity for complex acoustic features. These non-tone responsive and nonlinear neurons were commonly encountered at superficial cortical depths in A1. Our findings demonstrate how temporally and spectrally specific nonlinear integration of putative tone-tuned inputs might underlie a diverse range of high selectivity of A1 neurons in awake animals. We propose that describing A1 neurons with complex response properties in terms of tone-tuned input channels can conceptually unify a wide variety of observed neural selectivity to complex sounds into a lower dimensional description.},
  pmid = {19741126},
  file = {/Users/jonny/Papers/SadagopanS/2009/Sadagopan_2009_Nonlinear Spectrotemporal Interactions Underlying Selectivity for Complex.pdf}
}

@inproceedings{Salakhutdinov2011a,
  title = {Learning to Share Visual Appearance for Multiclass Object Detection},
  booktitle = {{{CVPR}} 2011},
  author = {Salakhutdinov, Ruslan and Torralba, Antonio and Tenenbaum, Josh},
  year = {2011},
  month = jun,
  pages = {1481--1488},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2011.5995720},
  url = {http://ieeexplore.ieee.org/document/5995720/},
  urldate = {2017-05-05},
  isbn = {978-1-4577-0394-2},
  file = {/Users/jonny/Papers/SalakhutdinovR/2011/Salakhutdinov_2011_Learning to share visual appearance for multiclass object detection2.pdf}
}

@book{Saussure1916,
  title = {Cours de Linguistique G\'en\'erale.},
  author = {de Saussure, Ferdinand},
  editor = {Bally, C and Sechehaye, A and Reidlinger, A},
  year = {1916},
  publisher = {{Payot}},
  address = {{Lausanne, Paris}},
  keywords = {\#nosource}
}

@techreport{schatzEarlyPhoneticLearning2019,
  type = {Preprint},
  title = {Early Phonetic Learning without Phonetic Categories -- {{Insights}} from Machine Learning},
  author = {Schatz, Thomas and Feldman, Naomi and Goldwater, Sharon and Cao, Xuan Nga and Dupoux, Emmanuel},
  year = {2019},
  month = may,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/fc4wh},
  url = {https://osf.io/fc4wh},
  urldate = {2019-08-20},
  abstract = {Decades of work have shown that even before they can talk, human infants are quickly tuning into the properties of the language(s) spoken in their environment. By their first birthday, they have already become attuned to the sounds of their language(s), processing native phonetic contrasts more easily than non-native ones. For example, between 6-8 months and 10-12 months, infants learning American English get better at distinguishing American English [{$\Elztrnr$}] and [l], as in `rock' vs `lock', relative to infants learning Japanese. This phenomenon has been dubbed early phonetic learning and has been taken as evidence that, during the first year of life, infants form phonetic categories like [{$\Elztrnr$}] and [l]. We present evidence that calls this interpretation into question. We show that a machine learning model trained without supervision on raw, unsegmented speech recordings can predict the documented changes in discrimination of [{$\Elztrnr$}] and [l] in Japanese and American English infants\textemdash but that it does so by learning units that are too localised in time and acoustically variable to correspond to phonetic categories. These results constitute the first demonstration of a feasible mechanism for early phonetic learning under realistic learning conditions. They help resolve a tension between the hypothesis that early phonetic learning is driven by statistical analysis of the speech sounds in the infant's environment, and the difficulty encountered by computational models to robustly discover phonetic categories using such mechanisms. More broadly, our results challenge the view commonly held for several decades that perceptual changes in infancy constitute evidence for the early formation of phonetic categories, and invite us to reconsider the nature of early linguistic knowledge. Our model provides one possible alternative to phonetic categories, but many others can and should be investigated.},
  keywords = {\#nosource}
}

@article{schiavoCapacitiesNeuralMechanisms2019,
  title = {Capacities and Neural Mechanisms for Auditory Statistical Learning across Species},
  author = {Schiavo, Jennifer K. and Froemke, Robert C.},
  year = {2019},
  month = may,
  journal = {Hearing Research},
  series = {Annual {{Reviews}} 2019},
  volume = {376},
  pages = {97--110},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2019.02.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0378595518304441},
  urldate = {2019-08-20},
  abstract = {Statistical learning has been proposed as a possible mechanism by which individuals can become sensitive to the structures of language fundamental for speech perception. Since its description in human infants, statistical learning has been described in human adults and several non-human species as a general process by which animals learn about stimulus-relevant statistics. The neurobiology of statistical learning is beginning to be understood, but many questions remain about the underlying mechanisms. Why is the developing brain particularly sensitive to stimulus and environmental statistics, and what neural processes are engaged in the adult brain to enable learning from statistical regularities in the absence of external reward or instruction? This review will survey the statistical learning abilities of humans and non-human animals with a particular focus on communicative vocalizations. We discuss the neurobiological basis of statistical learning, and specifically what can be learned by exploring this process in both humans and laboratory animals. Finally, we describe advantages of studying vocal communication in rodents as a means to further our understanding of the cortical plasticity mechanisms engaged during statistical learning. We examine the use of rodents in the context of pup retrieval, which is an auditory-based and experience-dependent form of maternal behavior.},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SchiavoJ/schiavo_2019_capacities_and_neural_mechanisms_for_auditory_statistical_learning_across.pdf;/Users/jonny/Zotero/storage/VZURHLXR/S0378595518304441.html}
}

@article{Schindelin2012,
  title = {Fiji: An Open-Source Platform for Biological-Image Analysis},
  author = {Schindelin, Johannes and {Arganda-Carreras}, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert},
  year = {2012},
  journal = {Nature Methods},
  volume = {9},
  number = {7},
  pages = {676--682},
  issn = {1548-7091},
  doi = {10.1038/nmeth.2019},
  url = {http://www.nature.com/doifinder/10.1038/nmeth.2019},
  abstract = {Fiji is a distribution of the popular open-source software ImageJ focused on biological-image analysis. Fiji uses modern software engineering practices to combine powerful software libraries with a broad range of scripting languages to enable rapid prototyping of image-processing algorithms. Fiji facilitates the transformation of new algorithms into ImageJ plugins that can be shared with end users through an integrated update system. We propose Fiji as a platform for productive collaboration between computer science and biology research communities.},
  arxiv = {1081-8693},
  isbn = {1548-7105 (Electronic)\textbackslash r1548-7091 (Linking)},
  pmid = {22743772},
  keywords = {*Software,\#nosource,Algorithms,Animals,Brain/ultrastructure,Computational Biology/*methods,Drosophila melanogaster/ultrastructure,Image Enhancement/methods,Image Processing; Computer-Assisted/*methods,Imaging; Three-Dimensional/methods,Information Dissemination,Software Design}
}

@article{Schindelin2015,
  title = {The {{ImageJ}} Ecosystem: {{An}} Open Platform for Biomedical Image Analysis},
  author = {Schindelin, Johannes and Rueden, Curtis T. and Hiner, Mark C. and Eliceiri, Kevin W.},
  year = {2015},
  journal = {Molecular Reproduction and Development},
  volume = {82},
  number = {7-8},
  pages = {518--529},
  issn = {10982795},
  doi = {10.1002/mrd.22489},
  abstract = {Technology in microscopy advances rapidly, enabling increasingly affordable, faster, and more precise quantitative biomedical imaging, which necessitates correspondingly more-advanced image processing and analysis techniques. A wide range of software is available\textemdash from commercial to academic, special-purpose to Swiss army knife, small to large\textemdash but a key characteristic of software that is suitable for scientific inquiry is its accessibility. Open-source software is ideal for scientific endeavors because it can be freely inspected, modified, and redistributed; in particular, the open-software platform ImageJ has had a huge impact on the life sciences, and continues to do so. From its inception, ImageJ has grown significantly due largely to being freely available and its vibrant and helpful user community. Scientists as diverse as interested hobbyists, technical assistants, students, scientific staff, and advanced biology researchers use ImageJ on a daily basis, and exchange knowledge via its dedicated mailing list. Uses of ImageJ range from data visualization and teaching to advanced image processing and statistical analysis. The software's extensibility continues to attract biologists at all career stages as well as computer scientists who wish to effectively implement specific image-processing algorithms. In this review, we use the ImageJ project as a case study of how open-source software fosters its suites of software tools, making multitudes of image-analysis technology easily accessible to the scientific community. We specifically explore what makes ImageJ so popular, how it impacts the life sciences, how it inspires other projects, and how it is self-influenced by coevolving projects within the ImageJ ecosystem. Mol. Reprod. Dev. 82: 518\textendash 529, 2015. \textcopyright{} 2015 Wiley Periodicals, Inc.},
  isbn = {1098-2795},
  pmid = {26153368},
  keywords = {\#nosource}
}

@inproceedings{Schouten2003,
  title = {The End of Categorical Perception as We Know It},
  booktitle = {Speech {{Communication}}},
  author = {Schouten, Bert and Gerrits, Ellen and Van Hessen, Arjan},
  year = {2003},
  volume = {41},
  pages = {71--80},
  issn = {01676393},
  doi = {10.1016/S0167-6393(02)00094-8},
  abstract = {Comparing phoneme classification and discrimination (or "categorical perception") of a stimulus continuum has for a long time been regarded as a useful method for investigating the storage and retrieval of phoneme categories in long-term memory. The closeness of the relationship between the two tasks, i.e. the degree of categorical perception, depends on a number of factors, some of which are unknown or random. One very important factor, however, seems to be the degree of bias (in the signal-detection sense of the term) in the discrimination task. When the task is such (as it is in 2IFC, for example) that the listener has to rely heavily on an internal, subjective, criterion, discrimination can seem to be almost perfectly categorical, if the stimuli are natural enough. Presenting the same stimuli in a much less biasing task, however, leads to discrimination results that are completely unrelated to phoneme classification. Even the otherwise ubiquitous peak at the phoneme boundary has disappeared. The traditional categorical-perception experiment measures the bias inherent in the discrimination task; if we want to know how speech sounds are categorized, we will have to look elsewhere. \textcopyright{} 2002 Elsevier Science B.V. All rights reserved.},
  isbn = {0167-6393},
  keywords = {\#nosource,Categorical perception}
}

@article{Schultz2016,
  title = {Dopamine Reward Prediction Error Coding},
  author = {Schultz, Wolfram},
  year = {2016},
  journal = {Dialogues in Clinical Neuroscience},
  volume = {18},
  number = {1},
  eprint = {gr-qc/9809069v1},
  eprinttype = {arxiv},
  pages = {23--32},
  publisher = {{Nature Publishing Group}},
  issn = {12948322},
  doi = {10.1038/nrn.2015.26},
  url = {http://dx.doi.org/10.1038/nrn.2015.26},
  abstract = {Reward prediction errors consist of the differences between received and predicted rewards. They are crucial for basic forms of learning about rewards and make us strive for more rewards-an evolutionary beneficial trait. Most dopamine neurons in the midbrain of humans, monkeys, and rodents signal a reward prediction error; they are activated by more reward than predicted (positive prediction error), remain at baseline activity for fully predicted rewards, and show depressed activity with less reward than predicted (negative prediction error). The dopamine signal increases nonlinearly with reward value and codes formal economic utility. Drugs of addiction generate, hijack, and amplify the dopamine reward signal and induce exaggerated, uncontrolled dopamine effects on neuronal plasticity. The striatum, amygdala, and frontal cortex also show reward prediction error coding, but only in subpopulations of neurons. Thus, the important concept of reward prediction errors is implemented in neuronal hardware.\textbackslash n\textbackslash nAbstract available from the publisher.\textbackslash n\textbackslash nAbstract available from the publisher.},
  archiveprefix = {arXiv},
  isbn = {3-540-27590-8},
  pmid = {27069377},
  keywords = {Dopamine,Neuron,Neurophysiology,Prediction,Reward,Striatum,Substantia nigra,Ventral tegmental area},
  file = {/Users/jonny/Zotero/storage/AZ76YAD4/Schultz - 2016 - Dopamine reward prediction error coding(3).pdf}
}

@article{Schwartz2012,
  title = {Grounding Stop Place Systems in the Perceptuo-Motor Substance of Speech: {{On}} the Universality of the Labial\textendash Coronal\textendash Velar Stop Series},
  author = {Schwartz, Jean-Luc and Bo{\"e}, Louis-Jean and Badin, Pierre and Sawallis, Thomas R.},
  year = {2012},
  journal = {Journal of Phonetics},
  volume = {40},
  number = {1},
  pages = {20--36},
  issn = {00954470},
  doi = {10.1016/j.wocn.2011.10.004},
  abstract = {Vowels are by far the best understood units in human sound systems, and are well characterized at the articulatory, acoustic, and perceptual levels. This has permitted explanations of vowel systems as structured by perception, and has led to effective substance-based theories. By contrast, stops are far less thoroughly understood. In this paper we use an articulatory-acoustic model of the vocal tract to examine stop consonant place in terms of both articulation and formant values. This allows us to locate each place of articulation in the F1-F2-F3 space, and to demonstrate in "articulatory nomograms" how formants evolve while closure is displaced from the front to the back of the vocal tract. Then, in the framework of the "Perception for Action Control Theory" that we have developed in recent years, we show that the near universal labial-coronal-velar stop series (i.e., /b d g/ or /p t k/) is a perceptually optimal structure for stops just as /i a u/ is for vowels, provided that it is embedded in a suitable perceptuo-motor framework. ?? 2011 Elsevier Ltd.},
  isbn = {0095-4470},
  file = {/Users/jonny/Zotero/storage/CZGBCANQ/Schwartz et al. - 2012 - Grounding stop place systems in the perceptuo-motor substance of speech On the universality of the labial–(3).pdf}
}

@article{Shannon1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E},
  year = {1948},
  journal = {The Bell System Technical Journal},
  volume = {27},
  number = {July 1928},
  eprint = {chao-dyn/9411012},
  eprinttype = {arxiv},
  pages = {379--423},
  issn = {15591662},
  doi = {10.1145/584091.584093},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
  archiveprefix = {arXiv},
  isbn = {0252725484},
  pmid = {9230594},
  file = {/Users/jonny/Papers/ShannonC/undefined/Shannon_A Mathematical Theory of Communication.pdf;/Users/jonny/Papers/ShannonC/undefined/Shannon_A Mathematical Theory of Communication2.pdf;/Users/jonny/Zotero/storage/2Q6GPIMY/Shannon - 1948 - A mathematical theory of communication(6).pdf;/Users/jonny/Zotero/storage/GRVC4NUR/Shannon - 1948 - A mathematical theory of communication(5).pdf}
}

@article{Shepard1987,
  title = {Toward a Universal Law of Generalization for Psychological Science},
  author = {Shepard, R.},
  year = {1987},
  journal = {Science},
  volume = {237},
  number = {4820},
  pages = {1317--1323},
  issn = {0036-8075},
  doi = {10.1126/science.3629243},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.3629243},
  abstract = {A psychological space is established for any set of stimuli by determining metric distances between the stimuli such that the probability that a response learned to any stimulus will generalize to any other is an invariant monotonic function of the distance between them. To a good approximation, this probability of generalization (i) decays exponentially with this distance, and (ii) does so in accordance with one of two metrics, depending on the relation between the dimensions along which the stimuli vary. These empirical regularities are mathematically derivable from universal principles of natural kinds and probabilistic geometry that may, through evolutionary internalization, tend to govern the behaviors of all sentient organisms.},
  isbn = {0036-8075 (Print)\textbackslash r0036-8075 (Linking)},
  pmid = {3629243},
  keywords = {\#nosource}
}

@article{sjerpsSpeakernormalizedSoundRepresentations2019a,
  title = {Speaker-Normalized Sound Representations in the Human Auditory Cortex},
  author = {Sjerps, Matthias J. and Fox, Neal P. and Johnson, Keith and Chang, Edward F.},
  year = {2019},
  month = jun,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {2465},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10365-z},
  abstract = {The acoustic dimensions that distinguish speech sounds (like the vowel differences in "boot" and "boat") also differentiate speakers' voices. Therefore, listeners must normalize across speakers without losing linguistic information. Past behavioral work suggests an important role for auditory contrast enhancement in normalization: preceding context affects listeners' perception of subsequent speech sounds. Here, using intracranial electrocorticography in humans, we investigate whether and how such context effects arise in auditory cortex. Participants identified speech sounds that were preceded by phrases from two different speakers whose voices differed along the same acoustic dimension as target words (the lowest resonance of the vocal tract). In every participant, target vowels evoke a speaker-dependent neural response that is consistent with the listener's perception, and which follows from a contrast enhancement model. Auditory cortex processing thus displays a critical feature of normalization, allowing listeners to extract meaningful content from the voices of diverse speakers.},
  langid = {english},
  pmid = {31165733},
  keywords = {_tablet},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SjerpsM/sjerps_2019_speaker-normalized_sound_representations_in_the_human_auditory_cortex.pdf}
}

@article{Steinschneider2003,
  title = {Representation of the Voice Onset Time ({{VOT}}) Speech Parameter in Population Responses within Primary Auditory Cortex of the Awake Monkey.},
  author = {Steinschneider, Mitchell and Fishman, Yonatan I and Arezzo, Joseph C},
  year = {2003},
  journal = {The Journal of the Acoustical Society of America},
  volume = {114},
  number = {1},
  pages = {307--321},
  issn = {00014966},
  doi = {10.1121/1.1582449},
  abstract = {Voice onset time (VOT) signifies the interval between consonant onset and the start of rhythmic vocal-cord vibrations. Differential perception of consonants such as /d/ and /t/ is categorical in American English, with the boundary generally lying at a VOT of 20-40 ms. This study tests whether previously identified response patterns that differentially reflect VOT are maintained in large-scale population activity within primary auditory cortex (A1) of the awake monkey. Multiunit activity and current source density patterns evoked by the syllables /da/ and /ta/ with variable VOTs are examined. Neural representation is determined by the tonotopic organization. Differential response patterns are restricted to lower best-frequency regions. Response peaks time-locked to both consonant and voicing onsets are observed for syllables with a 40- and 60-ms VOT, whereas syllables with a 0- and 20-ms VOT evoke a single response time-locked only to consonant onset. Duration of aspiration noise is represented in higher best-frequency regions. Representation of VOT and aspiration noise in discrete tonotopic areas of A1 suggest that integration of these phonetic cues occurs in secondary areas of auditory cortex. Findings are consistent with the evolving concept that complex stimuli are encoded by synchronized activity in large-scale neuronal ensembles.},
  isbn = {00014966},
  pmid = {12880043},
  file = {/Users/jonny/Zotero/storage/MUFW5DKL/Steinschneider, Fishman, Arezzo - 2003 - Representation of the voice onset time (VOT) speech parameter in population responses within(3).pdf}
}

@article{Stevens,
  title = {Diverse Acoustic Cues at Consonantal Landmarks.},
  author = {Stevens, K N},
  journal = {Phonetica},
  volume = {57},
  number = {2-4},
  pages = {139--51},
  issn = {0031-8388},
  doi = {28468},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/10992135},
  urldate = {2017-02-08},
  abstract = {The consonantal segments that underlie an utterance are manifested in the acoustic signal by abrupt discontinuities or dislocations in the spectral pattern. There are potentially two such discontinuities for each consonant, corresponding to the formation and release of a constriction in the oral cavity by the lips, the tongue blade, or the tongue body. Acoustic cues for the various consonant features of place, voicing and nasality reside in the signal in quite different forms on the two sides of each acoustic discontinuity. Examples of these diverse cues and their origin in acoustic theory are reviewed, with special attention to place features and features related to the laryngeal state and to nasalization. A listener appears to have the ability to integrate these diverse, brief acoustic cues for the features of consonants, although the mechanism for this integration process is unclear.},
  pmid = {10992135},
  keywords = {\#nosource}
}

@article{Stevens1978,
  title = {Invariant Cues for Place of Articulation in Stop Consonants.},
  author = {Stevens, K N and Blumstein, S E},
  year = {1978},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {64},
  number = {5},
  pages = {1358--68},
  issn = {0001-4966},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/744836},
  urldate = {2017-01-25},
  abstract = {In a series of experiments, identification responses for place of articulation were obtained for synthetic stop consonants in consonant-vowel syllables with different vowels. The acoustic attributes of the consonants were systematically manipulated, the selection of stimulus characteristics being guided in part by theoretical considerations concerning the expected properties of the sound generated in the vocal tract as place of articulation is varied. Several stimulus series were generated with and without noise bursts at the onset, and with and without formant transitions following consonantal release. Stimuli with transitions only, and with bursts plus transitions, were consistently classified according to place of articulation, whereas stimuli with bursts only and no transitions were not consistently identified. The acoustic attributes of the stimuli were examined to determine whether invariant properties characterized each place of atriculation independent of vowel context. It was determined that the gross shape of the spectrum sampled at the consonantal release showed a distinctive shape for each place of articulation: a prominent midfrequency spectral peak for velars, a diffuse-rising spectrum for alveolars, and a diffuse-falling spectrum for labials. These attributes are evident for stimuli containing transitions only, but are enhanced by the presence of noise bursts at the onset.},
  pmid = {744836},
  keywords = {\#nosource}
}

@article{Stevens1978a,
  title = {Invariant Cues for Place of Articulation in Stop Consonants},
  author = {Stevens, K. N. and Blumstein, S. E.},
  year = {1978},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {64},
  number = {5},
  pages = {1358--1368},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.382102},
  url = {http://asa.scitation.org/doi/10.1121/1.382102},
  urldate = {2017-12-13},
  abstract = {In a series of experiments, identification responses for place of articulation were obtained for synthetic stop consonants in consonant\textendash vowel syllables with different vowels. The acoustic attributes of the consonants were systematically manipulated, the selection of stimulus characteristics being guided in part by theoretical considerations concerning the expected properties of the sound generated in the vocal tract as place of articulation is varied. Several stimulus series were generated with and without noise bursts at the onset, and with and without formant transitions following consonantal release. Stimuli with transitions only, and with bursts plus transitions, were consistently classified according to place of articulation, whereas stimuli with bursts only and no transitions were not consistently identified. The acoustic attributes of the stimuli were examined to determine whether invariant properties characterized each place of atriculation independent of vowel context. It was determined that the ...},
  file = {/Users/jonny/Papers/StevensK/1978/Stevens_1978_Invariant cues for place of articulation in stop consonants.pdf}
}

@article{Stevens1989,
  title = {On the Quantal Nature of Speech},
  author = {Stevens, K. N},
  year = {1989},
  journal = {Journal of Phonetics},
  volume = {17},
  pages = {3--45},
  issn = {00954470},
  doi = {10.1109/ICSLP.1996.607202},
  abstract = {This is a review of regularities which we have observed in the analysis of text reading, mostly Swedish, directed to the timing of vowels and consonants, syllables, inter-stress intervals and pauses. We have found tendencies of quantal aspects of temporal structure, superimposed on more gradual variations, which add quasi-rhythmical elements to speech. A local average of inter-stress intervals of the order of 0.5 sec. appears to function as a reference quantum for the planning of pause durations. A recent study, confirming our previous findings of multiple peaks with about 0.5 sec. spacing in histograms of pause durations, provides support for this model. It is well established that pause durations tend to increase with increasing syntactic level of boundaries. However, these variations tend to be quantally scaled, even within a specific boundary category, e.g. between sentences or between paragraphs. Relatively short pauses, as between phrases or clauses, show durations in complementary relation to terminal lengthening. There are indications of approximately 1, 1/2, 1/4 and 1/8 ratios of the average durations of inter-stress intervals, stressed syllables, unstressed syllables and phoneme segments, which add to the observed regularities. The timing of syllables and phonetic segments, with due regard to relative distinctiveness and reading speed, is discussed, and also tempo variations within a sentence},
  isbn = {0-7803-3555-4},
  pmid = {982},
  keywords = {\#nosource}
}

@book{Stevens1998,
  title = {Acoustic Phonetics},
  author = {Stevens, Kenneth N.},
  year = {1998},
  publisher = {{MIT Press}},
  url = {https://books.google.com/books?id=Gej94hCGrLMC\&source=gbs_navlinks_s},
  urldate = {2017-12-22},
  abstract = {Content Description \#Includes bibliographical references (p.) and index. Preface -- 1. Anatomy and physiology of speech production -- 2. Source mechanisms -- 3. Basic acoustics of vocal tract resonators -- 4. Auditory processing of speechlike sounds -- 5. Phonological representation of utterances -- 6. Vowels : acoustic events with a relatively open vocal tract -- 7. The basic stop consonants : bursts and formant transitions -- 8. Obstruent consonants -- 9. Sonorant consonants -- 10. Some influences of context on speech sound production.},
  isbn = {0-262-69250-3},
  keywords = {\#nosource}
}

@article{Stilp2010,
  title = {Cochlea-Scaled Entropy, Not Consonants, Vowels, or Time, Best Predicts Speech Intelligibility},
  author = {Stilp, Christian E. and Kluender, Keith R.},
  year = {2010},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {107},
  number = {27},
  pages = {12387--12392},
  issn = {0027-8424},
  doi = {10.1073/pnas.0913625107},
  abstract = {Speech sounds are traditionally divided into consonants and vowels.\textbackslash r\textbackslash nWhen only vowels or only consonants are replaced by noise, listeners\textbackslash r\textbackslash nare more accurate understanding sentences in which consonants are\textbackslash r\textbackslash nreplaced but vowels remain. From such data, vowels have been\textbackslash r\textbackslash nsuggested to be more important for understanding sentences;\textbackslash r\textbackslash nhowever, such conclusions are mitigated by the fact that replaced\textbackslash r\textbackslash nconsonant segments were roughly one-third shorter than vowels.\textbackslash r\textbackslash nWe report two experiments that demonstrate listener performance\textbackslash r\textbackslash nto be better predicted by simple psychoacoustic measures of cochleascaled\textbackslash r\textbackslash nspectral change across time. First, listeners identified sentences\textbackslash r\textbackslash nin which portions of consonants (C), vowels (V), CV transitions,\textbackslash r\textbackslash nor VC transitions were replaced by noise. Relative intelligibility was\textbackslash r\textbackslash nnot well accounted for on the basis of Cs, Vs, or their transitions. In\textbackslash r\textbackslash na second experiment, distinctions between Cs and Vs were abandoned.\textbackslash r\textbackslash nInstead, portions of sentences were replaced on the basis of\textbackslash r\textbackslash ncochlea-scaled spectral entropy (CSE). Sentence segments having\textbackslash r\textbackslash nrelatively high, medium, or low entropy were replaced with noise.\textbackslash r\textbackslash nIntelligibility decreased linearly as the amount of replaced CSE\textbackslash r\textbackslash nincreased. Duration of signal replaced and proportion of consonants/vowels\textbackslash r\textbackslash nreplaced fail to account for listener data. CSE corresponds\textbackslash r\textbackslash nclosely with the linguistic construct of sonority (or vowellikeness)\textbackslash r\textbackslash nthat is useful for describing phonological systematicity,\textbackslash r\textbackslash nespecially syllable composition. Results challenge traditional distinctions\textbackslash r\textbackslash nbetween consonants and vowels. Speech intelligibility is better\textbackslash r\textbackslash npredicted by nonlinguistic sensory measures of uncertainty (potential\textbackslash r\textbackslash ninformation) than by orthodox physical acoustic measures or\textbackslash r\textbackslash nlinguistic constructs.},
  isbn = {0027-8424\textbackslash r1091-6490},
  pmid = {20566842},
  file = {/Users/jonny/Dropbox/papers/zotero/S/StilpC/stilp_2010_cochlea-scaled_entropy,_not_consonants,_vowels,_or_time,_best_predicts_speech.pdf}
}

@article{Strauss2007,
  title = {{{jTRACE}}: {{A}} Reimplementation and Extension of the {{TRACE}} Model of Speech Perception and Spoken Word Recognition},
  author = {Strauss, Ted J. and Harris, Harlan D. and Magnuson, James S.},
  year = {2007},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {39},
  number = {1},
  pages = {19--30},
  publisher = {{Springer-Verlag}},
  issn = {1554-351X},
  doi = {10.3758/BF03192840},
  url = {http://www.springerlink.com/index/10.3758/BF03192840},
  urldate = {2017-05-01},
  file = {/Users/jonny/Papers/StraussT/2007/Strauss_2007_jTRACE2.pdf}
}

@article{Suga1978,
  title = {Cortical Neurons Sensitive to Combinations of Information-Bearing Elements of Biosonar Signals in the Mustache Bat},
  author = {Suga, N and O'Neill, WE and Manabe, T},
  year = {1978},
  journal = {Science},
  volume = {200},
  number = {4343},
  url = {http://science.sciencemag.org/content/200/4343/778},
  urldate = {2017-08-08},
  keywords = {\#nosource}
}

@misc{Sundar2014,
  title = {Binom: {{Binomial Confidence Intervals For Several Parameterizations}}},
  author = {Sundar, Dorai-Raj},
  year = {2014},
  pages = {R package version 1.1-1},
  abstract = {Constructs confidence intervals on the probability of success in a binomial experiment via several parameterizations},
  keywords = {\#nosource}
}

@article{Sussman1998,
  title = {Linear Correlates in the Speech Signal: The Orderly Output Constraint.},
  author = {Sussman, Harvey M and Fruchter, David and Hilbert, Jon and Sirosh, Joseph},
  year = {1998},
  journal = {The Behavioral and brain sciences},
  volume = {21},
  number = {2},
  pages = {241-59; discussion 260-99},
  publisher = {{University of Oregon Library}},
  issn = {0140-525X},
  doi = {10.1017/S0140525X98001174},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/10097014},
  abstract = {Neuroethological investigations of mammalian and avian auditory systems have documented species-specific specializations for processing complex acoustic signals that could, if viewed in abstract terms, have an intriguing and striking relevance for human speech sound categorization and representation. Each species forms biologically relevant categories based on combinatorial analysis of information-bearing parameters within the complex input signal. This target article uses known neural models from the mustached bat and barn owl to develop, by analogy, a conceptualization of human processing of consonant plus vowel sequences that offers a partial solution to the noninvariance dilemma--the nontransparent relationship between the acoustic waveform and the phonetic segment. Critical input sound parameters used to establish species-specific categories in the mustached bat and barn owl exhibit high correlation and linearity due to physical laws. A cue long known to be relevant to the perception of stop place of articulation is the second formant (F2) transition. This article describes an empirical phenomenon--the locus equations--that describes the relationship between the F2 of a vowel and the F2 measured at the onset of a consonant-vowel (CV) transition. These variables, F2 onset and F2 vowel within a given place category, are consistently and robustly linearly correlated across diverse speakers and languages, and even under perturbation conditions as imposed by bite blocks. A functional role for this category-level extreme correlation and linearity (the "orderly output constraint") is hypothesized based on the notion of an evolutionarily conserved auditory-processing strategy. High correlation and linearity between critical parameters in the speech signal that help to cue place of articulation categories might have evolved to satisfy a preadaptation by mammalian auditory systems for representing tightly correlated, linearly related components of acoustic signals.},
  isbn = {0140-525X},
  pmid = {10097014},
  keywords = {acoustic,and frustration,because they,categories,Humans,linearity,locus equations,must,must not tolerate is,neuroethology,noninvariance,perception,phoneme,Phonetics,place of articulation,scientists do tolerate uncertainty,sound,Speech,Speech Acoustics,Speech Perception,Speech Perception: physiology,speech signal,Speech: physiology,the one thing that,they do not and},
  file = {/Users/jonny/Zotero/storage/BW56LEYG/Sussman et al. - 1998 - Linear correlates in the speech signal the orderly output constraint(6).pdf;/Users/jonny/Zotero/storage/E9MXL78M/Sussman et al. - 1998 - Linear correlates in the speech signal the orderly output constraint(5).pdf}
}

@article{Syka2002,
  title = {Gap Detection Threshold in the Rat before and after Auditory Cortex Ablation},
  author = {Syka, J and Rybalko, N and Mazelov{\'a}, J and Druga, R},
  year = {2002},
  journal = {Hearing Research},
  volume = {172},
  number = {1},
  pages = {151--159},
  issn = {03785955},
  doi = {10.1016/S0378-5955(02)00578-6},
  url = {http://www.sciencedirect.com/science/article/pii/S0378595502005786},
  urldate = {2017-06-06},
  abstract = {Gap detection threshold (GDT) was measured in adult female pigmented rats (strain Long\textendash Evans) by an operant conditioning technique with food reinforcement, before and after bilateral ablation of the auditory cortex. GDT was dependent on the frequency spectrum and intensity of the continuously present noise in which the gaps were embedded. The mean values of GDT for gaps embedded in white noise or low-frequency noise (upper cutoff frequency 3 kHz) at 70 dB sound pressure level (SPL) were 1.57{$\pm$}0.07 ms and 2.9{$\pm$}0.34 ms, respectively. Decreasing noise intensity from 80 dB SPL to 20 dB SPL produced a significant increase in GDT. The increase in GDT was relatively small in the range of 80\textendash 50 dB SPL for white noise and in the range of 80\textendash 60 dB for low-frequency noise. The minimal intensity level of the noise that enabled GDT measurement was 20 dB SPL for white noise and 30 dB SPL for low-frequency noise. Mean GDT values at these intensities were 10.6{$\pm$}3.9 ms and 31.3{$\pm$}4.2 ms, respectively. Bilateral ablation of the primary auditory cortex (complete destruction of the Te1 and partial destruction of the Te2 and Te3 areas) resulted in an increase in GDT values. The fifth day after surgery, the rats were able to detect gaps in the noise. The values of GDT observed at this time were 4.2{$\pm$}1.1 ms for white noise and 7.4{$\pm$}3.1 ms for low-frequency noise at 70 dB SPL. During the first month after cortical ablation, recovery of GDT was observed. However, 1 month after cortical ablation GDT still remained slightly higher than in controls (1.8{$\pm$}0.18 for white noise, 3.22{$\pm$}0.15 for low-frequency noise, P{$<$}0.05). A decrease in GDT values during the subsequent months was not observed.},
  keywords = {\#nosource}
}

@article{Tabain2000,
  title = {Coarticulation in {{CV}} Syllables: A Comparison of {{Locus Equation}} and {{EPG}} Data},
  author = {Tabain, Marija},
  year = {2000},
  journal = {Journal of Phonetics},
  volume = {28},
  number = {2},
  pages = {137--159},
  issn = {00954470},
  doi = {10.1006/jpho.2000.0110},
  abstract = {Using electropalatographic (EPG) data, the following hypothesis is tested: the slope value generated by a locus equation (LE) analysis of the F 2 transition in CV syllables is an accurate reflection of the degree of coarticulation between the consonant and the vowel in that syllable. The consonants studied are /{$\partial$} z z d g n n l r/. Comparisons between EPG and LE data suggest that the LE analysis provides an accurate reflection of the degree of coarticulation for the stop and nasal classes in English, which have an alveolar and a velar place of articulation amongst the lingual consonants. By contrast, the correlation between EPG and LE data for the fricative consonants is very poor. Two explanations are offered for the poorer results for the fricatives: (1) the fricative noise following consonant release obscures the F 2 transition, rendering formant measurement less accurate, and (2) the LE is capable of encoding gross differences in degree of coarticulation (such as that between an alveolar and a velar) but not more subtle differences such as those between the various coronal articulations.},
  file = {/Users/jonny/Papers/TabainM/2000/Tabain_2000_Coarticulation in CV syllables.pdf}
}

@misc{Team2015,
  title = {{{RStudio}}: {{Integrated Development}} for {{R}}.},
  author = {Team, RStudio},
  year = {2015},
  publisher = {{RStudio, Inc., Boston, MA}},
  url = {http://www.rstudio.com/},
  keywords = {\#nosource}
}

@misc{Team2016,
  title = {R: {{A}} Language and Environment for Statistical Computing.},
  author = {Team, R Core},
  year = {2016},
  journal = {R Foundation for Statistical Computing, Vienna, Austria.},
  keywords = {\#nosource}
}

@article{Theunissen2014,
  title = {Neural Processing of Natural Sounds.},
  author = {Theunissen, Fr{\'e}d{\'e}ric E and Elie, Julie E},
  year = {2014},
  journal = {Nature reviews. Neuroscience},
  volume = {15},
  number = {6},
  pages = {355--66},
  issn = {1471-0048},
  doi = {10.1038/nrn3731},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/24840800},
  abstract = {We might be forced to listen to a high-frequency tone at our audiologist's office or we might enjoy falling asleep with a white-noise machine, but the sounds that really matter to us are the voices of our companions or music from our favourite radio station. The auditory system has evolved to process behaviourally relevant natural sounds. Research has shown not only that our brain is optimized for natural hearing tasks but also that using natural sounds to probe the auditory system is the best way to understand the neural computations that enable us to comprehend speech or appreciate music.},
  isbn = {1471-0048 (Electronic)\textbackslash r1471-003X (Linking)},
  pmid = {24840800},
  keywords = {Acoustic Stimulation,Animals,Auditory Pathways,Auditory Pathways: physiology,Auditory Perception,Auditory Perception: physiology,Brain Mapping,Hearing,Hearing: physiology,Humans,Music,Sound},
  file = {/Users/jonny/Papers/TheunissenF/2014/Theunissen_2014_Neural processing of natural sounds.pdf}
}

@article{Tremblay2016,
  title = {Broca and {{Wernicke}} Are Dead, or Moving Past the Classic Model of Language Neurobiology},
  author = {Tremblay, Pascale and Dick, Anthony Steven},
  year = {2016},
  journal = {Brain and Language},
  volume = {162},
  pages = {60--71},
  issn = {0093934X},
  doi = {10.1016/j.bandl.2016.08.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0093934X16300475},
  urldate = {2017-04-27},
  abstract = {With the advancement of cognitive neuroscience and neuropsychological research, the field of language neurobiology is at a cross-roads with respect to its framing theories. The central thesis of this article is that the major historical framing model, the Classic ``Wernicke-Lichtheim-Geschwind'' model, and associated terminology, is no longer adequate for contemporary investigations into the neurobiology of language. We argue that the Classic model (1) is based on an outdated brain anatomy; (2) does not adequately represent the distributed connectivity relevant for language, (3) offers a modular and ``language centric'' perspective, and (4) focuses on cortical structures, for the most part leaving out subcortical regions and relevant connections. To make our case, we discuss the issue of anatomical specificity with a focus on the contemporary usage of the terms ``Broca's and Wernicke's area'', including results of a survey that was conducted within the language neurobiology community. We demonstrate that there is no consistent anatomical definition of ``Broca's and Wernicke's Areas'', and propose to replace these terms with more precise anatomical definitions. We illustrate the distributed nature of the language connectome, which extends far beyond the single-pathway notion of arcuate fasciculus connectivity established in Geschwind's version of the Classic Model. By illustrating the definitional confusion surrounding ``Broca's and Wernicke's areas'', and by illustrating the difficulty integrating the emerging literature on perisylvian white matter connectivity into this model, we hope to expose the limits of the model, argue for its obsolescence, and suggest a path forward in defining a replacement.},
  keywords = {\#nosource}
}

@article{Vapnik1999,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, V.N.},
  year = {1999},
  journal = {IEEE Transactions on Neural Networks},
  volume = {10},
  number = {5},
  pages = {988--999},
  issn = {10459227},
  doi = {10.1109/72.788640},
  url = {http://ieeexplore.ieee.org/document/788640/},
  urldate = {2017-05-03},
  keywords = {\#nosource}
}

@article{Wang2005a,
  title = {Sustained Firing in Auditory Cortex Evoked by Preferred Stimuli.},
  author = {Wang, Xiaoqin and Lu, Thomas and Snider, Ross K and Liang, Li},
  year = {2005},
  journal = {Nature},
  volume = {435},
  number = {7040},
  pages = {341--6},
  issn = {1476-4687},
  doi = {10.1038/nature03565},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/15902257},
  abstract = {It has been well documented that neurons in the auditory cortex of anaesthetized animals generally display transient responses to acoustic stimulation, and typically respond to a brief stimulus with one or fewer action potentials. The number of action potentials evoked by each stimulus usually does not increase with increasing stimulus duration. Such observations have long puzzled researchers across disciplines and raised serious questions regarding the role of the auditory cortex in encoding ongoing acoustic signals. Contrary to these long-held views, here we show that single neurons in both primary (area A1) and lateral belt areas of the auditory cortex of awake marmoset monkeys (Callithrix jacchus) are capable of firing in a sustained manner over a prolonged period of time, especially when they are driven by their preferred stimuli. In contrast, responses become more transient or phasic when auditory cortex neurons respond to non-preferred stimuli. These findings suggest that when the auditory cortex is stimulated by a sound, a particular population of neurons fire maximally throughout the duration of the sound. Responses of other, less optimally driven neurons fade away quickly after stimulus onset. This results in a selective representation of the sound across both neuronal population and time.},
  isbn = {1476-4687 (Electronic)\textbackslash n0028-0836 (Linking)},
  pmid = {15902257},
  keywords = {Acoustic Stimulation,Action Potentials,Action Potentials: physiology,Animals,Auditory Cortex,Auditory Cortex: cytology,Auditory Cortex: physiology,Auditory Perception,Auditory Perception: physiology,Callithrix,Callithrix: physiology,Models,Neurological,Neurons,Neurons: physiology,Sound,Time Factors,Wakefulness,Wakefulness: physiology},
  file = {/Users/jonny/Dropbox/papers/zotero/W/WangX/wang_2005_sustained_firing_in_auditory_cortex_evoked_by_preferred_stimuli.pdf;/Users/jonny/Zotero/storage/BJQDPFTC/Wang et al. - 2005 - Sustained firing in auditory cortex evoked by preferred stimuli(6).pdf}
}

@article{Weible2014,
  title = {Auditory Cortex Is Required for Fear Potentiation of Gap Detection.},
  author = {Weible, Aldis P and Liu, Christine and Niell, Cristopher M and Wehr, Michael},
  year = {2014},
  journal = {The Journal of neuroscience},
  volume = {34},
  number = {46},
  pages = {15437--45},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.3408-14.2014},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/25392510},
  abstract = {Auditory cortex is necessary for the perceptual detection of brief gaps in noise, but is not necessary for many other auditory tasks such as frequency discrimination, prepulse inhibition of startle responses, or fear conditioning with pure tones. It remains unclear why auditory cortex should be necessary for some auditory tasks but not others. One possibility is that auditory cortex is causally involved in gap detection and other forms of temporal processing in order to associate meaning with temporally structured sounds. This predicts that auditory cortex should be necessary for associating meaning with gaps. To test this prediction, we developed a fear conditioning paradigm for mice based on gap detection. We found that pairing a 10 or 100 ms gap with an aversive stimulus caused a robust enhancement of gap detection measured 6 h later, which we refer to as fear potentiation of gap detection. Optogenetic suppression of auditory cortex during pairing abolished this fear potentiation, indicating that auditory cortex is critically involved in associating temporally structured sounds with emotionally salient events.},
  isbn = {1529-2401 (Electronic)\textbackslash r0270-6474 (Linking)},
  pmid = {25392510},
  keywords = {auditory cortex,fear conditioning,gap detection,optogenetics},
  file = {/Users/jonny/Zotero/storage/4D7WQHN2/Weible et al. - 2014 - Auditory cortex is required for fear potentiation of gap detection(5).pdf;/Users/jonny/Zotero/storage/GAI58BUY/Weible et al. - 2014 - Auditory cortex is required for fear potentiation of gap detection(6).pdf}
}

@article{Weiss2001,
  title = {The Effect of Class Distribution on Classifier Learning: An Empirical Study},
  author = {Weiss, Gm and Provost, Foster},
  year = {2001},
  journal = {Rutgers Univ},
  abstract = {In this article we analyze the effect of class distribution on classifier learning. We begin by describing the different ways in which class distribution affects learning and how it affects the evaluation of learned classifiers. We then present the results of two comprehensive experimental studies. The first study compares the performance of classifiers generated from unbalanced data sets with the performance of classifiers generated from balanced versions of the same data sets. This...},
  file = {/Users/jonny/Papers/WeissG/undefined/Weiss_The Effect of Class Distribution on Classifier Learning.pdf;/Users/jonny/Zotero/storage/EECY8PK5/Weiss, Provost - 2001 - The effect of class distribution on classifier learning an empirical study(3).pdf}
}

@book{Wickham2009,
  title = {Ggplot2: {{Elegant Graphics}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2009},
  publisher = {{Springer-Verlag}},
  address = {{New York, NY}},
  isbn = {978-0-387-98140-6},
  keywords = {\#nosource}
}

@article{Wickham2011,
  title = {The {{Split-Apply-Combine Strategy}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {1},
  pages = {1--29},
  url = {http://www.jstatsoft.org/v40/i01/},
  keywords = {\#nosource}
}

@book{Wittgenstein1958,
  title = {Philosophical {{Investigations}}},
  author = {Wittgenstein, Ludwig},
  editor = {Blackwell, Basil},
  year = {1958},
  publisher = {{Basil Blackwell Ltd.}},
  address = {{Oxford, UK}},
  keywords = {\#nosource}
}

@misc{XiongSelectiveCorticostriatal,
  title = {Xiong: {{Selective}} Corticostriatal Plasticity during... - {{Google Scholar}}},
  url = {https://scholar.google.com/scholar_lookup?title=Selective\%20corticostriatal\%20plasticity\%20during\%20acquisition\%20of\%20an\%20auditory\%20discrimination\%20task\&author=Q\%20Xiong\&author=P\%20Znamenskiy\&author=AM\%20Zador\&publication_year=2015\&journal=Nature\&volume=521\&pages=1-16},
  urldate = {2019-01-02},
  file = {/Users/jonny/Zotero/storage/UB62A6Q6/scholar_lookup.html}
}

@article{yiEncodingSpeechSounds2019,
  title = {The {{Encoding}} of {{Speech Sounds}} in the {{Superior Temporal Gyrus}}},
  author = {Yi, Han Gyol and Leonard, Matthew K. and Chang, Edward F.},
  year = {2019},
  month = jun,
  journal = {Neuron},
  volume = {102},
  number = {6},
  pages = {1096--1110},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.04.023},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(19)30380-0},
  urldate = {2019-07-28},
  langid = {english},
  pmid = {31220442},
  keywords = {_tablet,acoustic-phonetic features,auditory cortex,context-dependent representation,electrocorticography,phonological sequence,speech processing,superior temporal gyrus,temporal integration,temporal landmarks,temporally recurrent connections},
  file = {/Users/jonny/Dropbox/papers/zotero/Y/YiH/yi_2019_the_encoding_of_speech_sounds_in_the_superior_temporal_gyrus.pdf;/Users/jonny/Zotero/storage/ZSWH63WD/S0896-6273(19)30380-0.html}
}

@article{zadorCritiquePureLearning2019,
  title = {A {{Critique}} of {{Pure Learning}}: {{What Artificial Neural Networks}} Can {{Learn}} from {{Animal Brains}}},
  shorttitle = {A {{Critique}} of {{Pure Learning}}},
  author = {Zador, Anthony M.},
  year = {2019},
  month = mar,
  journal = {bioRxiv},
  pages = {582643},
  doi = {10.1101/582643},
  url = {https://www.biorxiv.org/content/10.1101/582643v1},
  urldate = {2019-07-28},
  abstract = {{$<$}h3{$>$}ABSTRACT{$<$}/h3{$>$} {$<$}p{$>$}Over the last decade, artificial neural networks (ANNs), have undergone a revolution, catalyzed in large part by better tools for supervised learning. However, training such networks requires enormous data sets of labeled examples, whereas young animals (including humans) typically learn with few or no labeled examples. This stark contrast with biological learning has led many in the ANN community posit that instead of supervised paradigms, animals must rely instead primarily on unsupervised learning, leading the search for better unsupervised algorithms. Here we argue that much of an animal's behavioral repertoire is not the result of clever learning algorithms\textemdash supervised or unsupervised\textemdash but arises instead from behavior programs already present at birth. These programs arise through evolution, are encoded in the genome, and emerge as a consequence of wiring up the brain. Specifically, animals are born with highly structured brain connectivity, which enables them learn very rapidly. Recognizing the importance of the highly structured connectivity suggests a path toward building ANNs capable of rapid learning.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/jonny/Papers/ZadorA/2019/Zador_2019_A Critique of Pure Learning.pdf;/Users/jonny/Zotero/storage/AXVRU6GP/582643v1.html}
}


