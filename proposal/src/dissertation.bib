
@article{kaneRealtimeLowlatencyClosedloop2020,
  title = {Real-Time, Low-Latency Closed-Loop Feedback Using Markerless Posture Tracking},
  author = {Kane, Gary A and Lopes, Gonçalo and Saunders, Jonny L and Mathis, Alexander and Mathis, Mackenzie W},
  editor = {Berman, Gordon J and Behrens, Timothy E and Berman, Gordon J and Branco, Tiago},
  date = {2020-12-08},
  journaltitle = {eLife},
  volume = {9},
  pages = {e61909},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.61909},
  url = {https://doi.org/10.7554/eLife.61909},
  urldate = {2021-07-30},
  abstract = {The ability to control a behavioral task or stimulate neural activity based on animal behavior in real-time is an important tool for experimental neuroscientists. Ideally, such tools are noninvasive, low-latency, and provide interfaces to trigger external hardware based on posture. Recent advances in pose estimation with deep learning allows researchers to train deep neural networks to accurately quantify a wide variety of animal behaviors. Here, we provide a new DeepLabCut-Live! package that achieves low-latency real-time pose estimation (within 15 ms, {$>$}100 FPS), with an additional forward-prediction module that achieves zero-latency feedback, and a dynamic-cropping mode that allows for higher inference speeds. We also provide three options for using this tool with ease: (1) a stand-alone GUI (called DLC-Live! GUI), and integration into (2) Bonsai, and (3) AutoPilot. Lastly, we benchmarked performance on a wide range of systems so that experimentalists can easily decide what hardware is required for their needs.},
  keywords = {any animal,DeepLabCut,low-latency,pose-estimation,real-time tracking},
  file = {/Users/jonny/Dropbox/papers/zotero/K/KaneG/kane_2020_real-time,_low-latency_closed-loop_feedback_using_markerless_posture_tracking.pdf}
}

@article{lachancePVP1PeopleVentilator2020,
  type = {article; https://web.archive.org/web/20220205022650/https://www.medrxiv.org/content/10.1101/2020.10.02.20206037v1},
  title = {{{PVP1}}–{{The People}}’s {{Ventilator Project}}: {{A}} Fully Open, Low-Cost, Pressure-Controlled Ventilator},
  shorttitle = {{{PVP1}}–{{The People}}’s {{Ventilator Project}}},
  author = {LaChance, Julienne and Zajdel, Tom J. and Schottdorf, Manuel and Saunders, Jonny L. and Dvali, Sophie and Marshall, Chase and Seirup, Lorenzo and Notterman, Daniel A. and Cohen, Daniel J.},
  date = {2020-10-05},
  journaltitle = {medRxiv},
  pages = {2020.10.02.20206037},
  doi = {10.1101/2020.10.02.20206037v1},
  url = {https://www.medrxiv.org/content/10.1101/2020.10.02.20206037v1},
  urldate = {2022-02-05},
  abstract = {We present a fully open ventilator platform–The People’s Ventilator: PVP1– with complete documentation and detailed build instructions, and a DIY cost of \$1,300 USD. Here, we validate PVP1 against key performance criteria specified in the U.S. Food and Drug Administration’s Emergency Use Authorization for Ventilators. Notably, PVP1 performs well over a wide range of test conditions and has been demonstrated to perform stably for a minimum of 72,000 breath cycles over three days with a mechanical test lung. As an open project, PVP1 can enable both future educational, academic, and clinical developments in the ventilator space.},
  langid = {english},
  keywords = {archived},
  file = {/Users/jonny/Dropbox/papers/zotero/L/LaChanceJ/lachance_2020_pvp1–the_people’s_ventilator_project.pdf}
}

@article{saundersAutopilotAutomatingBehavioral2019,
  title = {Autopilot: {{Automating}} Behavioral Experiments with Lots of {{Raspberry Pis}}},
  shorttitle = {Autopilot},
  author = {Saunders, Jonny L. and Wehr, Michael},
  date = {2019-10-17},
  journaltitle = {bioRxiv},
  pages = {807693},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/807693},
  url = {https://www.biorxiv.org/content/10.1101/807693v1},
  urldate = {2021-03-12},
  abstract = {{$<$}p{$>$}Neuroscience needs behavior, and behavioral experiments require the coordination of large numbers of heterogeneous hardware components and data streams. Currently available tools strongly limit the complexity and reproducibility of experiments. Here we introduce Autopilot, a complete, open-source Python framework for behavioral neuroscience that distributes experiments over networked swarms of Raspberry Pis. Autopilot enables qualitatively greater experimental flexibility by allowing arbitrary numbers of hardware components to be combined in arbitrary experimental designs. Research is made reproducible by documenting all data and task design parameters in a human-readable and publishable format at the time of collection. Autopilot provides an order-of-magnitude performance improvement over existing tools while also being an order of magnitude less costly to implement. Autopilot’s flexible, scalable architecture allows neuroscientists to design the next generation of experiments to investigate the behaving brain.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/jonny/Dropbox/papers/zotero/S/SaundersJ/saunders_2019_autopilot.pdf}
}

@article{saundersMiceCanLearn2019,
  title = {Mice Can Learn Phonetic Categories},
  author = {Saunders, Jonny L. and Wehr, Michael},
  date = {2019-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {145},
  number = {3},
  pages = {1168--1177},
  issn = {0001-4966},
  doi = {10.1121/1.5091776},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.5091776},
  urldate = {2019-08-12},
  file = {/Users/jonny/Papers/SaundersJ/2019/Saunders_2019_Mice can learn phonetic categories2.pdf;/Users/jonny/Zotero/storage/W3FSZK8G/1.html}
}


